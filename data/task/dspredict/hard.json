[
    {
      "challenge_name": "LANL-Earthquake-Prediction",
      "description": "Challenge description:\n# LANL Earthquake Prediction\n\nCan you predict upcoming laboratory earthquakes?\n\nForecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be.\n\nIn this competition, you will address when the earthquake will take place. Specifically, you'll predict the time remaining before laboratory earthquakes occur from real-time seismic data.\n\nIf this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure.\n\nThis challenge is hosted by Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.\n\n## Acknowledgments\n\nGeophysics Group:\nThe competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.\n\nDepartment of Geosciences:\nData are from experiments performed by Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.\n\nDepartment of Physics & Astronomy:\nThis competition stemmed from the DOE Council workshop \"Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks\" held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.\n\nDepartment of Energy Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division:\nThe Geosciences core research.\n\nPhoto by Nik Shuliahin on Unsplash\n\n## Evaluation\n\nSubmissions are evaluated using the mean absolute error between the predicted time remaining before the next lab earthquake and the actual remaining time.\n\n## Submission File\n\nFor each seg_id in the test set folder, you must predict time_to_failure, which is the remaining time before the next lab earthquake. The file should contain a header and have the following format:\n\nseg_id,time_to_failure\nseg_00030f,0\nseg_0012b5,0\nseg_00184e,0\n...\n\n## Timeline\n\nMay 27, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMay 27, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nMay 27, 2019 - External Data Disclosure deadline. All external data used in the competition must be disclosed in the forums by this date.\nJune 3, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $20,000\n2nd Place - $15,000\n3rd Place - $7,000\n4th Place - $5,000\n5th Place - $3,000\n\n## Additional Information\n\nThe data are from an experiment conducted on rock in a double direct shear geometry subjected to bi-axial loading, a classic laboratory earthquake model (fig. a).\n\nTwo fault gouge layers are sheared simultaneously while subjected to a constant normal load and a prescribed shear velocity. The laboratory faults fail in repetitive cycles of stick and slip that is meant to mimic the cycle of loading and failure on tectonic faults. While the experiment is considerably simpler than a fault in Earth, it shares many physical characteristics. (fig. b)\n\nLos Alamos' initial work showed that the prediction of laboratory earthquakes from continuous seismic data is possible in the case of quasi-periodic laboratory seismic cycles. In this competition, the team has provided a much more challenging dataset with considerably more aperiodic earthquake failures.\n\n## Citation\n\nBertrand RL, Laura Pyrak-Nolte, Walter Reade, and Addison Howard. LANL Earthquake Prediction. https://kaggle.com/competitions/LANL-Earthquake-Prediction, 2019. Kaggle.\n\n## Competition Statistics\n\nCompetition Host: Los Alamos National Laboratory\nTotal Prize Pool: $50,000\n28,088 Entrants\n5,454 Participants\n4,516 Teams\n59,891 Submissions\n\nTags: Physics, Earth Science, Signal Processing, Mean Absolute Error\n\nData description:\nLANL Earthquake PredictionCan you predict upcoming laboratory earthquakes?Dataset DescriptionThe goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. Theacoustic_datainput signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure).The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The datawithineach test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.For eachseg_idin the test folder, you should predict asingletime_to_failurecorresponding to the time between thelast row of the segmentand the next laboratory earthquake.File descriptionstrain.csv- A single, continuous training segment of experimental data.test- A folder containing many small segments of test data.sample_sumbission.csv- A sample submission file in the correct format.Data fieldsacoustic_data- the seismic signal [int16]time_to_failure- the time (in seconds) until the next laboratory earthquake [float64]seg_id- the test segment ids for which predictions should be made (one prediction per segment)Files2626 filesSize10.42 GBTypecsvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/LANL-Earthquake-Prediction",
      "competition_description": "Challenge description:\n# LANL Earthquake Prediction\n\nCan you predict upcoming laboratory earthquakes?\n\nForecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: when the event will occur, where it will occur, and how large it will be.\n\nIn this competition, you will address when the earthquake will take place. Specifically, you'll predict the time remaining before laboratory earthquakes occur from real-time seismic data.\n\nIf this challenge is solved and the physics are ultimately shown to scale from the laboratory to the field, researchers will have the potential to improve earthquake hazard assessments that could save lives and billions of dollars in infrastructure.\n\nThis challenge is hosted by Los Alamos National Laboratory which enhances national security by ensuring the safety of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.\n\n## Acknowledgments\n\nGeophysics Group:\nThe competition builds on initial work from Bertrand Rouet-Leduc, Claudia Hulbert, and Paul Johnson. B. Rouet-Leduc prepared the data for the competition.\n\nDepartment of Geosciences:\nData are from experiments performed by Chas Bolton, Jacques Riviere, Paul Johnson and Prof. Chris Marone.\n\nDepartment of Physics & Astronomy:\nThis competition stemmed from the DOE Council workshop \"Information is in the Noise: Signatures of Evolving Fracture and Fracture Networks\" held March 2018 that was organized by Prof. Laura J. Pyrak-Nolte.\n\nDepartment of Energy Office of Science, Basic Energy Sciences, Chemical Sciences, Geosciences and Biosciences Division:\nThe Geosciences core research.\n\nPhoto by Nik Shuliahin on Unsplash",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated using the mean absolute error between the predicted time remaining before the next lab earthquake and the actual remaining time.",
      "dataset_description": "Data description:\nLANL Earthquake PredictionCan you predict upcoming laboratory earthquakes?Dataset DescriptionThe goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. Theacoustic_datainput signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure).The training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The datawithineach test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.For eachseg_idin the test folder, you should predict asingletime_to_failurecorresponding to the time between thelast row of the segmentand the next laboratory earthquake.File descriptionstrain.csv- A single, continuous training segment of experimental data.test- A folder containing many small segments of test data.sample_sumbission.csv- A sample submission file in the correct format.Data fieldsacoustic_data- the seismic signal [int16]time_to_failure- the time (in seconds) until the next laboratory earthquake [float64]seg_id- the test segment ids for which predictions should be made (one prediction per segment)Files2626 filesSize10.42 GBTypecsvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "forecasting",
          "time_series",
          "signal processing",
          "geophysics",
          "mae"
        ]
      }
    },
    {
      "challenge_name": "ashrae-energy-prediction",
      "description": "Challenge description:\n# ASHRAE - Great Energy Predictor III\n\n## Competition Description\n\nQ: How much does it cost to cool a skyscraper in the summer?\nA: A lot! And not just in dollars, but in environmental impact.\n\nThankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That's where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don't work with different building types.\n\nIn this competition, you'll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.\n\n## About the Host\n\nFounded in 1894, ASHRAE serves to advance the arts and sciences of heating, ventilation, air conditioning refrigeration and their allied fields. ASHRAE members represent building system design and industrial process professionals around the world. With over 54,000 members serving in 132 countries, ASHRAE supports research, standards writing, publishing and continuing education - shaping tomorrow's built environment today.\n\n## Evaluation\n\n### Evaluation Metric\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as\n$$\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\n\nWhere:\n$\\epsilon$ is the RMSLE value (score)\n$n$ is the total number of observations in the (public/private) data set,\n$p_i$ is your prediction of target, and\n$a_i$ is the actual target for $i$.\n$\\log(x)$ is the natural logarithm of $x$\n\nNote that not all rows will necessarily be scored.\n\n### Notebook Submissions\n\nYou can make submissions directly from Kaggle Notebooks. By adding your teammates as collaborators on a notebook, you can share and edit code privately with them.\n\n### Submission File\n\nFor each id in the test set, you must predict the target variable. The file should contain a header and have the following format:\nid,meter_reading\n0,0\n1,0\n2,0\netc.\n\n## Prizes\n\n1st place - $10,000\n2nd place - $7,000\n3rd place - $5,000\n4th place - $2,000\n5th place - $1,000\n\nBecause this competition is being hosted in coordination with the ASHRAE Organization and its Winter or Annual meetings in 2020, winners will be invited and strongly encouraged to attend the conference, contingent on review of solution and fulfillment of winners' obligations.\n\nNote that in addition to the standard Kaggle Winners' Obligations (open-source licensing requirements, solution packaging/delivery, presentation to host), the host team also asks that you create a short video (under 5 minutes) summarizing your solution.\n\n## Timeline\n\nDecember 12, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nDecember 12, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nDecember 19, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prior Competitions\n\n### History of Great Building Energy Predictor Shootout I and II Competitions\n\nASHRAE has previously hosted two data competitions, called the \"Great Building Energy Predictor Shootout I\" (1993) and \"Great Building Energy Predictor Shootout II\" (1994). If you are interested in seeing how the field has changed over the years, we have rehosted a copy of the first competition here with a leaderboard. It's entirely for fun and does not award points, medals, etc.\n\nFor both of these competitions, participants were asked to develop empirical models for predicting building energy data from data sets and compare how those models could be used to forecast energy usage (Shootout I) and calculate energy conservation retrofit savings (Shootout II).\n\nThe 1994 competition asked participants to retrieve the competition training data set via an FTP server. Teams were required to submit their empirical models along with predictions via floppy disks. The submitted package included predictions of energy savings and a sufficient explanation of how the specific method (calculation method, data removal, etc.) was applied. Submissions using black-box, or proprietary methods, were disqualified. A combination accuracy metric of CV-RMSE (Coefficient of Root Mean Square Error) and MBE (Mean Bias Error) was used to evaluate prediction accuracy.\n\nWhile the 1994 competition awarded no monetary prizes, over 150 teams competed. Six winning teams were formally recognized, all participants were asked to write an ASHRAE paper to document their efforts and to present at ASHRAE conferences. As a part of these efforts, over a dozen peer-reviewed papers were published and several software vendors incorporated the algorithms described in these papers.\n\nHaberl, J. (1994, July 1). Instructions - \"The Great Building Energy Predictor Shootout II: Measuring Retrofit Energy Savings\".\n\n## Acknowledgments\n\nIn addition to the Data Driven Modeling (DDM) Subcommittee of ASHRAE Technical Committee 4.7: Energy Calculations, the following organizations have generously contributed to data collection, travel and resources to host this competition:\n- Singapore Berkeley Building Efficiency and Sustainability in the Tropics\n- BUDS Lab\n- Engineering Experiment Station Texas A&M University\n\n## Citation\n\nAddison Howard, Chris Balbach, Clayton Miller, Jeff Haberl, Krishnan Gowri, and Sohier Dane. ASHRAE - Great Energy Predictor III. https://kaggle.com/competitions/ashrae-energy-prediction, 2019. Kaggle.\n\nData description:\nASHRAE - Great Energy Predictor III\nHow much energy will a building consume?\n\nAssessing the value of energy efficiency improvements can be challenging as there's no way to truly know how much energy a building would have used without the improvements. The best we can do is to build counterfactual models. Once a building is overhauled the new (lower) energy consumption is compared against modeled values for the original building to calculate the savings from the retrofit. More accurate models could support better market incentives and enable lower cost financing.\nThis competition challenges you to build these counterfactual models across four energy types based on historic usage rates and observed weather. The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world.\n\nDataset Description\n\ntrain.csv\nbuilding_id- Foreign key for the building metadata.\nmeter- The meter id code. Read as{0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.\ntimestamp- When the measurement was taken\nmeter_reading- The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error. UPDATE: as discussedhere, the site 0 electric meter readings are in kBTU.\n\nbuilding_meta.csv\nsite_id- Foreign key for the weather files.\nbuilding_id- Foreign key fortraining.csv\nprimary_use- Indicator of the primary category of activities for the building based onEnergyStar property type definitions\nsquare_feet- Gross floor area of the building\nyear_built- Year building was opened\nfloor_count- Number of floors of the building\n\nweather_[train/test].csv\nWeather data from a meteorological station as close as possible to the site.\nsite_id\nair_temperature- Degrees Celsius\ncloud_coverage- Portion of the sky covered in clouds, inoktas\ndew_temperature- Degrees Celsius\nprecip_depth_1_hr- Millimeters\nsea_level_pressure- Millibar/hectopascals\nwind_direction- Compass direction (0-360)\nwind_speed- Meters per second\n\ntest.csv\nThe submission files use row numbers for ID codes in order to save space on the file uploads.\ntest.csvhas no feature data; it exists so you can get your predictions into the correct order.\nrow_id- Row id for your submission file\nbuilding_id- Building id code\nmeter-  The meter id code\ntimestamp- Timestamps for the test data period\n\nsample_submission.csv\nA valid sample submission.\nAll floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload.\n\nThere are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.\n\nFiles6 filesSize2.61 GBTypecsvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/ashrae-energy-prediction",
      "competition_description": "## Competition Description\n\nQ: How much does it cost to cool a skyscraper in the summer?\nA: A lot! And not just in dollars, but in environmental impact.\n\nThankfully, significant investments are being made to improve building efficiencies to reduce costs and emissions. The question is, are the improvements working? That's where you come in. Under pay-for-performance financing, the building owner makes payments based on the difference between their real energy consumption and what they would have used without any retrofits. The latter values have to come from a model. Current methods of estimation are fragmented and do not scale well. Some assume a specific meter type or don't work with different building types.\n\nIn this competition, you'll develop accurate models of metered building energy usage in the following areas: chilled water, electric, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe. With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.",
      "evaluation_metric": "### Evaluation Metric\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as\n$$\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\n\nWhere:\n$\\epsilon$ is the RMSLE value (score)\n$n$ is the total number of observations in the (public/private) data set,\n$p_i$ is your prediction of target, and\n$a_i$ is the actual target for $i$.\n$\\log(x)$ is the natural logarithm of $x$\n\nNote that not all rows will necessarily be scored.",
      "dataset_description": "Dataset Description\n\ntrain.csv\nbuilding_id- Foreign key for the building metadata.\nmeter- The meter id code. Read as{0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types.\ntimestamp- When the measurement was taken\nmeter_reading- The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error. UPDATE: as discussedhere, the site 0 electric meter readings are in kBTU.\n\nbuilding_meta.csv\nsite_id- Foreign key for the weather files.\nbuilding_id- Foreign key fortraining.csv\nprimary_use- Indicator of the primary category of activities for the building based onEnergyStar property type definitions\nsquare_feet- Gross floor area of the building\nyear_built- Year building was opened\nfloor_count- Number of floors of the building\n\nweather_[train/test].csv\nWeather data from a meteorological station as close as possible to the site.\nsite_id\nair_temperature- Degrees Celsius\ncloud_coverage- Portion of the sky covered in clouds, inoktas\ndew_temperature- Degrees Celsius\nprecip_depth_1_hr- Millimeters\nsea_level_pressure- Millibar/hectopascals\nwind_direction- Compass direction (0-360)\nwind_speed- Meters per second\n\ntest.csv\nThe submission files use row numbers for ID codes in order to save space on the file uploads.\ntest.csvhas no feature data; it exists so you can get your predictions into the correct order.\nrow_id- Row id for your submission file\nbuilding_id- Building id code\nmeter-  The meter id code\ntimestamp- Timestamps for the test data period\n\nsample_submission.csv\nA valid sample submission.\nAll floats in the solution file were truncated to four decimal places; we recommend you do the same to save space on your file upload.\n\nThere are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "regression",
          "time_series",
          "feature engineering",
          "building energy",
          "rmsle"
        ]
      }
    },
    {
      "challenge_name": "career-con-2019",
      "description": "Challenge description:\nCompete to get your resume in front of our sponsors  \nCareerCon 2019 is upon us!  \nCareerCon is a digital event all about landing your first data science job \u2014 and registration is now open!  \nAhead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.  \n___________________________________  \nThe Competition  \nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.  \nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).  \nWe\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity.  \nSucceed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.  \nSpecial thanks for making this competition possible:  \nThe data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!  \nEvaluation  \nSubmissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.  \nSubmission File  \nFor each series_id in the test set, you must predict a value for the surface variable. The file should have the following format:  \nseries_id,surface  \n0,fine_concrete  \n1,concrete  \n2,concrete  \netc.  \nTimeline  \nApril 4, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.  \nApril 4, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.  \nApril 11, 2019 - Final submission deadline.  \nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.  \nCareerCon 2019  \nKaggle CareerCon is a totally free, totally digital three-day event that's all about helping new data scientists land their first data science jobs. This year the event will be held from April 16 - 18th. Last year over 17,000 people signed up and this year we expect even more!  \nRegister early for free here.  \nThe full list of speakers will be released soon, but already we've secured top data scientist and hiring managers like:  \nSiraj Raval from The School of AI  \nRosebud Anwuri from Ernst & Young  \nJeff Dean from Google Brain  \nDara Straussman from Stripe  \nIn addition to the seminars, we'll be holding live coding workshops covering practical, professional data science skills lead by Kaggle data scientist, Rachael Tatman. You'll also gain access to the CareerCon Slack Channel where we'll hold a digital career fair for networking and live AMAs with recruiters.  \nPrizes  \nTop 100  \nThe top 100 on the private leaderboard will have the opportunity to send their resumes directly to any hiring company of their choosing that's featured in our 2019 CareerCon Digital Career Fair. These companies are eager to connect with data scientists, and this is a great way to accelerate your job search -- the full list of sponsor companies will be released in the upcoming weeks.  \nTop 10  \nIn addition to the prize above, the top 10 on the private leaderboard will win a Kaggle Swag Pack including a special CareerCon t-shirt and Kaggle sweatshirt, water bottle and stickers!  \nFun Prizes  \nIn addition to the leaderboard prizes, to sweeten the deal, we're also offering a handful of fun prizes.  \nSwag for the top 5 team names \u2013 We\u2019re looking for names that are clever, punny, and pack a punch.  \nSwag for the first submission to beat the benchmark \u2013 Hurry! This should go quickly, we\u2019re expecting no more than 24-hours.  \nWeekly swag for the most intriguing Kernels \u2013 These will be chosen by the Kaggle team. We look for kernels that combine insightful narrative and visualizations along with code that helps others learn from the data. We also consider the impact of your work by looking at the number of upvotes, comments, and forks (copies) made from your code.  \nCitation  \nMaggie and Sohier Dane. CareerCon 2019 - Help Navigate Robots. https://kaggle.com/competitions/career-con-2019, 2019. Kaggle.\n\nData description:\nCareerCon 2019 - Help Navigate Robots\nCompete to get your resume in front of our sponsors.\n\nDataset Description\nX_[train/test].csv- the input data, covering 10 sensor channels and 128 measurements per time series plus three ID columns:\n-row_id: The ID for this row.\n-series_id: ID number for the measurement series. Foreign key to y_train/sample_submission.\n-measurement_number: Measurement number within the series.\nThe orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. The 10 sensor channels are:\norientation_X\norientation_Y\norientation_Z\norientation_W\nangular_velocity_X\nangular_velocity_Y\nangular_velocity_Z\nlinear_acceleration_X\nlinear_acceleration_Y\nlinear_acceleration_Z\ny_train.csv- the surfaces for training set.\n-series_id: ID number for the measurement series.\n-group_id: ID number for all of the measurements taken in a recording session. Provided for the training set only, to enable more cross validation strategies.\n-surface: the target for this competition.\nsample_submission.csv- a sample submission file in the correct format.\n\nSubject to Competition Rules",
      "docker_challenge_path": "/data/career-con-2019",
      "competition_description": "Challenge description:\nCompete to get your resume in front of our sponsors  \nCareerCon 2019 is upon us!  \nCareerCon is a digital event all about landing your first data science job \u2014 and registration is now open!  \nAhead of the event, we have a fun competition to get you started. See below for a unique challenge and opportunity to share your resume with select CareerCon sponsors.  \n___________________________________  \nThe Competition  \nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.  \nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).  \nWe\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity.  \nSucceed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.  \nSpecial thanks for making this competition possible:  \nThe data for this competition has been collected by Heikki Huttunen and Francesco Lomio from the Department of Signal Processing and Damoon Mohamadi, Kaan Celikbilek, Pedram Ghazi and Reza Ghabcheloo from the Department of Automation and Mechanical Engineering both from Tampere University, Finland. We at Kaggle would like thank them all for kindly donating the data that has made this competition possible!",
      "evaluation_metric": "Submissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.",
      "dataset_description": "Data description:\nCareerCon 2019 - Help Navigate Robots\nCompete to get your resume in front of our sponsors.\n\nDataset Description\nX_[train/test].csv- the input data, covering 10 sensor channels and 128 measurements per time series plus three ID columns:\n-row_id: The ID for this row.\n-series_id: ID number for the measurement series. Foreign key to y_train/sample_submission.\n-measurement_number: Measurement number within the series.\nThe orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. The 10 sensor channels are:\norientation_X\norientation_Y\norientation_Z\norientation_W\nangular_velocity_X\nangular_velocity_Y\nangular_velocity_Z\nlinear_acceleration_X\nlinear_acceleration_Y\nlinear_acceleration_Z\ny_train.csv- the surfaces for training set.\n-series_id: ID number for the measurement series.\n-group_id: ID number for all of the measurements taken in a recording session. Provided for the training set only, to enable more cross validation strategies.\n-surface: the target for this competition.\nsample_submission.csv- a sample submission file in the correct format.\n\nSubject to Competition Rules",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "multiclass_classification",
          "time_series_sensor_signal",
          "signal_processing_feature_engineering",
          "robotics",
          "accuracy"
        ]
      }
    },
    {
      "challenge_name": "champs-scalar-coupling",
      "description": "Challenge description:\n# Predicting Molecular Properties\n\n## Competition Overview\n\nThink you can use your data science smarts to make big predictions at a molecular level?\n\nThis challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.\n\nResearchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.\n\nThis competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication.\n\n## Your Challenge\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\nOnce the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution.\n\n## About Scalar Coupling\n\nUsing NMR to gain insight into a molecule's structure and dynamics depends on the ability to accurately predict so-called \"scalar couplings\". These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule's three-dimensional structure.\n\nUsing state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.\n\nA fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.\n\nUltimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.\n\nJoin the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.\n\n## Evaluation\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n$$score = \\frac{1}{T} \\sum_{t=1}^{T} \\log \\left( \\frac{1}{n_{t}} \\sum_{i=1}^{n_t} \\lvert y_i - \\hat{y_i} \\rvert \\right) $$\n\nWhere:\n- $T$ is the number of scalar coupling types\n- $n_{t}$ is the number of observations of type $t$\n- $y_{i}$ is the actual scalar coupling constant for the observation\n- $\\hat{y_i}$ is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.\n\n## Submission File\n\nFor each id in the test set, you must predict the scalar_coupling_constant variable. The file should contain a header and have the following format:\n\nid,scalar_coupling_constant\n4659076,0.0\n4659077,0.0\n4659078,0.0\netc.\n\n## Timeline\n\nAugust 21, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nAugust 21, 2019 - Pre-trained model and external data disclosure deadline. Participants must disclose any external data or pre-trained models used in the official forum thread in adherence with competition rules.\nAugust 21, 2019 - Team merger deadline. This is the last day participants may join or merge teams.\nAugust 28, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\nThe following prizes will be awarded to the winners of the competition:\n1st Place - $12,500\n2nd Place - $7,500\n3rd Place - $5,000\n4th Place - $3,000\n5th Place - $2,000\n\n## Citation\n\nAddison Howard, inversion, and Lars Bratholm. Predicting Molecular Properties. https://kaggle.com/competitions/champs-scalar-coupling, 2019. Kaggle.\n\n## Competition Host\n\nCHAMPS (CHemistry And Mathematics in Phase Space)\n\nData description:\nPredicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nDataset Description\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data.\n\nFiles\ntrain.csv - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates (the corresponding XYZ file is located at ./structures/.xyz), the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (scalar_coupling_constant) is the scalar coupling constant that we want to be able to predict\ntest.csv - the test set; same info as train, without the target variable\nsample_submission.csv - a sample submission file in the correct format\nstructures.zip - folder containing molecular structure (xyz) files, where the first line is the number of atoms in the molecule, followed by a blank line, and then a line for every atom, where the first column contains the atomic element (H for hydrogen, C for carbon etc.) and the remaining columns contain the X, Y and Z cartesian coordinates (a standard format for chemists and molecular visualization programs)\nstructures.csv - this file contains the same information as the individual xyz structure files, but in a single file\n\nAdditional Data\nNOTE: additional data is provided for the molecules in Train only!\ndipole_moments.csv - contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.\nmagnetic_shielding_tensors.csv - contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor/matrix respectively.\nmulliken_charges.csv - contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\npotential_energy.csv - contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.\nscalar_coupling_contributions.csv - The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms. The first column (molecule_name) are the name of the molecule, the second (atom_index_0) and third column (atom_index_1) are the atom indices of the atom-pair, the fourth column indicates the type of coupling, the fifth column (fc) is the Fermi Contact contribution, the sixth column (sd) is the Spin-dipolar contribution, the seventh column (pso) is the Paramagnetic spin-orbit contribution and the eighth column (dso) is the Diamagnetic spin-orbit contribution.\n\nFiles\n130798 files\nSize 1.22 GB\nType xyz, csv\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/champs-scalar-coupling",
      "competition_description": "Challenge description:\n# Predicting Molecular Properties\n\n## Competition Overview\n\nThink you can use your data science smarts to make big predictions at a molecular level?\n\nThis challenge aims to predict interactions between atoms. Imaging technologies like MRI enable us to see and understand the molecular composition of tissues. Nuclear Magnetic Resonance (NMR) is a closely related technology which uses the same principles to understand the structure and dynamics of proteins and molecules.\n\nResearchers around the world conduct NMR experiments to further understanding of the structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, and materials science.\n\nThis competition is hosted by members of the CHemistry and Mathematics in Phase Space (CHAMPS) at the University of Bristol, Cardiff University, Imperial College and the University of Leeds. Winning teams will have an opportunity to partner with this multi-university research program on an academic publication.\n\n## Your Challenge\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\nOnce the competition finishes, CHAMPS would like to invite the top teams to present their work, discuss the details of their models, and work with them to write a joint research publication which discusses an open-source implementation of the solution.\n\n## About Scalar Coupling\n\nUsing NMR to gain insight into a molecule's structure and dynamics depends on the ability to accurately predict so-called \"scalar couplings\". These are effectively the magnetic interactions between a pair of atoms. The strength of this magnetic interaction depends on intervening electrons and chemical bonds that make up a molecule's three-dimensional structure.\n\nUsing state-of-the-art methods from quantum mechanics, it is possible to accurately calculate scalar coupling constants given only a 3D molecular structure as input. However, these quantum mechanics calculations are extremely expensive (days or weeks per molecule), and therefore have limited applicability in day-to-day workflows.\n\nA fast and reliable method to predict these interactions will allow medicinal chemists to gain structural insights faster and cheaper, enabling scientists to understand how the 3D chemical structure of a molecule affects its properties and behavior.\n\nUltimately, such tools will enable researchers to make progress in a range of important problems, like designing molecules to carry out specific cellular tasks, or designing better drug molecules to fight disease.\n\nJoin the CHAMPS Scalar Coupling challenge to apply predictive analytics to chemistry and chemical biology.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n$$score = \\frac{1}{T} \\sum_{t=1}^{T} \\log \\left( \\frac{1}{n_{t}} \\sum_{i=1}^{n_t} \\lvert y_i - \\hat{y_i} \\rvert \\right) $$\n\nWhere:\n- $T$ is the number of scalar coupling types\n- $n_{t}$ is the number of observations of type $t$\n- $y_{i}$ is the actual scalar coupling constant for the observation\n- $\\hat{y_i}$ is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.",
      "dataset_description": "Data description:\nPredicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nDataset Description\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data.\n\nFiles\ntrain.csv - the training set, where the first column (molecule_name) is the name of the molecule where the coupling constant originates (the corresponding XYZ file is located at ./structures/.xyz), the second (atom_index_0) and third column (atom_index_1) is the atom indices of the atom-pair creating the coupling and the fourth column (scalar_coupling_constant) is the scalar coupling constant that we want to be able to predict\ntest.csv - the test set; same info as train, without the target variable\nsample_submission.csv - a sample submission file in the correct format\nstructures.zip - folder containing molecular structure (xyz) files, where the first line is the number of atoms in the molecule, followed by a blank line, and then a line for every atom, where the first column contains the atomic element (H for hydrogen, C for carbon etc.) and the remaining columns contain the X, Y and Z cartesian coordinates (a standard format for chemists and molecular visualization programs)\nstructures.csv - this file contains the same information as the individual xyz structure files, but in a single file\n\nAdditional Data\nNOTE: additional data is provided for the molecules in Train only!\ndipole_moments.csv - contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.\nmagnetic_shielding_tensors.csv - contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor/matrix respectively.\nmulliken_charges.csv - contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\npotential_energy.csv - contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.\nscalar_coupling_contributions.csv - The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms. The first column (molecule_name) are the name of the molecule, the second (atom_index_0) and third column (atom_index_1) are the atom indices of the atom-pair, the fourth column indicates the type of coupling, the fifth column (fc) is the Fermi Contact contribution, the sixth column (sd) is the Spin-dipolar contribution, the seventh column (pso) is the Paramagnetic spin-orbit contribution and the eighth column (dso) is the Diamagnetic spin-orbit contribution.\n\nFiles\n130798 files\nSize 1.22 GB\nType xyz, csv\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "chemistry",
        "keywords": [
          "regression",
          "chemistry",
          "graph_neural_networks",
          "3d-geometry",
          "mae"
        ]
      }
    },
    {
      "challenge_name": "data-science-bowl-2018",
      "description": "Challenge description:\n# 2018 Data Science Bowl: Find the nuclei in divergent images to advance medical discovery\n\n## Description\n\n### Spot Nuclei. Speed Cures.\n\nImagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection.\n\nWe've all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer's, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster.\n\nBy automating nucleus detection, you could help unlock cures faster\u2014from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video.\n\n### Why nuclei?\n\nIdentifying the cells' nuclei is the starting point for most analyses because most of the human body's 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work.\n\nBy participating, teams will work to automate the process of identifying nuclei, which will allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more.\n\n### What will participants do?\n\nTeams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther.\n\nVisit DataScienceBowl.com to:\n- Sign up to receive news about the competition\n- Learn about the history of the Data Science Bowl and past competitions\n- Read our latest insights on emerging analytics techniques\n\n## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$\n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value $t$, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n\n### Submission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nThe metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.\n\nThe file should contain a header and have the following format. Each row in your submission represents a single predicted nucleus segmentation for the given ImageId.\n\nImageId,EncodedPixels\n0114f484a16c152baa2d82fdd43740880a762c93f436c8988ac461c5c9dbe7d5,1\n10999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,1\n10999dab07b11bc85fb8464fc36c947fbd8b5d6ec49817361cb780659ca805eac,2389\netc...\n\nSubmission files may take several minutes to process due to the size.\n\n## Prizes\n\n- 1st place - $50,000 and 1 DGX system to the top placing team (MSRP value $69,000). The DGX system is provided by NVIDIA, a Data Science Bowl sponsor.\n- 2nd place - $25,000\n- 3rd place - $12,000\n- 4th place - $8,000\n- 5th place - $5,000\n\n## Timeline\n\n- April 9th, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\n- April 9th, 2018 - Team merger deadline. This is the last day participants may join or merge teams.\n- April 11, 2018 - Stage one deadline and stage two data release. Your model must be finalized and uploaded to Kaggle by this deadline. After this deadline, the test set is released, the answers to the validation set are released, and participants make predictions on the test set.\n  - PLEASE NOTE: If you do not make a submission during the second stage of the competition, you will not appear on the final competition leaderboard and you will not receive competition ranking points.\n- April 16, 2018 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## About\n\nThe Data Science Bowl, presented by Booz Allen and Kaggle, is the world's premier data science for social good competition.\n\nThe Data Science Bowl brings together data scientists, technologists, domain experts, and organizations to take on the world's challenges with data and technology. It's a platform through which people can harness their passion, unleash their curiosity, and amplify their impact to effect change on a global scale.\n\nTo present the competition, Booz Allen partnered with Kaggle, the leading online data science competition community with over 1 million members around the world. During a 90-day period, participants, either alone or working in teams, gain access to unique data sets to develop algorithms that address a specific challenge. And each year, the competition awards cash prizes to top teams.\n\nIn 2015, participants examined more than 100,000 underwater images, provided by the Hatfield Marine Science Center, to assess ocean health at a massive speed and scale. More than 1,000 teams participated, submitting more than 17,000 solutions to the challenge. The winning team, Team Deep Sea, developed a classification algorithm that beat the current state-of-the-art by more than 10%, achieving human-level performance for some classes.\n\nIn 2016, they applied analytics in cardiology, transforming the practice of assessing heart function. Though the challenge was decidedly more complex than the prior year, this competition received nearly 9,300 submissions from more than 1,100 teams. In fact, the winning team, Tencia Lee and Qi Liu, are hedge fund traders, not traditional data scientists. The National Institutes of Health is further studying results and sharing the winning approaches with the medical and research communities.\n\nIn 2017, nearly 10,000 participants worked to improve lung cancer screening technology, submitting more than 18,000 algorithms. Preliminary results indicate reduction in false positive rate of 10% while improving accuracy by 10% over the state-of-the-art. A follow-on competition is in progress to take the advances from the 2017 Data Science Bowl algorithms from concept to clinic, sponsored by the Bonnie J. Addario Lung Cancer Foundation along with DrivenData.org.\n\nVisit DataScienceBowl.com to learn more about the competition, get insights into emerging analytics techniques, and sign up for news alerts.\n\n### About Booz Allen Hamilton\n\nBooz Allen Hamilton (NYSE: BAH) has been at the forefront of strategy and technology for more than one hundred years. Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering, and innovation expertise. With international headquarters in McLean, Virginia, the firm employs approximately 24,225 people globally, and had revenue of $5.80 billion for the 12 months ended March 31, 2017. To learn more, visit www.boozallen.com.\n\nBooz Allen brings its pioneering work in advanced analytics\u2014and the industry-leading expertise of its more than 600-member data science team\u2014to transform our clients' data into actions that keep them competitive in today's data-driven economy. To learn about Booz Allen's data science capabilities, visit our NextGen Analytics & Data Science page.\n\n### About Kaggle\n\nKaggle is the world's largest online data science community. With more than one million members across 194 countries, the Kaggle community uses its diverse set of academic backgrounds to solve complex data science problems. Working as individuals or in teams, the winning competitors are awarded prizes and industry recognition for their accomplishments.\n\n### Broad Institute\n\nFor the 2018 Data Science Bowl challenge, our non-profit data partner is the Carpenter lab at the Broad Institute of Harvard and MIT. This research group aims to bring advanced computational methods into the hands of biologists and is best known for its open-source software, including CellProfiler, used by thousands of biologists around the world.\n\n## Social Media Contest\n\n### RULES AND GUIDELINES\n\nThe Data Science Bowl Social Media contest is open to all individuals over the age of 18 at the time of entry and to all validly formed legal entities that have not declared or been declared in bankruptcy. The contest will contain three phases, each with a separate mini-challenge and instructions on how to participate. The instructions on how to participate in the first mini-challenge/phase are listed below.\n\n### OVERALL CONTEST OVERVIEW:\n\n**Mini-Challenge #1: February 15 \u2013 28, 3:00 p.m. ET; Prize: Kaggle T-shirt, Data Science Bowl sticker**\n**Mini-Challenge #2: March 1 \u2013 16, 3:00 p.m. ET; Prize: Kaggle T-shirt, Data Science Bowl sticker**\n**Mini-Challenge #3: March 19 \u2013 30, 3:00 p.m. ET; Prize: Kaggle T-shirt, Data Science Bowl sticker**\n\n### Mini-Challenge #1 (February 15-28): HOW TO PARTICIPATE\n\nThis year's Data Science Bowl could change the field of medicine, unlocking cures to everything from cancer to the common cold. Whether you're a competitor, just learning, or a fan, this is a competition to watch! Help us spread the word. Post how you are connected to this year's Data Science Bowl challenge. Use #data4good and #datascibowl on Instagram and/or Twitter and you could win!\n\n### Mini-Challenge #2 (March 1-16): HOW TO PARTICIPATE\n\n(Stay tuned. Will be uploaded prior to competition open date)\n\n### Mini-Challenge #3 (March 19-30): HOW TO PARTICIPATE\n\n(Stay tuned. Will be uploaded prior to competition open date)\n\n### CONTEST CRITERIA:\n\n- Each mini-challenge will have one winner. The winner of each mini-challenge will be selected via a random drawing on the last day of each mini-challenge: February 28 (#1), March 16 (#2), March 30 (#3), 2018. Entries will be closed at 3pm ET on the last day of each mini-challenge/phase of the contest.\n- In order to be eligible for the random drawing, you must use the designated hashtags for each of the respective mini-challenge in which you are participating (e.g., #DataSciBowl & #Data4Good)\n- To be eligible, entries must use appropriate content \u2013 no foul language or trolling, and use only releasable, allowable, and appropriate imagery.\n- We will announce the winner for each phase in a Tweet from @kaggle, and then contact each winner offline for prize award details.\n- We encourage you to participate in each of the 3 phases throughout the contest \u2013 since the winner selection for each is a random drawing, an individual has the potential to win more than one contest!\n\n### ELIGIBILITY NOTE:\n\nMembers of the following organizations are invited to participate in the competition, however, they will not be eligible to win a cash prize: Booz Allen Hamilton, Kaggle, NVIDIA, PerkinElmer, and the Carpenter Lab at the Broad Institute of Harvard and MIT. Additionally, anyone who has previously accessed any of the data sources outside the purview of this competition will not be eligible to win a cash prize. Officers, directors, employees and advisory board members (and their immediate families and members of the same household) of the Competition Sponsor, Kaggle and their respective affiliates, subsidiaries, contractors (with the express exception of Kaggle's authorized Kaggle Community Evangelists), agents, judges and advertising and promotion agencies are not eligible for prizes. Residents of a country designated by the United States Treasury's Office of Foreign Assets Control (see http://www.treasury.gov/resource-center/sanctions/SDN-List/Pages/default.aspx for additional information) are not eligible to receive prizes.\n\n## Citation\n\nAllen Goodman, Anne Carpenter, Elizabeth Park, jlefman-nvidia, Josette_BoozAllen, Kyle, Maggie, Nilofer, Peter Sedivec, and Will Cukierski. 2018 Data Science Bowl. https://kaggle.com/competitions/data-science-bowl-2018, 2018. Kaggle.\n\nData description:\n2018 Data Science Bowl\nFind the nuclei in divergent images to advance medical discovery\nDataset Description\nThis dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.\nEach image is represented by an associatedImageId. Files belonging to an image are contained in a folder with thisImageId. Within this folder are two subfolders:\nimages contains the image file.\nmasks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).\nThe second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details.\nAs with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set.\nFile descriptions\n/stage1_train/* - training set images (images and annotated masks)\n/stage1_test/* - stage 1 test set images (images only, you are predicting the masks)\n/stage2_test/* (released later) - stage 2 test set images (images only, you are predicting the masks)\nstage1_sample_submission.csv - a submission file containing theImageIds for which you must predict during stage 1\nstage2_sample_submission.csv (released later) - a submission file containing theImageIds for which you must predict during stage 2\nstage1_train_labels.csv - a file showing the run-length encoded representation of the training images. This is provided as a convenience and is redundant with the mask image files.\nFiles 7 files Size 385.48 MB Type zip License Subject to Competition Rules",
      "docker_challenge_path": "/data/data-science-bowl-2018",
      "competition_description": "Challenge description:\n# 2018 Data Science Bowl: Find the nuclei in divergent images to advance medical discovery\n\n## Description\n\n### Spot Nuclei. Speed Cures.\n\nImagine speeding up research for almost every disease, from lung cancer and heart disease to rare disorders. The 2018 Data Science Bowl offers our most ambitious mission yet: create an algorithm to automate nucleus detection.\n\nWe've all seen people suffer from diseases like cancer, heart disease, chronic obstructive pulmonary disease, Alzheimer's, and diabetes. Many have seen their loved ones pass away. Think how many lives would be transformed if cures came faster.\n\nBy automating nucleus detection, you could help unlock cures faster\u2014from rare disorders to the common cold. Want a snapshot about the 2018 Data Science Bowl? View this video.\n\n### Why nuclei?\n\nIdentifying the cells' nuclei is the starting point for most analyses because most of the human body's 30 trillion cells contain a nucleus full of DNA, the genetic code that programs each cell. Identifying nuclei allows researchers to identify each individual cell in a sample, and by measuring how cells react to various treatments, the researcher can understand the underlying biological processes at work.\n\nBy participating, teams will work to automate the process of identifying nuclei, which will allow for more efficient drug testing, shortening the 10 years it takes for each new drug to come to market. Check out this video overview to find out more.\n\n### What will participants do?\n\nTeams will create a computer model that can identify a range of nuclei across varied conditions. By observing patterns, asking questions, and building a model, participants will have a chance to push state-of-the-art technology farther.\n\nVisit DataScienceBowl.com to:\n- Sign up to receive news about the competition\n- Learn about the history of the Data Science Bowl and past competitions\n- Read our latest insights on emerging analytics techniques",
      "evaluation_metric": "## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$\n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value $t$, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.",
      "dataset_description": "Data description:\n2018 Data Science Bowl\nFind the nuclei in divergent images to advance medical discovery\nDataset Description\nThis dataset contains a large number of segmented nuclei images. The images were acquired under a variety of conditions and vary in the cell type, magnification, and imaging modality (brightfield vs. fluorescence). The dataset is designed to challenge an algorithm's ability to generalize across these variations.\nEach image is represented by an associatedImageId. Files belonging to an image are contained in a folder with thisImageId. Within this folder are two subfolders:\nimages contains the image file.\nmasks contains the segmented masks of each nucleus. This folder is only included in the training set. Each mask contains one nucleus. Masks are not allowed to overlap (no pixel belongs to two masks).\nThe second stage dataset will contain images from unseen experimental conditions. To deter hand labeling, it will also contain images that are ignored in scoring. The metric used to score this competition requires that your submissions are in run-length encoded format. Please see the evaluation page for details.\nAs with any human-annotated dataset, you may find various forms of errors in the data. You may manually correct errors you find in the training set. The dataset will not be updated/re-released unless it is determined that there are a large number of systematic errors. The masks of the stage 1 test set will be released with the release of the stage 2 test set.\nFile descriptions\n/stage1_train/* - training set images (images and annotated masks)\n/stage1_test/* - stage 1 test set images (images only, you are predicting the masks)\n/stage2_test/* (released later) - stage 2 test set images (images only, you are predicting the masks)\nstage1_sample_submission.csv - a submission file containing theImageIds for which you must predict during stage 1\nstage2_sample_submission.csv (released later) - a submission file containing theImageIds for which you must predict during stage 2\nstage1_train_labels.csv - a file showing the run-length encoded representation of the training images. This is provided as a convenience and is redundant with the mask image files.\nFiles 7 files Size 385.48 MB Type zip License Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "instance_segmentation",
          "images",
          "unet_cnn",
          "biomedical",
          "mean_average_precision"
        ]
      }
    },
    {
      "challenge_name": "digit-recognizer",
      "description": "Challenge description:\n# Digit Recognizer\n\n## Competition Description\n\nStart here if...  \nYou have some experience with R or Python and machine learning basics, but you're new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features.\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \"hello world\" dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We've curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\n### Practice Skills\n- Computer vision fundamentals including simple neural networks\n- Classification methods such as SVM and K-nearest neighbors\n\n### Acknowledgements\nMore details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.\n\n## Tutorial\n\n### Kaggle Learn\nKaggle Learn offers hands-on courses for most data science topics. These short courses prepare you with everything you need to start your own projects, including deep learning and computer vision projects.\n\nThe Deep Learning Course will give you everything you need to succeed in this competition and others like it.\n\n### Other Python Tutorials\n- Deep neural network the Keras way\n  - Covers pre-processing including feature standardization and one-hot encoding\n  - Implements an artificial neural network approach using Keras\n- Simple deep MLP with Keras\n  - A straightforward implementation of MLP (multi-layer perceptron) in Keras\n  - Learn Keras from the author himself, Francois Chollet!\n- An introduction to dimensionality reduction\n  - Introduces and compares PCA, LDA, and t-SNE dimensionality reduction techniques\n  - Uses the Plotly library for intuitive, interactive visualizations\n\n### Other R Tutorials\n- Random forest benchmark\n  - A minimal example implementing the random forest algorithm\n- Build your own neural network in R\n  - Implements a simple 2-layer neural network from scratch\n  - Based on the CS231n course offered by Stanford\n- Minimum distance classifier\n  - Uses minimum distance as a simple approach to classification\n\n## Evaluation\n\n### Goal\nThe goal in this competition is to take an image of a handwritten single digit, and determine what that digit is. For every image in the test set, you should predict the correct label.\n\n### Metric\nThis competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).\n\n### Submission File Format\nThe file should contain a header and have the following format:\n```\nImageId,Label\n1,0\n2,0\n3,0\netc.\n```\n\n## Frequently Asked Questions\n\n### What is a Getting Started competition?\nGetting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.\n\nGetting Started competitions are a non-competitive way to get familiar with Kaggle's platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.\n\n### How do I create and manage a team?\nWhen you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the Team page.\n\nWe've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.\n\n### What are Notebooks?\nKaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Kernels supports scripts in R and Python, Jupyter Notebooks, and RMarkdown reports. Go to the Notebooks tab to view all of the publicly shared code on this competition. For more on how to use Notebooks to learn data science, visit Kaggle's Learn Courses.\n\n### What's the difference between a private and public leaderboard?\nIn this competition, because it is a Getting Started competition, there is no difference. We're scoring the entire test set on the Public Leaderboard. And we will refresh the competition every three months, so the Private Leaderboard is irrelevant.\n\nFor non-Getting Started Kaggle competitions, there is the concept of a public and private leaderboard to prevent participants from \"overfitting\" to the leaderboard. If your model is \"overfit\" to a dataset then it is not generalizable outside of the dataset you trained it on. This means that your model would have low accuracy on another sample of data taken from a similar dataset.\n\n### Why are there perfect scores on the leaderboard?\nThis competition's test set labels are completely public. So it's likely that some participants will submit perfect submissions. Since there are no prizes, medals, or points associated with your leaderboard ranking in this competition, these scores have little consequence. The rankings are purely for the benefit of users to learn and see how their approach is improving.\n\n\"It's a heavily time-intensive process to manage removals from a continuously live leaderboard. The threshold for what should be removed also becomes arbitrary -- if you remove 1.00 scores, then .99 scores will quickly appear. Since there is no prize consequence, we have chosen to leave the scores as an open sandbox.\"\n\n### Why did my team disappear from the leaderboard?\nTo keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.\n\nIf your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.\n\n### How do I contact Support?\nKaggle does not have a dedicated support team so you'll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you'll want to use this competition's discussion forum.)\n\nSupport is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can't find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\nIf your problem persists or it seems to be affecting all participants then please contact us.\n\n## Citation\nAstroDave and Will Cukierski. Digit Recognizer. https://kaggle.com/competitions/digit-recognizer, 2012. Kaggle.\n\n## Competition Details\n- **Competition Host**: Kaggle\n- **Prizes & Awards**: Does not award Points or Medals\n- **Participation**: \n  - 195,005 Entrants\n  - 945 Participants\n  - 945 Teams\n  - 2,751 Submissions\n- **Tags**: Tabular, Image, Multiclass Classification, Categorization Accuracy\n- **Timeline**: This competition runs indefinitely with a rolling leaderboard.\n\nData description:\nDigit Recognizer\nLearn computer vision fundamentals with the famous MNIST data\nGetting Started Prediction Competition Ongoing\n\nDataset Description\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\nVisually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n000 001 002 003 ... 026 027\n028 029 030 031 ... 054 055\n056 057 058 059 ... 082 083\n |   |   |   |  ...  |   |\n728 729 730 731 ... 754 755\n756 757 758 759 ... 782 783\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n\nYour submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\nImageId,Label\n1,3\n2,7\n3,8 \n(27997 more lines)\n\nThe evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.\n\nCC BY-SA 3.0",
      "docker_challenge_path": "/data/digit-recognizer",
      "competition_description": "## Competition Description\n\nStart here if...  \nYou have some experience with R or Python and machine learning basics, but you're new to computer vision. This competition is the perfect introduction to techniques like neural networks using a classic dataset including pre-extracted features.\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \"hello world\" dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n\nIn this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We've curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.\n\n### Practice Skills\n- Computer vision fundamentals including simple neural networks\n- Classification methods such as SVM and K-nearest neighbors\n\n### Acknowledgements\nMore details about the dataset, including algorithms that have been tried on it and their levels of success, can be found at http://yann.lecun.com/exdb/mnist/index.html. The dataset is made available under a Creative Commons Attribution-Share Alike 3.0 license.",
      "evaluation_metric": "### Metric\nThis competition is evaluated on the categorization accuracy of your predictions (the percentage of images you get correct).",
      "dataset_description": "Dataset Description\nThe data files train.csv and test.csv contain gray-scale images of hand-drawn digits, from zero through nine.\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.\nThe training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image.\nEach pixel column in the training set has a name like pixelx, where x is an integer between 0 and 783, inclusive. To locate this pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27, inclusive. Then pixelx is located on row i and column j of a 28 x 28 matrix, (indexing by zero).\nFor example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\nVisually, if we omit the \"pixel\" prefix, the pixels make up the image like this:\n000 001 002 003 ... 026 027\n028 029 030 031 ... 054 055\n056 057 058 059 ... 082 083\n |   |   |   |  ...  |   |\n728 729 730 731 ... 754 755\n756 757 758 759 ... 782 783\n\nThe test data set, (test.csv), is the same as the training set, except that it does not contain the \"label\" column.\n\nYour submission file should be in the following format: For each of the 28000 images in the test set, output a single line containing the ImageId and the digit you predict. For example, if you predict that the first image is of a 3, the second image is of a 7, and the third image is of a 8, then your submission file would look like:\nImageId,Label\n1,3\n2,7\n3,8 \n(27997 more lines)\n\nThe evaluation metric for this contest is the categorization accuracy, or the proportion of test images that are correctly classified. For example, a categorization accuracy of 0.97 indicates that you have correctly classified all but 3% of the images.\n\nCC BY-SA 3.0",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "classification",
          "images",
          "cnn",
          "handwritten_digits",
          "accuracy"
        ]
      }
    },
    {
      "challenge_name": "elo-merchant-category-recommendation",
      "description": "Challenge description:\nElo Merchant Category Recommendation\nHelp understand customer loyalty\n\nImagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key. Elo has built machine learning models to understand the most important aspects and preferences in their customers\u2019 lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers\u2019 lives and help Elo reduce unwanted campaigns, to create the right experience for customers.\n\nEvaluation\nRoot Mean Squared Error (RMSE)\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\\[\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2},\\]\nwhere \\( \\hat{y} \\) is the predicted loyalty score for each card_id, and \\( y \\) is the actual loyalty score assigned to a card_id.\n\nSubmission File\ncard_id, target\nC_ID_9e86007114,0\nC_ID_1c9f77086c,0.5\nC_ID_07b20e9908,0\nC_ID_63d6bac69a,0\nC_ID_bbc26a86eb,0\nC_ID_f749aad790,0\nC_ID_7b5c15ff41,-0.25\nC_ID_ec6b0f2d30,0\nC_ID_0a11e759c5,0\n\nTimeline\nFebruary 19, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nFebruary 19, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nFebruary 19, 2019 - External Data Disclosure deadline. All external data used in the competition must be disclosed in the forums by this date.\nFebruary 26, 2019 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nPrizes\n1st Place - $20,000\n2nd Place - $15,000\n3rd Place - $5,000\n4th Place - $5,000\n5th Place - $5,000\n\nCitation\nAddison Howard, Breno Esteves, and Phil Culliton. Elo Merchant Category Recommendation. https://kaggle.com/competitions/elo-merchant-category-recommendation, 2018. Kaggle.\n\nCompetition Host: Elo\nPrizes & Awards: $50,000\nParticipation: 18,014 Entrants, 4,712 Participants, 4,110 Teams, 81,772 Submissions\nTags: Tabular, Regression, Banking, Root Mean Squared Error\n\nData description:\nElo Merchant Category Recommendation\nHelp understand customer loyalty\n\nNote: All data is simulated and fictitious, and is not real customer data\n\nWhat files do I need?\nYou will need, at a minimum, the train.csv and test.csv files. These contain the card_ids that we'll be using for training and prediction.\nThe historical_transactions.csv and new_merchant_transactions.csv files contain information about each card's transactions. historical_transactions.csv contains up to 3 months' worth of transactions for every card at any of the provided merchant_ids. new_merchant_transactions.csv contains the transactions at new merchants (merchant_ids that this particular card_id has not yet visited) over a period of two months.\nmerchants.csv contains aggregate information for each merchant_id represented in the data set.\n\nWhat should I expect the data format to be?\nThe data is formatted as follows:\ntrain.csv and test.csv contain card_ids and information about the card itself - the first month the card was active, etc. train.csv also contains the target.\nhistorical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv, test.csv, and merchants.csv. They contain information about transactions for each card, as described above.\nmerchants can be joined with the transaction sets to provide additional merchant-level information.\n\nWhat am I predicting?\nYou are predicting a loyalty score for each card_id represented in test.csv and sample_submission.csv.\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\nsample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\nhistorical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\nmerchants.csv - additional information about all merchants / merchant_ids in the dataset.\nnew_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.\n\nData fields\nData field descriptions are provided in Data Dictionary.xlsx.\n\nFiles: 8 files\nSize: 3.1 GB\nType: csv, xlsx\nLicense: Subject to Competition Rules\nData Dictionary.xlsx (17.6 kB)",
      "docker_challenge_path": "/data/elo-merchant-category-recommendation",
      "competition_description": "Challenge description:\nElo Merchant Category Recommendation\nHelp understand customer loyalty\n\nImagine being hungry in an unfamiliar part of town and getting restaurant recommendations served up, based on your personal preferences, at just the right moment. The recommendation comes with an attached discount from your credit card provider for a local place around the corner! Right now, Elo, one of the largest payment brands in Brazil, has built partnerships with merchants in order to offer promotions or discounts to cardholders. But do these promotions work for either the consumer or the merchant? Do customers enjoy their experience? Do merchants see repeat business? Personalization is key. Elo has built machine learning models to understand the most important aspects and preferences in their customers\u2019 lifecycle, from food to shopping. But so far none of them is specifically tailored for an individual or profile. This is where you come in. In this competition, Kagglers will develop algorithms to identify and serve the most relevant opportunities to individuals, by uncovering signal in customer loyalty. Your input will improve customers\u2019 lives and help Elo reduce unwanted campaigns, to create the right experience for customers.",
      "evaluation_metric": "Evaluation\nRoot Mean Squared Error (RMSE)\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\\[\\textrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2},\\]\nwhere \\( \\hat{y} \\) is the predicted loyalty score for each card_id, and \\( y \\) is the actual loyalty score assigned to a card_id.",
      "dataset_description": "Data description:\nElo Merchant Category Recommendation\nHelp understand customer loyalty\n\nNote: All data is simulated and fictitious, and is not real customer data\n\nWhat files do I need?\nYou will need, at a minimum, the train.csv and test.csv files. These contain the card_ids that we'll be using for training and prediction.\nThe historical_transactions.csv and new_merchant_transactions.csv files contain information about each card's transactions. historical_transactions.csv contains up to 3 months' worth of transactions for every card at any of the provided merchant_ids. new_merchant_transactions.csv contains the transactions at new merchants (merchant_ids that this particular card_id has not yet visited) over a period of two months.\nmerchants.csv contains aggregate information for each merchant_id represented in the data set.\n\nWhat should I expect the data format to be?\nThe data is formatted as follows:\ntrain.csv and test.csv contain card_ids and information about the card itself - the first month the card was active, etc. train.csv also contains the target.\nhistorical_transactions.csv and new_merchant_transactions.csv are designed to be joined with train.csv, test.csv, and merchants.csv. They contain information about transactions for each card, as described above.\nmerchants can be joined with the transaction sets to provide additional merchant-level information.\n\nWhat am I predicting?\nYou are predicting a loyalty score for each card_id represented in test.csv and sample_submission.csv.\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\nsample_submission.csv - a sample submission file in the correct format - contains all card_ids you are expected to predict for.\nhistorical_transactions.csv - up to 3 months' worth of historical transactions for each card_id\nmerchants.csv - additional information about all merchants / merchant_ids in the dataset.\nnew_merchant_transactions.csv - two months' worth of data for each card_id containing ALL purchases that card_id made at merchant_ids that were not visited in the historical data.\n\nData fields\nData field descriptions are provided in Data Dictionary.xlsx.\n\nFiles: 8 files\nSize: 3.1 GB\nType: csv, xlsx\nLicense: Subject to Competition Rules\nData Dictionary.xlsx (17.6 kB)",
      "metadata": {
        "domain": "business",
        "keywords": [
          "regression",
          "tabular",
          "feature engineering",
          "banking",
          "rmse"
        ]
      }
    },
    {
      "challenge_name": "gendered-pronoun-resolution",
      "description": "Challenge description:\nGendered Pronoun Resolution\nCan you help end gender bias in pronoun resolution?\n\nPronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.\n\nUnfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers. Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).\n\nIn this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.\n\nPlease cite the original paper if you use GAP in your work:\n@inproceedings{webster2018gap,\n  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},\n  author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},\n  booktitle = {Transactions of the ACL},\n  year =      {2018},\n  pages =     {to appear},\n}\n\nSubmissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is then,\n$$log loss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}),$$\nwhere N is the number of samples in the test set, M is 3,  \\\\(log\\\\) is the natural logarithm, \\\\(y_{ij}\\\\) is 1 if observation \\\\(i\\\\) belongs to class \\\\(j\\\\) and 0 otherwise, and \\\\(p_{ij}\\\\) is the predicted probability that observation \\\\(i\\\\) belongs to class \\\\(j\\\\).\n\nThe submitted probabilities are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\).\n\nYou must submit a csv file with the probabilities that a pronoun refers to A, B, or NEITHER. The order of the rows does not matter. The file must have a header and should look like the following:\nID,A,B,NEITHER\ndevelopment-1,0.33333,0.33333,0.33333\ndevelopment-2,0.33333,0.33333,0.33333\ndevelopment-3,0.33333,0.33333,0.33333\netc.\n\nApril 15, 2019- Entry deadline. You must accept the competition rules before this date in order to compete.\nApril 15, 2019- Team Merger deadline. This is the last day participants may join or merge teams.\nApril 15, 2019- Stage 1 Submission & Model upload deadline*\nApril 16, 2019- Stage 2 begins. New test set uploaded.\nApril 22, 2019- Stage 2 ends & Final submission deadline. Stage 2 final submissions selection must be completed by this date.\nMay 3, 2019- Description Paper Submission deadline.** Winners Obligations deadline.\nMay 24, 2019- Description Paper Reviews and Acceptance announcements.\nJune 7, 2019- Camera-ready Papers deadline.\nAugust 2, 2019- ACL (Association for Computational Linguistics) 2019 Workshop on Gender Bias in Natural Language Processing.\n\n* In order to be eligible for Stage 2, each team's Stage 1 submission must include the 2 selected models zipped & uploaded, via Team -> Your Model, per the Competition Rules. These 2 models should match that which was used to generate the 2 final submissions selected for scoring. Be aware that if you do not select final submission(s) (via 'My Submissions') in stage 2, the platform will auto-select your best-scoring model(s). The deadline for model upload is firmly the end of Stage 1. This requirement is in place to confirm the performance of the uploaded models matches the Stage 2 submission file. For more information on two-stage competitions, please review the Two-Stage Competition FAQ\n\n** All entrants are welcome to submit a paper for the Workshop on Gender Bias for Natural Language Processing as part of the ACL 2019 conference. However, Prize winners are required, as a condition of receipt of the Prize, to write a paper within the required deadlines and to the requirements specified by the Competition Sponsor.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n1st Place - $12,000\n2nd Place - $8,000\n3rd Place - $5,000\n\nJulia Elliott, Kellie Webster, and Will Cukierski. Gendered Pronoun Resolution. https://kaggle.com/competitions/gendered-pronoun-resolution, 2019. Kaggle.\n\nCompetition Host: Google Research\n\nData description:\nGendered Pronoun Resolution\nPair pronouns to their correct entities\n\nIn this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to nameA, nameB, or neither.\n\nUnlike many Kaggle challenges, this competition does not provide an explicit labeled training set. Files are also available on theGAP Dataset Github Repo. Note that the labels for the test set are available on this page. However, your final score and ranking will be determined in stage 2, against a withheld private test set.\n\ntest_stage_1.tsv- the test set data for stage 1\nsample_submission_stage_1.csv- a file showing the correct submission format for stage 1\n\nColumns\nID- Unique identifier for an example (Matches to Id in output file format)\nText- Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\nPronoun- The target pronoun (text)\nPronoun-offsetThe character offset of Pronoun in Text\nA- The first name candidate (text)\nA-offset-  The character offset of name A in Text\nB-  The second name candidate\nB-offset-  The character offset of name B in Text\nURL-  The URL of the source Wikipedia page for the example\n\nFiles\n4 files\nSize\n7.84 MB\nType\ntsv, csv\nLicense\nSubject to Competition Rules",
      "docker_challenge_path": "/data/gendered-pronoun-resolution",
      "competition_description": "Challenge description:\nGendered Pronoun Resolution\nCan you help end gender bias in pronoun resolution?\n\nPronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.\n\nUnfortunately, recent studies have suggested gender bias among state-of-the-art coreference resolvers. Google AI Language aims to improve gender-fairness in modeling by releasing the Gendered Ambiguous Pronouns (GAP) dataset, containing gender-balanced pronouns (50% of its examples containing feminine pronouns, and 50% containing masculine pronouns).\n\nIn this two-stage competition, Kagglers are challenged to build pronoun resolution systems that perform equally well regardless of pronoun gender. Stage two's final evaluation will use a new dataset following the same format. To encourage gender-fair modeling, the ratio of masculine to feminine examples in the official test data will not be known ahead of time.\n\nPlease cite the original paper if you use GAP in your work:\n@inproceedings{webster2018gap,\n  title =     {Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns},\n  author =    {Webster, Kellie and Recasens, Marta and Axelrod, Vera and Baldridge, Jason},\n  booktitle = {Transactions of the ACL},\n  year =      {2018},\n  pages =     {to appear},\n}",
      "evaluation_metric": "Submissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is then,\n$$log loss = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^My_{ij}\\log(p_{ij}),$$\nwhere N is the number of samples in the test set, M is 3,  \\\\(log\\\\) is the natural logarithm, \\\\(y_{ij}\\\\) is 1 if observation \\\\(i\\\\) belongs to class \\\\(j\\\\) and 0 otherwise, and \\\\(p_{ij}\\\\) is the predicted probability that observation \\\\(i\\\\) belongs to class \\\\(j\\\\).\n\nThe submitted probabilities are not required to sum to one because they are rescaled prior to being scored (each row is divided by the row sum). In order to avoid the extremes of the log function, predicted probabilities are replaced with \\\\(max(min(p,1-10^{-15}),10^{-15})\\\\).",
      "dataset_description": "Data description:\nGendered Pronoun Resolution\nPair pronouns to their correct entities\n\nIn this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to nameA, nameB, or neither.\n\nUnlike many Kaggle challenges, this competition does not provide an explicit labeled training set. Files are also available on theGAP Dataset Github Repo. Note that the labels for the test set are available on this page. However, your final score and ranking will be determined in stage 2, against a withheld private test set.\n\ntest_stage_1.tsv- the test set data for stage 1\nsample_submission_stage_1.csv- a file showing the correct submission format for stage 1\n\nColumns\nID- Unique identifier for an example (Matches to Id in output file format)\nText- Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\nPronoun- The target pronoun (text)\nPronoun-offsetThe character offset of Pronoun in Text\nA- The first name candidate (text)\nA-offset-  The character offset of name A in Text\nB-  The second name candidate\nB-offset-  The character offset of name B in Text\nURL-  The URL of the source Wikipedia page for the example\n\nFiles\n4 files\nSize\n7.84 MB\nType\ntsv, csv\nLicense\nSubject to Competition Rules",
      "metadata": {
        "domain": "nlp",
        "keywords": [
          "classification",
          "text",
          "transformer_embedding",
          "coreference-resolution",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "geolifeclef-2024",
      "description": "Challenge description:\n# GeoLifeCLEF 2024 @ LifeCLEF & CVPR-FGVC: Location-based species presence prediction\n\n## Competition Description\n\nThis challenge aims to predict plant species in a given location and time using various possible predictors: satellite images and time series, climatic time series, and other rasterized environmental data: land cover, human footprint, bioclimatic, and soil variables.\n\nPredicting plant species composition and its change in space and time at a fine resolution is useful for many scenarios related to biodiversity management and conservation, improving species identification and inventory tools, and educational purposes.\n\nTo do so, we provide a large-scale training set of about 5M plant occurrences in Europe (single-label, presence-only data) as well as a validation set of about 5K plots and a test set with 20K plots, with all the present species (multi-label, presence-absence data).\n\nThe difficulties of the challenge include multi-label learning from single positive labels, strong class imbalance, multi-modal learning, and large-scale.\n\n## Motivation\n\nPredicting the plant species present at a given location is helpful for many biodiversity management and conservation scenarios.\n\nFirst, it allows for building high-resolution maps of species composition and related biodiversity indicators such as species diversity, endangered species, and invasive species. In scientific ecology, the problem is known as Species Distribution Modelling.\n\nMoreover, it could significantly improve the accuracy of species identification tools - such as Pl@ntNet - by reducing the list of candidate species observable at a given site.\n\nMore generally, it could facilitate biodiversity inventories by developing location-based recommendation services (e.g., on mobile phones), encouraging citizen scientist observers' involvement, and accelerating the annotation and validation of species observations to produce large, high-quality data sets.\n\nFinally, this could be used for educational purposes through biodiversity exploration applications with features such as quests or contextualized educational pathways.\n\n## Timeline\n\n- December 2023: Registration opens for all LifeCLEF challenges (free of charge)\n- February 28, 2024: Training and test data release\n- May 24, 2024: Competition Deadline\n- June 7, 2024: Deadline for submission of working note papers [CEUR-WS proceedings]\n- June 21, 2024: Notification of acceptance - working note papers [CEUR-WS proceedings]\n- July 8, 2024: Camera-ready deadline for working note papers\n- September 9-12, 2024: CLEF 2024 Grenoble - France\n\nAll deadlines are at 11:59 PM CET of the corresponding day unless otherwise stated. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Other Resources\n\nBesides this Kaggle page, make sure to check these other resources:\n- GLC GitHub allowing easy access and loading of data\n- Malpolon: A deep learning framework to help participants build their species distribution models\n- LifeCLEF 2024 webpage for more information about LifeCLEF challenges and working notes submission procedure\n- FGVC11 page for more information about the FGVC10 workshop\n- Our protocol note explaining the data sources and procedure used to build the dataset will soon be published and made available on this page, stay tuned\n\nLast editions winning solutions working notes:\n- GeoLifeCLEF 2021 winner, S. Seneviratne\n- GeoLifeCLEF 2022 2nd, B. Kellenberg et T. Devis\n- GeoLifeCLEF 2023 winner, H.Q. Ung et al.\n- Overview paper of GeoLifeCLEF 2023\n\n## CVPR24 and CLEF24 Context\n\nThis competition is held jointly as part of:\n- the LifeCLEF 2024 lab of the CLEF 2024 conference, and of\n- the FGVC11 workshop organized in conjunction with CVPR 2024 conference\n\nBeing part of scientific research, the participants are encouraged to participate to both events. In particular, only participants who submitted a working note paper to LifeCLEF (see below) will be part of the officially published ranking used for scientific communication.\n\n### FGVC11 at CVPR 2024\n\nThis competition is part of the Fine-Grained Visual Categorization FGVC11 workshop on the 18th of June at the Computer Vision and Pattern Recognition Conference CVPR 2024. The task results will be presented at the workshop, and the contribution of the winning team(s) will be highlighted. Attending the workshop is not required to participate in the competition.\n\nCVPR 2024 will take place in Seattle, USA, on June 17-21, 2024. PLEASE NOTE: CVPR frequently sells out early; we cannot guarantee CVPR registration after the competition's end. If you are interested in attending, please plan ahead.\n\nYou can see a list of the FGVC11 competitions here.\n\n### LifeCLEF 2024 at CLEF 2024\n\nLifeCLEF lab is part of the Conference and Labs of the Evaluation Forum (CLEF). CLEF consists of independent peer-reviewed workshops on a broad range of challenges in multilingual and multimodal information access evaluation and benchmarking activities in various labs designed to test different aspects of mono and cross-language Information retrieval systems.\n\nCLEF 2024 will take place in Grenoble, France, on September 9-12, 2024. You can find more details on the CLEF 2024 website.\n\n## Evaluation\n\nThe evaluation metric for this competition is the samples-averaged \\(F_1\\)-score (called F-Score Beta (Micro) on Kaggle) computed on the test set made of species presence-absence (PA) samples. In terms of machine learning, it is a multi-label classification task. The \\(F_1\\)-score is an average measure of overlap between the predicted and actual set of species present at a given location and time.\n\nEach test PA sample \\(i\\) is associated with a set of ground-truth labels \\(Y_i\\), namely the set of plant species (=speciesId) associated with a given combination of the columns patchID and dayOfYear (see the Data tab for details on the species observation data structure).\n\nFor each sample, the submission will provide a list of labels, i.e. the set of species predicted present \\(\\widehat{Y}_{i,1}, \\widehat{Y}_{i,2}, \\dots, {\\widehat{Y}}_{i,R_i}\\).\n\nThe micro \\(F_1\\)-score is then computed using\n\n\\[\nF_1 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\text{TP}_i}{\\text{TP}_i+(\\text{FP}_i+\\text{FN}_i)/2} \\\\ \\quad \\text{Where} \\begin{cases} \n\\text{TP}_i =\\text{ Number of predicted species truly present, i.e. }|\\widehat{Y}_i \\cap Y_i |\\\\ \n\\text{FP}_i =\\text{ Number of species predicted but absent, i.e. } |\\widehat{Y}_i \\setminus Y_i | \\\\ \n\\text{FN}_i =\\text{ Number of species not predicted but present, i.e. } | Y_i \\setminus \\widehat{Y}_i |\n\\end{cases}\n\\]\n\n### Submission Format\n\nFor each id in the test set, you must predict a set of species that occur at the given location. The file should contain a header and have the following format:\n\n```\nsurveyId,predictions\n1,1 52 1023\n12,78 2011243 13332310 4841\n...\n```\n\nThe submission format is a CSV file containing two columns for each sample (row):\n- surveyId column containing integers corresponding to the test sample ids, corresponding to unique combinations of patchID and dayOfYear column values.\n- predictions column containing space-delimited lists of the predicted species identifiers (column spId in training/validation datasets)\n\nFor each sample (row), the predicted species identifiers must be ordered by increasing the value from left to right. No test sample is empty, and the test set only contains species from the train or validation set.\n\n## Organizers and contributors\n\n- Lukas Picek, INRIA, LIRMM, Montpellier\n- Christophe Botella, INRIA, LIRMM, Montpellier\n- Diego Marcos, INRIA, Montpellier\n- Th\u00e9o Larcher, INRIA, LIRMM, Montpellier\n- Joachim Estopinan, INRIA, LIRMM, Montpellier\n- C\u00e9sar Leblanc, INRIA, LIRMM, Montpellier\n- Maximilien Servajean, Universit\u00e9 Paul Val\u00e9ry, LIRMM, Montpellier\n- Alexis Joly, INRIA, LIRMM, Montpellier\n\n## Acknowledgement\n\nThis project has received funding from the European Union's Horizon Research and Innovation program under grant agreements No. 101060639 (MAMBO project) and No. 101060693 (GUARDEN project).\n\n## Citation\n\nAlexis Joly, C\u00e9sar Leblanc, DZombie, HCL-Jevster, HCL-Rantig, Maximilien Servajean, picekl, and tlarcher. GeoLifeCLEF 2024 @ LifeCLEF & CVPR-FGVC. https://kaggle.com/competitions/geolifeclef-2024, 2024. Kaggle.\n\nData description:\n# GeoLifeCLEF 2024 @ LifeCLEF & CVPR-FGVC: Location-based species presence prediction\n\nThe training data comprises species observations and environmental data. Below, we explain the data in detail.\n\n## Links\n\u2757New Seafile repository\u2757: repository containing all the data. To optimize download times, see the section data downloading at the bottom of this page.\n\u2757GLC GitHub repository\u2757: Useful codes to manipulate data with simple data loaders, examples, and sample data. More dataloaders can be added after the challenge starts.\n\n## Observations data\nThe species related training data comprises:\n\n**Presence-Absence (PA) surveys**: including around 90 thousand surveys with roughly 10,000 species of the European flora. The presence-absence data (PA) is provided to compensate for the problem of false-absences of PO data and calibrate models to avoid associated biases.\n\n**Presence-Only (PO) occurrences**: combines around five million observations from numerous datasets gathered from the Global Biodiversity Information Facility (GBIF, www.gbif.org). This data constitutes the larger piece of the training data and covers all countries of our study area, but it has been sampled opportunistically (without standardized sampling protocol), leading to various sampling biases. The local absence of a species among PO data doesn't mean it is truly absent. An observer might not have reported it because it was difficult to \"see\" it at this time of the year, to identify it as not a monitoring target, or just unattractive.\n\nThere are two CSVs with species occurrence data on the Seafile available for training. The detailed description is provided again on SeaFile in separate ReadME files in relevant folders.\n- The PO metadata are available in PresenceOnlyOccurences/GLC24_PO_metadata_train.csv.\n- The PA metadata are available in PresenceAbsenceSurveys/GLC24_PA_metadata_train.csv.\n\n## Environmental data\nBesides species data, we provide spatialized geographic and environmental data as additional input variables (see Figure 1). More precisely, for each species observation location, we provide:\n\n- Satellite image patches: 3-band (RGB) and 1-band (NIR) 128x128 images at 10m resolution.\n- Satellite time series: Up to 20 years of values for six satellite bands (R, G, B, NIR, SWIR1, and SWIR2).\n- Environmental rasters: Various climatic, pedologic, land use, and human footprint variables at the European scale. We provide scalar values, time-series, and original rasters from which you may extract local 2D images.\n\nThere are three separate folders with the relevant data on the Seafile available for training. The detailed description is provided below and again on SeaFile in separate \"Readme\" files in relevant folders.\n- The Satellite image patches in ./SatellitePatches/.\n- The Satellite time series in ./SatelliteTimeSeries/.\n- The Environmental rasters in ./EnvironmentalRasters/.\n\nFigure. Illustration of the environmental data for an occurrence (glcID=4859165) collected in northern Switzerland (lon=8.5744;lat=47.7704) in 2021. A. The 1280x1280m satellite image patches were sampled in 2021 around the observation. B. Quarterly time series of six satellite bands at the point location since winter 1999-2000. C. Three example bioclimatic images (~65x65km) around the observation were extracted from the provided environmental rasters.\n\n### Satellite image patches\n1280mx1280m RGB and NIR patches (four bands) centered at the observation geolocation and taken the same year. The patches are compressed in two zip files (patchs_rgb.zip, patchs_nir.zip) accessible in folder /SatelliteImages/.\n\nFormat: 128x128 JPEG images, a color JPEG file for RGB data and a grayscale one for Near-Infrared.\nResolution: 10 meters per pixel\nSource: Sentinel2 remote sensing data pre-processed by the Ecodatacube platform\nAccess: First, one must download and decompress the provided zip files. Each JPEG file corresponds to a unique observation location (via \"surveyId\"). To load the RGB or NIR patch for a selected observation, take the \"surveyId\" from any occurrence CSV and load it following this rule --> '\u2026/CD/AB/XXXXABCD.jpeg'. For example, the image location for the surveyId 3018575 is \"./75/85/3018575.jpeg\". For all \"surveyId\" with less than four digits, you can use a similar rule. For a \"surveyId\" 1 is \"./1/1.jpeg\".\n\n### Satellite time series\nEach observation is associated with the time series of the satellite median point values over each season since the winter of 1999 for six satellite bands (R, G, B, NIR, SWIR1, and SWIR2). This data carries a high-resolution local signature of the past 20 years' succession of seasonal vegetation changes, potential extreme natural events (fires), or land use changes.\n\nFormat1: Six CSV files, one per band. The corresponds to the \"surveyId,\" and the columns are the 84 seasons from winter 2000 until autumn 2020.\nFormat2: TimeSeries-Cubes - The above-mentioned CSV aggregated into 3d tensors with axes as BAND, QUARTER, and YEAR.\nResolution: The original satellite data has a resolution of 30m per pixel\nSource: Landsat remote sensing data pre-processed by the Ecodatacube platform\nAccess: /SatelliteTimeSeries/\n\n### Monthly climatic rasters\nFour climatic variables computed monthly (mean, minimum and maximum temperature, and total precipitation) from January 2000 to December 2019, yielding 960 low-resolution rasters covering Europe.\n\nFormat1: CSV files, one per raster referenced through the \"surveyId\".\nFormat2: TimeSeries-Cubes - Above mentioned CSV aggregated into 3d tensors with axis as RASTER-TYPE, YEAR, and MONTH.\nResolution: ~1 kilometer\nSource: Chelsa\nAccess: /EnvironmentalRasters/Climate/Climatic_Monthly_2000-2019\n\n### Environmental rasters\nFor each observation, we provide additional environmental data such as GeoTIFF rasters and scalar values already extracted from the rasters. We provide CSV files, one per band raster type, i.e., Climate, Elevation, Human Footprint, LandCover, and SoilGrids.\n\n**Bioclimatic rasters**: 19 low-resolution rasters covering Europe; commonly used in species distribution modeling. Provided in longitude/latitude coordinates (WGS84).\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: 30 arcsec (~ 1 kilometer)\nSource: CHELSA\nAccess: /EnvironmentalRasters/Climate/BioClimatic_Average_1981-2010\n\n**Soil rasters**: Nine pedologic low-resolution rasters covering Europe. Provided variables describe the soil properties from 5 to 15cm depth and are determinant of plant species distributions. Check the definition.txt file about the provided variables (e.g., pH, clay, organic carbon and nitrogen contents, etc.).\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: ~1 kilometer\nSource: Soilgrids\nAccess: /EnvironmentalRasters/Soilgrids\n\n**Elevation**: High-resolution raster covering Europe.\nFormat: GeoTIFF file with compression, Int16 numeric storage (13.2GB) and CSV file with extracted values.\nResolution: 1 arc second (~30 meter)\nSource: ASTER Global Digital Elevation Model V3\nAccess: /EnvironmentalRasters/Elevation\n\n**Land Cover**: A medium-resolution multi-band land cover raster covering Europe. Each band describes either the land cover class prediction or its confidence under various classifications. We recommend the use of IGBP (17 classes) or LCCS (43 classes) layers, often used in species distribution modeling.\nFormat: GeoTIFF file with compression and CSV file with extracted values.\nResolution: ~500m\nSource: MODIS Terra+Aqua 500m\nAccess: /EnvironmentalRasters/LandCover/\n\n**Human footprint**: Several low-resolution rasters describing human footprint, encapsulating seven pressures on the environment (e.g., nighlight level, population density) induced by human presence and activity, are provided for two time periods, the early 90's (~1993) and late 2000' (~2009). We provide two summary rasters combining all human pressures and two detailed rasters per pressure, which avoid an arbitrary degradation of the original data.\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: ~1km\nSource: Venter et al., 2016\nAccess: The folder /EnvironmentalRasters/HumanFootprint/ contains a readme file with detailed information about this data and two subfolders: summarized/ contains the two summary rasters (for 1993 and 2009) and detailed/ contains the 2*7 single pressure raster.\n\n## Data downloading\nAll PA-related data are already provided through Kaggle; however, all the original rasters are provided on the Seafile. To download the full dataset, we recommend downloading each group separately using zipped archives.\n\nFiles: 374841 files\nSize: 2.83 GB\nType: jpeg, pt, csv\nLicense: CC BY-NC-SA 4.0",
      "docker_challenge_path": "/data/geolifeclef-2024",
      "competition_description": "## Competition Description\n\nThis challenge aims to predict plant species in a given location and time using various possible predictors: satellite images and time series, climatic time series, and other rasterized environmental data: land cover, human footprint, bioclimatic, and soil variables.\n\nPredicting plant species composition and its change in space and time at a fine resolution is useful for many scenarios related to biodiversity management and conservation, improving species identification and inventory tools, and educational purposes.\n\nTo do so, we provide a large-scale training set of about 5M plant occurrences in Europe (single-label, presence-only data) as well as a validation set of about 5K plots and a test set with 20K plots, with all the present species (multi-label, presence-absence data).\n\nThe difficulties of the challenge include multi-label learning from single positive labels, strong class imbalance, multi-modal learning, and large-scale.",
      "evaluation_metric": "## Evaluation\n\nThe evaluation metric for this competition is the samples-averaged \\(F_1\\)-score (called F-Score Beta (Micro) on Kaggle) computed on the test set made of species presence-absence (PA) samples. In terms of machine learning, it is a multi-label classification task. The \\(F_1\\)-score is an average measure of overlap between the predicted and actual set of species present at a given location and time.\n\nEach test PA sample \\(i\\) is associated with a set of ground-truth labels \\(Y_i\\), namely the set of plant species (=speciesId) associated with a given combination of the columns patchID and dayOfYear (see the Data tab for details on the species observation data structure).\n\nFor each sample, the submission will provide a list of labels, i.e. the set of species predicted present \\(\\widehat{Y}_{i,1}, \\widehat{Y}_{i,2}, \\dots, {\\widehat{Y}}_{i,R_i}\\).\n\nThe micro \\(F_1\\)-score is then computed using\n\n\\[\nF_1 = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\text{TP}_i}{\\text{TP}_i+(\\text{FP}_i+\\text{FN}_i)/2} \\\\\\quad \\text{Where} \\begin{cases} \n\\text{TP}_i =\\text{ Number of predicted species truly present, i.e. }|\\widehat{Y}_i \\cap Y_i |\\\\ \n\\text{FP}_i =\\text{ Number of species predicted but absent, i.e. } |\\widehat{Y}_i \\setminus Y_i | \\\\ \n\\text{FN}_i =\\text{ Number of species not predicted but present, i.e. } | Y_i \\setminus \\widehat{Y}_i |\n\\end{cases}\n\\]",
      "dataset_description": "# GeoLifeCLEF 2024 @ LifeCLEF & CVPR-FGVC: Location-based species presence prediction\n\nThe training data comprises species observations and environmental data. Below, we explain the data in detail.\n\n## Links\n\u2757New Seafile repository\u2757: repository containing all the data. To optimize download times, see the section data downloading at the bottom of this page.\n\u2757GLC GitHub repository\u2757: Useful codes to manipulate data with simple data loaders, examples, and sample data. More dataloaders can be added after the challenge starts.\n\n## Observations data\nThe species related training data comprises:\n\n**Presence-Absence (PA) surveys**: including around 90 thousand surveys with roughly 10,000 species of the European flora. The presence-absence data (PA) is provided to compensate for the problem of false-absences of PO data and calibrate models to avoid associated biases.\n\n**Presence-Only (PO) occurrences**: combines around five million observations from numerous datasets gathered from the Global Biodiversity Information Facility (GBIF, www.gbif.org). This data constitutes the larger piece of the training data and covers all countries of our study area, but it has been sampled opportunistically (without standardized sampling protocol), leading to various sampling biases. The local absence of a species among PO data doesn't mean it is truly absent. An observer might not have reported it because it was difficult to \"see\" it at this time of the year, to identify it as not a monitoring target, or just unattractive.\n\nThere are two CSVs with species occurrence data on the Seafile available for training. The detailed description is provided again on SeaFile in separate ReadME files in relevant folders.\n- The PO metadata are available in PresenceOnlyOccurences/GLC24_PO_metadata_train.csv.\n- The PA metadata are available in PresenceAbsenceSurveys/GLC24_PA_metadata_train.csv.\n\n## Environmental data\nBesides species data, we provide spatialized geographic and environmental data as additional input variables (see Figure 1). More precisely, for each species observation location, we provide:\n\n- Satellite image patches: 3-band (RGB) and 1-band (NIR) 128x128 images at 10m resolution.\n- Satellite time series: Up to 20 years of values for six satellite bands (R, G, B, NIR, SWIR1, and SWIR2).\n- Environmental rasters: Various climatic, pedologic, land use, and human footprint variables at the European scale. We provide scalar values, time-series, and original rasters from which you may extract local 2D images.\n\nThere are three separate folders with the relevant data on the Seafile available for training. The detailed description is provided below and again on SeaFile in separate \"Readme\" files in relevant folders.\n- The Satellite image patches in ./SatellitePatches/.\n- The Satellite time series in ./SatelliteTimeSeries/.\n- The Environmental rasters in ./EnvironmentalRasters/.\n\nFigure. Illustration of the environmental data for an occurrence (glcID=4859165) collected in northern Switzerland (lon=8.5744;lat=47.7704) in 2021. A. The 1280x1280m satellite image patches were sampled in 2021 around the observation. B. Quarterly time series of six satellite bands at the point location since winter 1999-2000. C. Three example bioclimatic images (~65x65km) around the observation were extracted from the provided environmental rasters.\n\n### Satellite image patches\n1280mx1280m RGB and NIR patches (four bands) centered at the observation geolocation and taken the same year. The patches are compressed in two zip files (patchs_rgb.zip, patchs_nir.zip) accessible in folder /SatelliteImages/.\n\nFormat: 128x128 JPEG images, a color JPEG file for RGB data and a grayscale one for Near-Infrared.\nResolution: 10 meters per pixel\nSource: Sentinel2 remote sensing data pre-processed by the Ecodatacube platform\nAccess: First, one must download and decompress the provided zip files. Each JPEG file corresponds to a unique observation location (via \"surveyId\"). To load the RGB or NIR patch for a selected observation, take the \"surveyId\" from any occurrence CSV and load it following this rule --> '\u2026/CD/AB/XXXXABCD.jpeg'. For example, the image location for the surveyId 3018575 is \"./75/85/3018575.jpeg\". For all \"surveyId\" with less than four digits, you can use a similar rule. For a \"surveyId\" 1 is \"./1/1.jpeg\".\n\n### Satellite time series\nEach observation is associated with the time series of the satellite median point values over each season since the winter of 1999 for six satellite bands (R, G, B, NIR, SWIR1, and SWIR2). This data carries a high-resolution local signature of the past 20 years' succession of seasonal vegetation changes, potential extreme natural events (fires), or land use changes.\n\nFormat1: Six CSV files, one per band. The corresponds to the \"surveyId,\" and the columns are the 84 seasons from winter 2000 until autumn 2020.\nFormat2: TimeSeries-Cubes - The above-mentioned CSV aggregated into 3d tensors with axes as BAND, QUARTER, and YEAR.\nResolution: The original satellite data has a resolution of 30m per pixel\nSource: Landsat remote sensing data pre-processed by the Ecodatacube platform\nAccess: /SatelliteTimeSeries/\n\n### Monthly climatic rasters\nFour climatic variables computed monthly (mean, minimum and maximum temperature, and total precipitation) from January 2000 to December 2019, yielding 960 low-resolution rasters covering Europe.\n\nFormat1: CSV files, one per raster referenced through the \"surveyId\".\nFormat2: TimeSeries-Cubes - Above mentioned CSV aggregated into 3d tensors with axis as RASTER-TYPE, YEAR, and MONTH.\nResolution: ~1 kilometer\nSource: Chelsa\nAccess: /EnvironmentalRasters/Climate/Climatic_Monthly_2000-2019\n\n### Environmental rasters\nFor each observation, we provide additional environmental data such as GeoTIFF rasters and scalar values already extracted from the rasters. We provide CSV files, one per band raster type, i.e., Climate, Elevation, Human Footprint, LandCover, and SoilGrids.\n\n**Bioclimatic rasters**: 19 low-resolution rasters covering Europe; commonly used in species distribution modeling. Provided in longitude/latitude coordinates (WGS84).\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: 30 arcsec (~ 1 kilometer)\nSource: CHELSA\nAccess: /EnvironmentalRasters/Climate/BioClimatic_Average_1981-2010\n\n**Soil rasters**: Nine pedologic low-resolution rasters covering Europe. Provided variables describe the soil properties from 5 to 15cm depth and are determinant of plant species distributions. Check the definition.txt file about the provided variables (e.g., pH, clay, organic carbon and nitrogen contents, etc.).\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: ~1 kilometer\nSource: Soilgrids\nAccess: /EnvironmentalRasters/Soilgrids\n\n**Elevation**: High-resolution raster covering Europe.\nFormat: GeoTIFF file with compression, Int16 numeric storage (13.2GB) and CSV file with extracted values.\nResolution: 1 arc second (~30 meter)\nSource: ASTER Global Digital Elevation Model V3\nAccess: /EnvironmentalRasters/Elevation\n\n**Land Cover**: A medium-resolution multi-band land cover raster covering Europe. Each band describes either the land cover class prediction or its confidence under various classifications. We recommend the use of IGBP (17 classes) or LCCS (43 classes) layers, often used in species distribution modeling.\nFormat: GeoTIFF file with compression and CSV file with extracted values.\nResolution: ~500m\nSource: MODIS Terra+Aqua 500m\nAccess: /EnvironmentalRasters/LandCover/\n\n**Human footprint**: Several low-resolution rasters describing human footprint, encapsulating seven pressures on the environment (e.g., nighlight level, population density) induced by human presence and activity, are provided for two time periods, the early 90's (~1993) and late 2000' (~2009). We provide two summary rasters combining all human pressures and two detailed rasters per pressure, which avoid an arbitrary degradation of the original data.\nFormat: GeoTIFF files with compression and CSV file with extracted values.\nResolution: ~1km\nSource: Venter et al., 2016\nAccess: The folder /EnvironmentalRasters/HumanFootprint/ contains a readme file with detailed information about this data and two subfolders: summarized/ contains the two summary rasters (for 1993 and 2009) and detailed/ contains the 2*7 single pressure raster.\n\n## Data downloading\nAll PA-related data are already provided through Kaggle; however, all the original rasters are provided on the Seafile. To download the full dataset, we recommend downloading each group separately using zipped archives.\n\nFiles: 374841 files\nSize: 2.83 GB\nType: jpeg, pt, csv\nLicense: CC BY-NC-SA 4.0",
      "metadata": {
        "domain": "geology",
        "keywords": [
          "multi-label classification",
          "images, time_series, tabular",
          "multimodal feature engineering & fusion",
          "geospatial biology",
          "f1 (micro)"
        ]
      }
    },
    {
      "challenge_name": "google-smartphone-decimeter-challenge",
      "description": "Challenge description:\nImprove high precision GNSS positioning and navigation accuracy on smartphones\n\nHave you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.\nGlobal Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a \u201cjumpy\u201d experience. For many use cases the results are not fine nor stable enough to be reliable.\nThis competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them.\nIn this competition, you'll use data collected from the host team\u2019s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions.\nIf successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.\n\nAcknowledgments\nThe Android GPS team would like to show its appreciation to Verizon Hyper Precise Location Service and Swift Navigation Skylark Correction Service who provided assistance data for datasets in the challenge.\n\nEvaluation\nSubmissions are scored on the mean of the 50th and 95th percentile distance errors. For every phone and at every millisSinceGpsEpoch, the horizontal distance (in meters) is computed between the predicted lat/lng and the ground truth lat/lng. These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller). The 50th and 95th percentile errors are then averaged for each phone. Lastly, the mean of these averaged values is calculated across all phones in the test set.\n\nSubmission File\nFor each phone and millisSinceGpsEpoch in the test set, you must predict the latitude and longitude. You must provide estimations for all timestamps that have at least four valid GNSS signals. The sample submission contains a list of these timestamps. The submission file should contain a header and have the following format:\nphone,millisSinceGpsEpoch,latDeg,lngDeg\n2020-05-15-US-MTV-1_Pixel4,1273608785432,53.599227001298125,-2.4339795741464334\n2020-05-15-US-MTV-1_Pixel4,1273608786432,53.599227001298125,-2.4339795741464334\n2020-05-15-US-MTV-1_Pixel4,1273608787432,53.599227001298125,-2.4339795741464334\netc.\n\nTimeline\nMay 12, 2021- Start Date.\nJuly 28, 2021- Entry Deadline. You must accept the competition rules before this date in order to compete.\nJuly 28, 2021- Team Merger Deadline. This is the last day participants may join or merge teams.\nAugust 4, 2021- Final Submission Deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nPrizes\n1st Place - $5,000\n2nd Place - $3,000\n3rd Place - $2,000\nNote that, per the competition rules, there is no requirement for winners to license their solutions. However the host team asks that in lieu of solution-licensing, winners must present their solutions (at a minimum virtually) in the host's dedicated session at the ION GNSS+ 2021 conference. Such attendance is as a condition to earn Prizes, and a portion of the prize money is expected to go towards registration.\n\nCitation\nDave Orendorff, Frank van Diggelen, Julia Elliott, Michael Fu, Mohammed Khider, and Sohier Dane. Google Smartphone Decimeter Challenge. https://kaggle.com/competitions/google-smartphone-decimeter-challenge, 2021. Kaggle.\n\nData description:\n# Google Smartphone Decimeter Challenge\n\nImprove high precision GNSS positioning and navigation accuracy on smartphones\n\nThis challenge provides data from a variety of instruments useful for determining a phone's position: signals from GPS satellites, accelerometer readings, gyroscope readings, and more.\n\nAs this challenge's design is focused on post-processing applications such as lane-level mapping, future data along a route will be available to generate positions as precisely as possible. You may also make use of information from neighboring phones to aid your estimation, as many routes may be represented by multiple phones. In order to encourage the development of a general GNSS positioning algorithm, in-phone GPS chipset locations will not be provided, as they are derived from a manufacturer proprietary algorithm that varies by phone model and other factors.\n\nA detailed explanation of the data collection process can be found in this paper. If publishing work based on this dataset/challenge, please ensure proper citation per the Competition Rules.\n\n## Files\n\n[train]/[drive_id]/[phone_name]/ground_truth.csv - Only provided for the training set. Reference locations at expected timestamps.\n\n[train/test]/[drive_id]/[phone_name]/supplemental/[phone_name][.20o/.21o/.nmea] - Equivalent data to the gnss logs in other formats used by the GPS community.\n\nbaseline_locations_[train/test].csv - Estimated coordinates generated using a simple approach.\n\nground_truth.csv - Reference locations at expected timestamps.\n\nmillisSinceGpsEpoch - An integer number of milliseconds since the GPS epoch (1980/1/6 midnight UTC). Its value equals (Raw::TimeNanos-Raw::FullBiasNanos) / 1000000.0 for each unique epoch described in the Raw sentences.\n\nlatDeg, lngDeg - The WGS84 latitude, longitude (in decimal degrees) estimated by the reference GNSS receiver (NovAtel SPAN). Linear interpolation has been applied where necessary to align the location to the expected non-integer timestamps.\n\nheightAboveWgs84EllipsoidM - The height above the WGS84 ellipsoid (in meters) estimated by the reference GNSS receiver.\n\ntimeSinceFirstFixSeconds - The elapsed time (in seconds) since the first location fix.\n\nhDop - Horizontal dilution of precision DOP, from the GGA sentence, describes how errors in the measurements affect the final horizontal position estimation.\n\nvDop - Vertical dilution of precision DOP, from the GSA sentence, describes how errors in the measurements affect the final vertical position estimation.\n\nspeedMps - The speed over ground in meters per second.\n\ncourseDegree - The course angle clockwise with respect to the truth north over ground (in degrees).\n\n[train/test]/[drive_id]/[phone_name]/[phone_name]_derived.csv - GNSS intermediate values derived from raw GNSS measurements, provided for convenience.\n\nWith these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite) can be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch.\n\ncollectionName - The name of the \"grand\" parent folder.\n\nphoneName - The name of the parent folder.\n\nmillisSinceGpsEpoch - An integer number of milliseconds since the GPS epoch (1980/1/6 midnight UTC).\n\nconstellationType: GNSS constellation type. An integer number, whose mapping string value is provided in constellation_type_mapping.csv.\n\nsvid - The satellite ID.\n\nsignalType - The GNSS signal type is a combination of the constellation name and the frequency band. Common signal types measured by smartphones include: GPS_L1, GPS_L5, GAL_E1, GAL_E5A, GLO_G1, BDS_B1I, BDS_B1C, BDS_B2A, QZS_J1, and QZS_J5.\n\nreceivedSvTimeInGpsNanos - The signal transmission time received by the chipset, in the numbers of nanoseconds since the GPS epoch. Converted from ReceivedSvTimeNanos, this derived value is in a unified time scale for all constellations, while ReceivedSvTimeNanos refers to the time of day for GLONASS and the time of week for non-GLONASS constellations.\n\n[x/y/z]SatPosM - The satellite position (meters) in an ECEF coordinate frame at best estimate of \"true signal transmission time\" defined as ttx = receivedSvTimeInGpsNanos - satClkBiasNanos (defined below). They are computed with the satellite broadcast ephemeris, and have ~1-meter error with respect to the true satellite position.\n\n[x/y/z]SatVelMps - The satellite velocity (meters per second) in an ECEF coordinate frame at the signal transmission time (receivedSvTimeInGpsNanos). They are computed with the satellite broadcast ephemeris, with this algorithm.\n\nsatClkBiasM - The satellite time correction combined with hardware delay in meters at the signal transmission time (receivedSvTimeInGpsNanos). Its time equivalent is termed as satClkBiasNanos. satClkBiasNanos equals the satelliteTimeCorrection minus the satelliteHardwareDelay. As defined in IS-GPS-200H Section 20.3.3.3.3.1, satelliteTimeCorrection is calculated from \u2206tsv = af0 + af1(t - toc) + af2(t - toc)2 + \u2206tr, while satelliteHardwareDelay is a term defined in Section 20.3.3.3.3.2. Parameters in equations above are provided on the satellite broadcast ephemeris.\n\nsatClkDriftMps - The satellite clock drift in meters per second at the signal transmission time (receivedSvTimeInGpsNanos). It equals the difference of the satellite clock biases at t+0.5s and t-0.5s.\n\nrawPrM - Raw pseudorange in meters. It is the product between the speed of light and the time difference from the signal transmission time (receivedSvTimeInGpsNanos) to the signal arrival time (Raw::TimeNanos-Raw::FullBiasNanos-Raw::BiasNanos).\n\nrawPrUncM - Raw pseudorange uncertainty in meters. It is the product between the speed of light and the ReceivedSvTimeUncertaintyNanos.\n\nisrbM - The Inter-Signal Range Bias (ISRB) in meters from a non-GPS-L1 signal to GPS-L1 signals. For example, when the isrbM of GPS L5 is 1000m, it implies that a GPS L5 pseudorange is 1000m longer than the GPS L1 pseudorange transmitted by the same GPS satellite. It's zero for GPS-L1 signals. ISRB is introduced in the GPS chipset level and estimated as a state in the Weighted Least Squares engine.\n\nionoDelayM - The ionospheric delay in meters, estimated with the Klobuchar model.\n\ntropoDelayM - The tropospheric delay in meters, estimated with the EGNOS model by Nigel Penna, Alan Dodson and W. Chen (2001).\n\n[train/test]/[drive_id]/[phone_name]/[phone_name]_GnssLog.txt - The phone's logs as generated by the GnssLogger App. This notebook demonstrates how to parse the logs.\n\nEach gnss file contains several sub-datasets, each of which is detailed below:\n\nRaw - The raw GNSS measurements of one GNSS signal (each satellite may have 1-2 signals for L5-enabled smartphones), collected from the Android API GnssMeasurement.\n\nutcTimeMillis - Milliseconds since UTC epoch (1970/1/1), converted from GnssClock\n\nTimeNanos - The GNSS receiver internal hardware clock value in nanoseconds.\n\nLeapSecond - The leap second associated with the clock's time.\n\nTimeUncertaintyNanos - The clock's time uncertainty (1-sigma) in nanoseconds.\n\nFullBiasNanos - The difference between hardware clock getTimeNanos() inside GPS receiver and the true GPS time since 0000Z, January 6, 1980, in nanoseconds.\n\nBiasNanos - The clock's sub-nanosecond bias.\n\nBiasUncertaintyNanos - The clock's bias uncertainty (1-sigma) in nanoseconds.\n\nDriftNanosPerSecond - The clock's drift in nanoseconds per second.\n\nDriftUncertaintyNanosPerSecond - The clock's drift uncertainty (1-sigma) in nanoseconds per second.\n\nHardwareClockDiscontinuityCount - Count of hardware clock discontinuities.\n\nSvid - The satellite ID. More info can be found here.\n\nTimeOffsetNanos - The time offset at which the measurement was taken in nanoseconds.\n\nState - Integer signifying sync state of the satellite. Each bit in the integer attributes to a particular state information of the measurement. See the metadata/raw_state_bit_map.json file for the mapping between bits and states.\n\nReceivedSvTimeNanos - The received GNSS satellite time, at the measurement time, in nanoseconds.\n\nReceivedSvTimeUncertaintyNanos - The error estimate (1-sigma) for the received GNSS time, in nanoseconds.\n\nCn0DbHz - The carrier-to-noise density in dB-Hz.\n\nPseudorangeRateMetersPerSecond - The pseudorange rate at the timestamp in m/s.\n\nPseudorangeRateUncertaintyMetersPerSecond - The pseudorange's rate uncertainty (1-sigma) in m/s.\n\nAccumulatedDeltaRangeState - This indicates the state of the 'Accumulated Delta Range' measurement. Each bit in the integer attributes to state of the measurement. See the metadata/accumulated_delta_range_state_bit_map.json file for the mapping between bits and states.\n\nAccumulatedDeltaRangeMeters - The accumulated delta range since the last channel reset, in meters.\n\nAccumulatedDeltaRangeUncertaintyMeters - The accumulated delta range's uncertainty (1-sigma) in meters.\n\nCarrierFrequencyHz - The carrier frequency of the tracked signal.\n\nCarrierCycles - The number of full carrier cycles between the satellite and the receiver. Null in these datasets.\n\nCarrierPhase - The RF phase detected by the receiver. Null in these datasets.\n\nCarrierPhaseUncertainty - The carrier-phase's uncertainty (1-sigma). Null in these datasets.\n\nMultipathIndicator - A value indicating the 'multipath' state of the event.\n\nSnrInDb - The (post-correlation & integration) Signal-to-Noise ratio (SNR) in dB.\n\nConstellationType - GNSS constellation type. It's an integer number, whose mapping to string value is provided in the constellation_type_mapping.csv file.\n\nAgcDb - The Automatic Gain Control level in dB.\n\nBasebandCn0DbHz - The baseband carrier-to-noise density in dB-Hz. Only available in Android 11.\n\nFullInterSignalBiasNanos - The GNSS measurement's inter-signal bias in nanoseconds with sub-nanosecond accuracy. Only available in Pixel 5 logs in 2021. Only available in Android 11.\n\nFullInterSignalBiasUncertaintyNanos - The GNSS measurement's inter-signal bias uncertainty (1 sigma) in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nSatelliteInterSignalBiasNanos - The GNSS measurement's satellite inter-signal bias in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nSatelliteInterSignalBiasUncertaintyNanos - The GNSS measurement's satellite inter-signal bias uncertainty (1 sigma) in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nCodeType - The GNSS measurement's code type. Only available in recent logs.\n\nChipsetElapsedRealtimeNanos - The elapsed real-time of this clock since system boot, in nanoseconds. Only available in recent logs.\n\nStatus - The status of a GNSS signal, as collected from the Android API GnssStatus.\n\nUnixTimeMillis - Milliseconds since UTC epoch (1970/1/1), reported from the last location changed by GPS provider.\n\nSignalCount - The total number of satellites in the satellite list.\n\nSignalIndex - The index of current signal.\n\nConstellationType - The constellation type of the satellite at the specified index.\n\nSvid - The satellite ID.\n\nCarrierFrequencyHz - The carrier frequency of the signal tracked.\n\nCn0DbHz - The carrier-to-noise density at the antenna of the satellite at the specified index in dB-Hz.\n\nAzimuthDegrees - The azimuth the satellite at the specified index.\n\nElevationDegrees - The elevation of the satellite at the specified index.\n\nUsedInFix - Whether the satellite at the specified index was used in the calculation of the most recent position fix.\n\nHasAlmanacData - Whether the satellite at the specified index has almanac data.\n\nHasEphemerisData - Whether the satellite at the specified index has ephemeris data.\n\nBasebandCn0DbHz - The baseband carrier-to-noise density of the satellite at the specified index in dB-Hz.\n\nUncalAccel - Readings from the uncalibrated accelerometer, as collected from the Android API Sensor#TYPE_ACCELEROMETER_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalAccel[X/Y/Z]Mps2 - [x/y/z]_uncalib without bias compensation.\n\nBias[X/Y/Z]Mps2 - Estimated [x/y/z]_bias. Null in datasets collected in earlier dates.\n\nUncalGyro - Readings from the uncalibrated gyroscope, as collected from the Android API Sensor#TYPE_GYROSCOPE_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalGyro[X/Y/Z]RadPerSec - Angular speed (w/o drift compensation) around the [X/Y/Z] axis in rad/s.\n\nDrift[X/Y/Z]RadPerSec - Estimated drift around [X/Y/Z] axis in rad/s. Null in datasets collected in earlier dates.\n\nUncalMag - Readings from the uncalibrated magnetometer as collected from the Android API Sensor#STRING_TYPE_MAGNETIC_FIELD_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalMag[X/Y/Z]MicroT - [x/y/z]_uncalib without bias compensation.\n\nBias[X/Y/Z]MicroT - Estimated [x/y/z]_bias. Null in datasets collected in earlier dates.\n\nOrientationDeg - Each row represents an estimated device orientation, collected from Android API SensorManager#getOrientation. This message is only available in logs collected since March 2021.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nyawDeg - If the screen is in portrait mode, this value equals the Azimuth degree (modulus to 0\u00b0~360\u00b0). If the screen is in landscape mode, it equals the sum (modulus to 0\u00b0~360\u00b0) of the screen rotation angle (either 90\u00b0 or 270\u00b0) and the Azimuth degree. Azimuth, refers to the angle of rotation about the -z axis. This value represents the angle between the device's y axis and the magnetic north pole.\n\nrollDeg - Roll, angle of rotation about the y axis. This value represents the angle between a plane perpendicular to the device's screen and a plane perpendicular to the ground.\n\npitchDeg - Pitch, angle of rotation about the x axis. This value represents the angle between a plane parallel to the device's screen and a plane parallel to the ground.\n\nFiles: 515 files\nSize: 12.68 GB\nType: csv, txt, nmea + 3 others\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/google-smartphone-decimeter-challenge",
      "competition_description": "Challenge description:\nImprove high precision GNSS positioning and navigation accuracy on smartphones\n\nHave you ever hit a surprise pothole or other road obstruction? Do you wish your navigation app could provide more precise location or lane-level accuracy? These and other novel features are powered by smartphone positioning services. Machine learning and precision GNSS algorithms are expected to improve this accuracy and provide billions of Android phone users with a more fine-tuned positioning experience.\nGlobal Navigation Satellite System (GNSS) provides raw signals, which the GPS chipset uses to compute a position. Current mobile phones only offer 3-5 meters of positioning accuracy. While useful in many cases, it can create a \u201cjumpy\u201d experience. For many use cases the results are not fine nor stable enough to be reliable.\nThis competition, hosted by the Android GPS team, is being presented at the ION GNSS+ 2021 Conference. They seek to advance research in smartphone GNSS positioning accuracy and help people better navigate the world around them.\nIn this competition, you'll use data collected from the host team\u2019s own Android phones to compute location down to decimeter or even centimeter resolution, if possible. You'll have access to precise ground truth, raw GPS measurements, and assistance data from nearby GPS stations, in order to train and test your submissions.\nIf successful, you'll help produce more accurate positions, bridging the connection between the geospatial information of finer human behavior and mobile internet with much finer granularity. Mobile users could gain better lane-level coordinates, enhanced experience in location-based gaming, and greater specificity in the location of road safety issues. You may even notice it's easier to get you where you need to go.\n",
      "evaluation_metric": "Evaluation\nSubmissions are scored on the mean of the 50th and 95th percentile distance errors. For every phone and at every millisSinceGpsEpoch, the horizontal distance (in meters) is computed between the predicted lat/lng and the ground truth lat/lng. These distance errors form a distribution from which the 50th and 95th percentile errors are calculated (i.e. the 95th percentile error is the value, in meters, for which 95% of the distance errors are smaller). The 50th and 95th percentile errors are then averaged for each phone. Lastly, the mean of these averaged values is calculated across all phones in the test set.\n",
      "dataset_description": "Data description:\n# Google Smartphone Decimeter Challenge\n\nImprove high precision GNSS positioning and navigation accuracy on smartphones\n\nThis challenge provides data from a variety of instruments useful for determining a phone's position: signals from GPS satellites, accelerometer readings, gyroscope readings, and more.\n\nAs this challenge's design is focused on post-processing applications such as lane-level mapping, future data along a route will be available to generate positions as precisely as possible. You may also make use of information from neighboring phones to aid your estimation, as many routes may be represented by multiple phones. In order to encourage the development of a general GNSS positioning algorithm, in-phone GPS chipset locations will not be provided, as they are derived from a manufacturer proprietary algorithm that varies by phone model and other factors.\n\nA detailed explanation of the data collection process can be found in this paper. If publishing work based on this dataset/challenge, please ensure proper citation per the Competition Rules.\n\n## Files\n\n[train]/[drive_id]/[phone_name]/ground_truth.csv - Only provided for the training set. Reference locations at expected timestamps.\n\n[train/test]/[drive_id]/[phone_name]/supplemental/[phone_name][.20o/.21o/.nmea] - Equivalent data to the gnss logs in other formats used by the GPS community.\n\nbaseline_locations_[train/test].csv - Estimated coordinates generated using a simple approach.\n\nground_truth.csv - Reference locations at expected timestamps.\n\nmillisSinceGpsEpoch - An integer number of milliseconds since the GPS epoch (1980/1/6 midnight UTC). Its value equals (Raw::TimeNanos-Raw::FullBiasNanos) / 1000000.0 for each unique epoch described in the Raw sentences.\n\nlatDeg, lngDeg - The WGS84 latitude, longitude (in decimal degrees) estimated by the reference GNSS receiver (NovAtel SPAN). Linear interpolation has been applied where necessary to align the location to the expected non-integer timestamps.\n\nheightAboveWgs84EllipsoidM - The height above the WGS84 ellipsoid (in meters) estimated by the reference GNSS receiver.\n\ntimeSinceFirstFixSeconds - The elapsed time (in seconds) since the first location fix.\n\nhDop - Horizontal dilution of precision DOP, from the GGA sentence, describes how errors in the measurements affect the final horizontal position estimation.\n\nvDop - Vertical dilution of precision DOP, from the GSA sentence, describes how errors in the measurements affect the final vertical position estimation.\n\nspeedMps - The speed over ground in meters per second.\n\ncourseDegree - The course angle clockwise with respect to the truth north over ground (in degrees).\n\n[train/test]/[drive_id]/[phone_name]/[phone_name]_derived.csv - GNSS intermediate values derived from raw GNSS measurements, provided for convenience.\n\nWith these derived values, a corrected pseudorange (i.e. a closer approximation to the geometric range from the phone to the satellite) can be computed as: correctedPrM = rawPrM + satClkBiasM - isrbM - ionoDelayM - tropoDelayM. The baseline locations are computed using correctedPrM and the satellite positions, using a standard Weighted Least Squares (WLS) solver, with the phone's position (x, y, z), clock bias (t), and isrbM for each unique signal type as states for each epoch.\n\ncollectionName - The name of the \"grand\" parent folder.\n\nphoneName - The name of the parent folder.\n\nmillisSinceGpsEpoch - An integer number of milliseconds since the GPS epoch (1980/1/6 midnight UTC).\n\nconstellationType: GNSS constellation type. An integer number, whose mapping string value is provided in constellation_type_mapping.csv.\n\nsvid - The satellite ID.\n\nsignalType - The GNSS signal type is a combination of the constellation name and the frequency band. Common signal types measured by smartphones include: GPS_L1, GPS_L5, GAL_E1, GAL_E5A, GLO_G1, BDS_B1I, BDS_B1C, BDS_B2A, QZS_J1, and QZS_J5.\n\nreceivedSvTimeInGpsNanos - The signal transmission time received by the chipset, in the numbers of nanoseconds since the GPS epoch. Converted from ReceivedSvTimeNanos, this derived value is in a unified time scale for all constellations, while ReceivedSvTimeNanos refers to the time of day for GLONASS and the time of week for non-GLONASS constellations.\n\n[x/y/z]SatPosM - The satellite position (meters) in an ECEF coordinate frame at best estimate of \"true signal transmission time\" defined as ttx = receivedSvTimeInGpsNanos - satClkBiasNanos (defined below). They are computed with the satellite broadcast ephemeris, and have ~1-meter error with respect to the true satellite position.\n\n[x/y/z]SatVelMps - The satellite velocity (meters per second) in an ECEF coordinate frame at the signal transmission time (receivedSvTimeInGpsNanos). They are computed with the satellite broadcast ephemeris, with this algorithm.\n\nsatClkBiasM - The satellite time correction combined with hardware delay in meters at the signal transmission time (receivedSvTimeInGpsNanos). Its time equivalent is termed as satClkBiasNanos. satClkBiasNanos equals the satelliteTimeCorrection minus the satelliteHardwareDelay. As defined in IS-GPS-200H Section 20.3.3.3.3.1, satelliteTimeCorrection is calculated from \u2206tsv = af0 + af1(t - toc) + af2(t - toc)2 + \u2206tr, while satelliteHardwareDelay is a term defined in Section 20.3.3.3.3.2. Parameters in equations above are provided on the satellite broadcast ephemeris.\n\nsatClkDriftMps - The satellite clock drift in meters per second at the signal transmission time (receivedSvTimeInGpsNanos). It equals the difference of the satellite clock biases at t+0.5s and t-0.5s.\n\nrawPrM - Raw pseudorange in meters. It is the product between the speed of light and the time difference from the signal transmission time (receivedSvTimeInGpsNanos) to the signal arrival time (Raw::TimeNanos-Raw::FullBiasNanos-Raw::BiasNanos).\n\nrawPrUncM - Raw pseudorange uncertainty in meters. It is the product between the speed of light and the ReceivedSvTimeUncertaintyNanos.\n\nisrbM - The Inter-Signal Range Bias (ISRB) in meters from a non-GPS-L1 signal to GPS-L1 signals. For example, when the isrbM of GPS L5 is 1000m, it implies that a GPS L5 pseudorange is 1000m longer than the GPS L1 pseudorange transmitted by the same GPS satellite. It's zero for GPS-L1 signals. ISRB is introduced in the GPS chipset level and estimated as a state in the Weighted Least Squares engine.\n\nionoDelayM - The ionospheric delay in meters, estimated with the Klobuchar model.\n\ntropoDelayM - The tropospheric delay in meters, estimated with the EGNOS model by Nigel Penna, Alan Dodson and W. Chen (2001).\n\n[train/test]/[drive_id]/[phone_name]/[phone_name]_GnssLog.txt - The phone's logs as generated by the GnssLogger App. This notebook demonstrates how to parse the logs.\n\nEach gnss file contains several sub-datasets, each of which is detailed below:\n\nRaw - The raw GNSS measurements of one GNSS signal (each satellite may have 1-2 signals for L5-enabled smartphones), collected from the Android API GnssMeasurement.\n\nutcTimeMillis - Milliseconds since UTC epoch (1970/1/1), converted from GnssClock\n\nTimeNanos - The GNSS receiver internal hardware clock value in nanoseconds.\n\nLeapSecond - The leap second associated with the clock's time.\n\nTimeUncertaintyNanos - The clock's time uncertainty (1-sigma) in nanoseconds.\n\nFullBiasNanos - The difference between hardware clock getTimeNanos() inside GPS receiver and the true GPS time since 0000Z, January 6, 1980, in nanoseconds.\n\nBiasNanos - The clock's sub-nanosecond bias.\n\nBiasUncertaintyNanos - The clock's bias uncertainty (1-sigma) in nanoseconds.\n\nDriftNanosPerSecond - The clock's drift in nanoseconds per second.\n\nDriftUncertaintyNanosPerSecond - The clock's drift uncertainty (1-sigma) in nanoseconds per second.\n\nHardwareClockDiscontinuityCount - Count of hardware clock discontinuities.\n\nSvid - The satellite ID. More info can be found here.\n\nTimeOffsetNanos - The time offset at which the measurement was taken in nanoseconds.\n\nState - Integer signifying sync state of the satellite. Each bit in the integer attributes to a particular state information of the measurement. See the metadata/raw_state_bit_map.json file for the mapping between bits and states.\n\nReceivedSvTimeNanos - The received GNSS satellite time, at the measurement time, in nanoseconds.\n\nReceivedSvTimeUncertaintyNanos - The error estimate (1-sigma) for the received GNSS time, in nanoseconds.\n\nCn0DbHz - The carrier-to-noise density in dB-Hz.\n\nPseudorangeRateMetersPerSecond - The pseudorange rate at the timestamp in m/s.\n\nPseudorangeRateUncertaintyMetersPerSecond - The pseudorange's rate uncertainty (1-sigma) in m/s.\n\nAccumulatedDeltaRangeState - This indicates the state of the 'Accumulated Delta Range' measurement. Each bit in the integer attributes to state of the measurement. See the metadata/accumulated_delta_range_state_bit_map.json file for the mapping between bits and states.\n\nAccumulatedDeltaRangeMeters - The accumulated delta range since the last channel reset, in meters.\n\nAccumulatedDeltaRangeUncertaintyMeters - The accumulated delta range's uncertainty (1-sigma) in meters.\n\nCarrierFrequencyHz - The carrier frequency of the tracked signal.\n\nCarrierCycles - The number of full carrier cycles between the satellite and the receiver. Null in these datasets.\n\nCarrierPhase - The RF phase detected by the receiver. Null in these datasets.\n\nCarrierPhaseUncertainty - The carrier-phase's uncertainty (1-sigma). Null in these datasets.\n\nMultipathIndicator - A value indicating the 'multipath' state of the event.\n\nSnrInDb - The (post-correlation & integration) Signal-to-Noise ratio (SNR) in dB.\n\nConstellationType - GNSS constellation type. It's an integer number, whose mapping to string value is provided in the constellation_type_mapping.csv file.\n\nAgcDb - The Automatic Gain Control level in dB.\n\nBasebandCn0DbHz - The baseband carrier-to-noise density in dB-Hz. Only available in Android 11.\n\nFullInterSignalBiasNanos - The GNSS measurement's inter-signal bias in nanoseconds with sub-nanosecond accuracy. Only available in Pixel 5 logs in 2021. Only available in Android 11.\n\nFullInterSignalBiasUncertaintyNanos - The GNSS measurement's inter-signal bias uncertainty (1 sigma) in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nSatelliteInterSignalBiasNanos - The GNSS measurement's satellite inter-signal bias in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nSatelliteInterSignalBiasUncertaintyNanos - The GNSS measurement's satellite inter-signal bias uncertainty (1 sigma) in nanoseconds with sub-nanosecond accuracy. Only available in Android 11.\n\nCodeType - The GNSS measurement's code type. Only available in recent logs.\n\nChipsetElapsedRealtimeNanos - The elapsed real-time of this clock since system boot, in nanoseconds. Only available in recent logs.\n\nStatus - The status of a GNSS signal, as collected from the Android API GnssStatus.\n\nUnixTimeMillis - Milliseconds since UTC epoch (1970/1/1), reported from the last location changed by GPS provider.\n\nSignalCount - The total number of satellites in the satellite list.\n\nSignalIndex - The index of current signal.\n\nConstellationType - The constellation type of the satellite at the specified index.\n\nSvid - The satellite ID.\n\nCarrierFrequencyHz - The carrier frequency of the signal tracked.\n\nCn0DbHz - The carrier-to-noise density at the antenna of the satellite at the specified index in dB-Hz.\n\nAzimuthDegrees - The azimuth the satellite at the specified index.\n\nElevationDegrees - The elevation of the satellite at the specified index.\n\nUsedInFix - Whether the satellite at the specified index was used in the calculation of the most recent position fix.\n\nHasAlmanacData - Whether the satellite at the specified index has almanac data.\n\nHasEphemerisData - Whether the satellite at the specified index has ephemeris data.\n\nBasebandCn0DbHz - The baseband carrier-to-noise density of the satellite at the specified index in dB-Hz.\n\nUncalAccel - Readings from the uncalibrated accelerometer, as collected from the Android API Sensor#TYPE_ACCELEROMETER_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalAccel[X/Y/Z]Mps2 - [x/y/z]_uncalib without bias compensation.\n\nBias[X/Y/Z]Mps2 - Estimated [x/y/z]_bias. Null in datasets collected in earlier dates.\n\nUncalGyro - Readings from the uncalibrated gyroscope, as collected from the Android API Sensor#TYPE_GYROSCOPE_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalGyro[X/Y/Z]RadPerSec - Angular speed (w/o drift compensation) around the [X/Y/Z] axis in rad/s.\n\nDrift[X/Y/Z]RadPerSec - Estimated drift around [X/Y/Z] axis in rad/s. Null in datasets collected in earlier dates.\n\nUncalMag - Readings from the uncalibrated magnetometer as collected from the Android API Sensor#STRING_TYPE_MAGNETIC_FIELD_UNCALIBRATED.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nUncalMag[X/Y/Z]MicroT - [x/y/z]_uncalib without bias compensation.\n\nBias[X/Y/Z]MicroT - Estimated [x/y/z]_bias. Null in datasets collected in earlier dates.\n\nOrientationDeg - Each row represents an estimated device orientation, collected from Android API SensorManager#getOrientation. This message is only available in logs collected since March 2021.\n\nutcTimeMillis - The sum of elapsedRealtimeNanos below and the estimated device boot time at UTC, after a recent NTP (Network Time Protocol) sync.\n\nelapsedRealtimeNanos - The time in nanoseconds at which the event happened.\n\nyawDeg - If the screen is in portrait mode, this value equals the Azimuth degree (modulus to 0\u00b0~360\u00b0). If the screen is in landscape mode, it equals the sum (modulus to 0\u00b0~360\u00b0) of the screen rotation angle (either 90\u00b0 or 270\u00b0) and the Azimuth degree. Azimuth, refers to the angle of rotation about the -z axis. This value represents the angle between the device's y axis and the magnetic north pole.\n\nrollDeg - Roll, angle of rotation about the y axis. This value represents the angle between a plane perpendicular to the device's screen and a plane perpendicular to the ground.\n\npitchDeg - Pitch, angle of rotation about the x axis. This value represents the angle between a plane parallel to the device's screen and a plane parallel to the ground.\n\nFiles: 515 files\nSize: 12.68 GB\nType: csv, txt, nmea + 3 others\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "regression",
          "sensor_signal",
          "sensor_fusion",
          "geospatial",
          "percentile_distance_error"
        ]
      }
    },
    {
      "challenge_name": "home-credit-default-risk",
      "description": "Challenge description:\nHome Credit Default Risk\n\nCan you predict how capable each applicant is of repaying a loan?\n\nDescription\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.\n\nEvaluation\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\nSubmission File\nFor each SK_ID_CURR in the test set, you must predict a probability for the TARGET variable. The file should contain a header and have the following format:\nSK_ID_CURR,TARGET\n100001,0.1\n100005,0.9\n100013,0.2\netc.\n\nPrizes\n1st Place - $35,000\n2nd Place - $25,000\n3rd Place - $10,000\n\nTimeline\nMay 17, 2018 - Competition start\nAugust 22, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nAugust 22, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nAugust 29, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nCompetition Host\nHome Credit Group\n\nPrizes & Awards\n$70,000\nAwards Points & Medals\n\nParticipation\n31,511 Entrants\n8,373 Participants\n7,176 Teams\n131,888 Submissions\n\nCitation\nAnna Montoya, inversion, KirillOdintsov, and Martin Kotek. Home Credit Default Risk. https://kaggle.com/competitions/home-credit-default-risk, 2018. Kaggle.\n\nTags\nTabular\nBanking\nArea Under Receiver Operating Characteristic Curve\n\nData description:\nHome Credit Default Risk\nCan you predict how capable each applicant is of repaying a loan?\nDataset Description\napplication_{train|test}.csv\nThis is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\nStatic data for all applications. One row represents one loan in our data sample.\nbureau.csv\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\nFor every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\nbureau_balance.csv\nMonthly balances of previous credits in Credit Bureau.\nThis table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\nPOS_CASH_balance.csv\nMonthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\ncredit_card_balance.csv\nMonthly balance snapshots of previous credit cards that the applicant has with Home Credit.\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\nprevious_application.csv\nAll previous applications for Home Credit loans of clients who have loans in our sample.\nThere is one row for each previous application related to loans in our data sample.\ninstallments_payments.csv\nRepayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\nThere is a) one row for every payment that was made plus b) one row each for missed payment.\nOne row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\nHomeCredit_columns_description.csv\nThis file contains descriptions for the columns in the various data files.\nFiles 10 files\nSize 2.68 GB\nType csv\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/home-credit-default-risk",
      "competition_description": "Challenge description:\nHome Credit Default Risk\n\nCan you predict how capable each applicant is of repaying a loan?\n\nDescription\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.",
      "evaluation_metric": "Evaluation\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.",
      "dataset_description": "Data description:\nHome Credit Default Risk\nCan you predict how capable each applicant is of repaying a loan?\nDataset Description\napplication_{train|test}.csv\nThis is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\nStatic data for all applications. One row represents one loan in our data sample.\nbureau.csv\nAll client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\nFor every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\nbureau_balance.csv\nMonthly balances of previous credits in Credit Bureau.\nThis table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\nPOS_CASH_balance.csv\nMonthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\ncredit_card_balance.csv\nMonthly balance snapshots of previous credit cards that the applicant has with Home Credit.\nThis table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\nprevious_application.csv\nAll previous applications for Home Credit loans of clients who have loans in our sample.\nThere is one row for each previous application related to loans in our data sample.\ninstallments_payments.csv\nRepayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\nThere is a) one row for every payment that was made plus b) one row each for missed payment.\nOne row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\nHomeCredit_columns_description.csv\nThis file contains descriptions for the columns in the various data files.\nFiles 10 files\nSize 2.68 GB\nType csv\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "binary classification",
          "tabular",
          "feature engineering",
          "banking",
          "auc-roc"
        ]
      }
    },
    {
      "challenge_name": "home-data-for-ml-course",
      "description": "Challenge description:\n# Housing Prices Competition for Kaggle Learn Users\n\n## Start here if...\nYou have some experience with R or Python and machine learning basics. This is a perfect competition for data science students who have completed an online course in machine learning and are looking to expand their skill set before trying a featured competition.\n\n## Competition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n## Practice Skills\n- Creative feature engineering\n- Advanced regression techniques like random forest and gradient boosting\n\n## Acknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.\n\n## Evaluation\n### Goal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n\n### Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n### Submission File Format\nThe file should contain a header and have the following format:\n```\nId,SalePrice\n1461,169000.1\n1462,187724.1233\n1463,175221\netc.\n```\nYou can download an example submission file (sample_submission.csv) on the Data page.\n\n## Frequently Asked Questions\n\n### What is a Getting Started competition?\nGetting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.\n\nGetting Started competitions are a non-competitive way to get familiar with Kaggle's platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.\n\n### What's the difference between a private and public leaderboard?\nThe Kaggle leaderboard has a public and private component to prevent participants from \"overfitting\" to the leaderboard. If your model is \"overfit\" to a dataset then it is not generalizable outside of the dataset you trained it on. This means that your model would have low accuracy on another sample of data taken from a similar dataset.\n\n**Public Leaderboard**: For all participants, the same 50% of predictions from the test set are assigned to the public leaderboard. The score you see on the public leaderboard reflects your model's accuracy on this portion of the test set.\n\n**Private Leaderboard**: The other 50% of predictions from the test set are assigned to the private leaderboard. The private leaderboard is not visible to participants until the competition has concluded. At the end of a competition, we will reveal the private leaderboard so you can see your score on the other 50% of the test data. The scores on the private leaderboard are used to determine the competition winners. Getting Started competitions are run on a rolling timeline so the private leaderboard is never revealed.\n\n### How do I create and manage a team?\nWhen you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the More <Team page.\n\nWe've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.\n\n### What are kernels?\nKaggle Kernels is a cloud computational environment that enables reproducible and collaborative analysis. Kernels supports scripts in R and Python, Jupyter Notebooks, and RMarkdown reports. Go to the Kernels tab to view all of the publicly shared code on this competition.\n\n### How do I contact Support?\nKaggle does not have a dedicated support team so you'll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you'll want to use the House Prices discussion forum).\n\nSupport is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can't find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\nIf your problem persists or it seems to be affecting all participants then please contact us.\n\n## Tutorials\n\n### Kaggle Learn\nKaggle Learn offers hands-on courses for most data science topics. These short courses prepare you with the key ideas to build your own projects.\n\nThe Machine Learning Course will give you everything you need to succeed in this competition and others like it.\n\n### Other Tutorials in R\n- **Detailed Exploratory Data Analysis Using R**: Use RMarkdown and popular R packages like data.table, dplyr, and ggplot2. Take an in-depth look at missing values, distributions, and correlations.\n- **Fun with Real Estate Data**: Use Rmarkdown to learn advanced regression techniques like random forests and XGBoost.\n- **XGBoost with Parameter Tuning**: Implement LASSO regression to avoid multicollinearity. Includes linear regression, random forest, and XGBoost models.\n- **Ensemble Modeling: Stack Model Example**: Use \"ensembling\" to combine the predictions of several models. Includes GBM (gradient boosting machine), XGBoost, ranger, and neural net using the caret package.\n- **A Clear Example of Overfitting**: Learn about the dreaded consequences of overfitting data.\n\n### Other Tutorials in Python\n- **Comprehensive Data Exploration with Python**: Understand how variables are distributed and how they interact. Apply different transformations before training machine learning models. Covers both univariate and multivariate approaches. Includes visualizations using matplotlib and seaborn.\n- **House Prices EDA**: Learn to use visualization techniques to study missing data and distributions. Covers both continuous and categorical data. Includes correlation heatmaps, pairplots, and t-SNE.\n- **A Study on Regression Applied to the Ames Dataset**: Demonstrate effective tactics for feature engineering. Explore linear regression with different regularization methods including ridge, LASSO, and ElasticNet using scikit-learn.\n- **Regularized Linear Models**: Build a basic linear model. Try more advanced algorithms including XGBoost and neural nets using Keras.\n\n## Citation\nDanB. Housing Prices Competition for Kaggle Learn Users. https://kaggle.com/competitions/home-data-for-ml-course, 2018. Kaggle.\n\n## Competition Details\n- **Competition Host**: Kaggle\n- **Prizes & Awards**: Does not award Points or Medals\n- **Timeline**: This competition runs indefinitely with a rolling leaderboard.\n- **Participation**: \n  - 215,737 Entrants\n  - 5,597 Participants\n  - 5,593 Teams\n  - 14,544 Submissions\n\nData description:\nHousing Prices Competition for Kaggle Learn Users\nKaggle \u00b7 Getting Started Prediction Competition \u00b7 Ongoing\n\nApply what you learned in the Machine Learning course on Kaggle Learn alongside others in the course.\n\nDataset Description\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\nData fields\nHere's a brief version of what you'll find in the data description file.\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n\nFiles 7 files\nSize 1.15 MB\nType csv, gz, txt\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/home-data-for-ml-course",
      "competition_description": "## Competition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.",
      "evaluation_metric": "### Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)",
      "dataset_description": "Data description:\nHousing Prices Competition for Kaggle Learn Users\nKaggle \u00b7 Getting Started Prediction Competition \u00b7 Ongoing\n\nApply what you learned in the Machine Learning course on Kaggle Learn alongside others in the course.\n\nDataset Description\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms\n\nData fields\nHere's a brief version of what you'll find in the data description file.\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nMSSubClass: The building class\nMSZoning: The general zoning classification\nLotFrontage: Linear feet of street connected to property\nLotArea: Lot size in square feet\nStreet: Type of road access\nAlley: Type of alley access\nLotShape: General shape of property\nLandContour: Flatness of the property\nUtilities: Type of utilities available\nLotConfig: Lot configuration\nLandSlope: Slope of property\nNeighborhood: Physical locations within Ames city limits\nCondition1: Proximity to main road or railroad\nCondition2: Proximity to main road or railroad (if a second is present)\nBldgType: Type of dwelling\nHouseStyle: Style of dwelling\nOverallQual: Overall material and finish quality\nOverallCond: Overall condition rating\nYearBuilt: Original construction date\nYearRemodAdd: Remodel date\nRoofStyle: Type of roof\nRoofMatl: Roof material\nExterior1st: Exterior covering on house\nExterior2nd: Exterior covering on house (if more than one material)\nMasVnrType: Masonry veneer type\nMasVnrArea: Masonry veneer area in square feet\nExterQual: Exterior material quality\nExterCond: Present condition of the material on the exterior\nFoundation: Type of foundation\nBsmtQual: Height of the basement\nBsmtCond: General condition of the basement\nBsmtExposure: Walkout or garden level basement walls\nBsmtFinType1: Quality of basement finished area\nBsmtFinSF1: Type 1 finished square feet\nBsmtFinType2: Quality of second finished area (if present)\nBsmtFinSF2: Type 2 finished square feet\nBsmtUnfSF: Unfinished square feet of basement area\nTotalBsmtSF: Total square feet of basement area\nHeating: Type of heating\nHeatingQC: Heating quality and condition\nCentralAir: Central air conditioning\nElectrical: Electrical system\n1stFlrSF: First Floor square feet\n2ndFlrSF: Second floor square feet\nLowQualFinSF: Low quality finished square feet (all floors)\nGrLivArea: Above grade (ground) living area square feet\nBsmtFullBath: Basement full bathrooms\nBsmtHalfBath: Basement half bathrooms\nFullBath: Full bathrooms above grade\nHalfBath: Half baths above grade\nBedroom: Number of bedrooms above basement level\nKitchen: Number of kitchens\nKitchenQual: Kitchen quality\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\nFunctional: Home functionality rating\nFireplaces: Number of fireplaces\nFireplaceQu: Fireplace quality\nGarageType: Garage location\nGarageYrBlt: Year garage was built\nGarageFinish: Interior finish of the garage\nGarageCars: Size of garage in car capacity\nGarageArea: Size of garage in square feet\nGarageQual: Garage quality\nGarageCond: Garage condition\nPavedDrive: Paved driveway\nWoodDeckSF: Wood deck area in square feet\nOpenPorchSF: Open porch area in square feet\nEnclosedPorch: Enclosed porch area in square feet\n3SsnPorch: Three season porch area in square feet\nScreenPorch: Screen porch area in square feet\nPoolArea: Pool area in square feet\nPoolQC: Pool quality\nFence: Fence quality\nMiscFeature: Miscellaneous feature not covered in other categories\nMiscVal: $Value of miscellaneous feature\nMoSold: Month Sold\nYrSold: Year Sold\nSaleType: Type of sale\nSaleCondition: Condition of sale\n\nFiles 7 files\nSize 1.15 MB\nType csv, gz, txt\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "regression",
          "tabular",
          "feature engineering, gradient boosting",
          "real_estate",
          "rmse"
        ]
      }
    },
    {
      "challenge_name": "humpback-whale-identification",
      "description": "Challenge description:\nAfter centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\nTo aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales\u2019 tails and unique markings found in footage to identify what species of whale they\u2019re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\nIn this competition, you\u2019re challenged to build an algorithm to identify individual whales in images. You\u2019ll analyze Happywhale\u2019s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you\u2019ll help to open rich fields of understanding for marine mammal population dynamics around the globe.\nNote, this competition is similar in nature tothis competitionwith an expanded and updated dataset.\nWe'd like to thankHappywhalefor providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.\nSubmissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):\n$$MAP@5 = \\frac{1}{U} \\sum_{u=1}^{U}  \\sum_{k=1}^{min(n,5)} P(k) \\times rel(k)$$\nwhere \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) label, zero otherwise.\nOnce a correct label has been scored foran observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label isAfor an observation, the following predictions all score an average precision of1.0.\n[A, B, C, D, E]\n[A,A,A,A,A]\n[A, B,A, C,A]\nFor eachImagein the test set, you may predict up to 5 labels for the whaleId. Whales that are not predicted to be one of the labels in the training data should be labeled asnew_whale. The file should contain a header and have the following format:\nImage,Id\n00028a005.jpg,new_whale w_23a388d w_9b5109b w_9c506f6w_0369a5c\n000dcf7d8.jpg,new_whale w_23a388d w_9b5109b w_9c506f6w_0369a5c...\nFebruary 21, 2019- Entry deadline. You must accept the competition rules before this date in order to compete.\nFebruary 21, 2019- Team Merger deadline. This is the last day participants may join or merge teams.\nFebruary 21, 2019- External Data Disclosure deadline. All external data used in the competition must be disclosed in the forums by this date.\nFebruary 28, 2019- Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n1st Place- $10,000\n2nd Place- $7,500\n3rd Place- $ 2,500\n4th Place- $ 2,500\n5th Place- $ 2,500\nAddison Howard, inversion, Ken Southerland, and Ted Cheeseman. Humpback Whale Identification. https://kaggle.com/competitions/humpback-whale-identification, 2018. Kaggle.\n\nData description:\nHumpback Whale Identification\nCan you identify a whale by its tail?\n\nKaggle\u00b7 Featured Prediction Competition \u00b77 years ago Late Submission\n\nDataset Description\nThis training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whaleId of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids.\n\nFile descriptions\ntrain.zip - a folder containing the training images\ntrain.csv (4 columns) - maps the training image to the appropriate whaleId. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.\ntest.zip - a folder containing the test images to predict the whaleId\nsample_submission.csv - a sample submission file in the correct format\n\nFiles: 33323\nSize: 5.95 GB\nType: jpg, csv\nLicense: Subject to Competition Rules\n\nCompetition Rules\nTo see this data you need to agree to the competition rules.\nPlease sign in or register to accept the rules.",
      "docker_challenge_path": "/data/humpback-whale-identification",
      "competition_description": "Challenge description:\nAfter centuries of intense whaling, recovering whale populations still have a hard time adapting to warming oceans and struggle to compete every day with the industrial fishing industry for food.\nTo aid whale conservation efforts, scientists use photo surveillance systems to monitor ocean activity. They use the shape of whales\u2019 tails and unique markings found in footage to identify what species of whale they\u2019re analyzing and meticulously log whale pod dynamics and movements. For the past 40 years, most of this work has been done manually by individual scientists, leaving a huge trove of data untapped and underutilized.\nIn this competition, you\u2019re challenged to build an algorithm to identify individual whales in images. You\u2019ll analyze Happywhale\u2019s database of over 25,000 images, gathered from research institutions and public contributors. By contributing, you\u2019ll help to open rich fields of understanding for marine mammal population dynamics around the globe.\nNote, this competition is similar in nature tothis competitionwith an expanded and updated dataset.\nWe'd like to thankHappywhalefor providing this data and problem. Happywhale is a platform that uses image process algorithms to let anyone to submit their whale photo and have it automatically identified.",
      "evaluation_metric": "Submissions are evaluated according to the Mean Average Precision @ 5 (MAP@5):\n$$MAP@5 = \\frac{1}{U} \\sum_{u=1}^{U}  \\sum_{k=1}^{min(n,5)} P(k) \\times rel(k)$$\nwhere \\( U \\)  is the number of images, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number predictions per image, and \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) label, zero otherwise.\nOnce a correct label has been scored foran observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label isAfor an observation, the following predictions all score an average precision of1.0.\n[A, B, C, D, E]\n[A,A,A,A,A]\n[A, B,A, C,A]",
      "dataset_description": "Data description:\nHumpback Whale Identification\nCan you identify a whale by its tail?\n\nKaggle\u00b7 Featured Prediction Competition \u00b77 years ago Late Submission\n\nDataset Description\nThis training data contains thousands of images of humpback whale flukes. Individual whales have been identified by researchers and given an Id. The challenge is to predict the whaleId of images in the test set. What makes this such a challenge is that there are only a few examples for each of 3,000+ whale Ids.\n\nFile descriptions\ntrain.zip - a folder containing the training images\ntrain.csv (4 columns) - maps the training image to the appropriate whaleId. Whales that are not predicted to have a label identified in the training data should be labeled as new_whale.\ntest.zip - a folder containing the test images to predict the whaleId\nsample_submission.csv - a sample submission file in the correct format\n\nFiles: 33323\nSize: 5.95 GB\nType: jpg, csv\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "classification",
          "images",
          "metric-learning",
          "biology",
          "map"
        ]
      }
    },
    {
      "challenge_name": "ieee-fraud-detection",
      "description": "Challenge description:\nImagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren\u2019t thinking about the data science that determined your fate. Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. \u201cPress 1 if you really tried to spend $500 on cheddar cheese.\u201d While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle. IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge. In this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. If successful, you\u2019ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives. Acknowledgements: Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually. Header Photo by Tim Evans on Unsplash Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target. For each TransactionID in the test set, you must predict a probability for the isFraud variable. The file should contain a header and have the following format: TransactionID,isFraud 3663549,0.5 3663550,0.5 3663551,0.5 etc. 1st Prize: $10,000 2nd Prize: $7,000 3rd Prize: $3,000 Winners will be required to submit a write-up for the IEEE CIS Conference, to which they are invited and highly encouraged to attend and present their work. UPDATE: The below timeline has been updated according to this post. Please see that post and the competition rules for more details. September 24, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete. September 24, 2019 - Team Merger deadline. This is the last day participants may join or merge teams. September 24, 2019 - External Data Disclosure deadline. This is the last day to disclose any used external data to the competition forums. October 3, 2019 - Final submission deadline. After this date, we will not be taking any more submissions. Remember to select your two best submissions for final scoring. Addison Howard, Bernadette Bouchon-Meunier, IEEE CIS, inversion, John Lei, Lynn@Vesta, Marcus2010, and Prof. Hussein Abbass. IEEE-CIS Fraud Detection. https://kaggle.com/competitions/ieee-fraud-detection, 2019. Kaggle.\n\nData description:\nIEEE Computational Intelligence Society\nIEEE-CIS Fraud Detection\nCan you detect fraud from customer transactions?\n\nDataset Description\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary targetisFraud.\nThe data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.\nCategorical Features - Transaction\nProductCD\ncard1-card6\naddr1,addr2\nP_emaildomain\nR_emaildomain\nM1-M9\nCategorical Features - Identity\nDeviceType\nDeviceInfo\nid_12-id_38\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\nYou can read more about the data from this post by the competition host.\nFiles\ntrain_{transaction, identity}.csv - the training set\ntest_{transaction, identity}.csv - the test set (you must predict the isFraud value for these observations)\nsample_submission.csv - a sample submission file in the correct format\nFiles\n5 files\nSize 1.35 GB\nType csv\nLicense Subject to Competition Rules\nsample_submission.csv (6.08 MB)\n871 columns",
      "docker_challenge_path": "/data/ieee-fraud-detection",
      "competition_description": "Challenge description:\nImagine standing at the check-out counter at the grocery store with a long line behind you and the cashier not-so-quietly announces that your card has been declined. In this moment, you probably aren\u2019t thinking about the data science that determined your fate. Embarrassed, and certain you have the funds to cover everything needed for an epic nacho party for 50 of your closest friends, you try your card again. Same result. As you step aside and allow the cashier to tend to the next customer, you receive a text message from your bank. \u201cPress 1 if you really tried to spend $500 on cheddar cheese.\u201d While perhaps cumbersome (and often embarrassing) in the moment, this fraud prevention system is actually saving consumers millions of dollars per year. Researchers from the IEEE Computational Intelligence Society (IEEE-CIS) want to improve this figure, while also improving the customer experience. With higher accuracy fraud detection, you can get on with your chips without the hassle. IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry, and now you are invited to join the challenge. In this competition, you\u2019ll benchmark machine learning models on a challenging large-scale dataset. The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. You also have the opportunity to create new features to improve your results. If successful, you\u2019ll improve the efficacy of fraudulent transaction alerts for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. And of course, you will save party people just like you the hassle of false positives. Acknowledgements: Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995, Vesta pioneered the process of fully guaranteed card-not-present (CNP) payment transactions for the telecommunications industry. Since then, Vesta has firmly expanded data science and machine learning capabilities across the globe and solidified its position as the leader in guaranteed ecommerce payments. Today, Vesta guarantees more than $18B in transactions annually. Header Photo by Tim Evans on Unsplash",
      "evaluation_metric": "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.",
      "dataset_description": "Data description:\nIEEE Computational Intelligence Society\nIEEE-CIS Fraud Detection\nCan you detect fraud from customer transactions?\n\nDataset Description\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary targetisFraud.\nThe data is broken into two files identity and transaction, which are joined by TransactionID. Not all transactions have corresponding identity information.\nCategorical Features - Transaction\nProductCD\ncard1-card6\naddr1,addr2\nP_emaildomain\nR_emaildomain\nM1-M9\nCategorical Features - Identity\nDeviceType\nDeviceInfo\nid_12-id_38\nThe TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\nYou can read more about the data from this post by the competition host.\nFiles\ntrain_{transaction, identity}.csv - the training set\ntest_{transaction, identity}.csv - the test set (you must predict the isFraud value for these observations)\nsample_submission.csv - a sample submission file in the correct format\nFiles\n5 files\nSize 1.35 GB\nType csv\nLicense Subject to Competition Rules\nsample_submission.csv (6.08 MB)\n871 columns",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "binary classification",
          "tabular",
          "feature engineering",
          "ecommerce_payments",
          "auc-roc"
        ]
      }
    },
    {
      "challenge_name": "imaterialist-challenge-fashion-2018",
      "description": "Challenge description:\niMaterialist Challenge (Fashion) at FGVC5\nImage classification of fashion products.\n\nAs shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today\u2019s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions.\nTackling issues like this is why theConference on Computer Vision and Pattern Recognition (CVPR)has put together a workshop specifically for data scientists focused on fine-grained visual categorization called theFGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification.\nIn this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection \u2013 to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.\nKaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.\n\nFor this competition each image has multiple ground truth labels. We will use Mean F1 score (micro-averaged, see detailshere) to measure the algorithm quality. The metric is also known as the example based F-score in the multi-label learning literature.\nThe F1 metric weights recall and precision equally, and a good recognition algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\nSubmission File\nFor every image in the dataset, submission files should contain two columns: image id and predicted labels. Labels should be a space-delimited list. Note that if the algorithm don\u2019t predict anything, the column can be left blank. The file must have a header and should look like the following:\nid,predicted\n12345,0\n367890,8\n3293,\netc.\n\n1st place - $1,200\n2nd place - $800\n3rd place - $500\nThe prize is provided by Wish.\n\nTop submissions will have the opportunity to present their work at theFGVCworkshop in Salt Lake City, colocated with premier computer vision conferenceCVPR 2018. Attending the workshop is not required to participate in the competition, however only teams that are attending the workshop will be considered to present their work. Attendees presenting in person are responsible for all costs associated with travel, expenses, and fees to attend CVPR 2018.\n\nMay 23, 2018- Entry deadline. You must accept the competition rules before this date in order to compete.\nMay 23, 2018- Team Merger deadline. This is the last day participants may join or merge teams.\nMay 30, 2018- Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nandypassion, RavenClaw, and Wendy Kan. iMaterialist Challenge (Fashion) at FGVC5. https://kaggle.com/competitions/imaterialist-challenge-fashion-2018, 2018. Kaggle.\n\nCompetition Host: Wendy Kan\n\nData description:\nImage classification of fashion products.\niMaterialist Challenge (Fashion) at FGVC5\n\nDataset Description\nAll the data described below are txt files in JSON format.\n\nOverview\nsample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format\ntest.json: images of which the participants need to generate predictions. Only image URLs are provided.\ntrain.json: training data with image urls and labels\nvalidation.json: validation data with the same format as train.json\n\nTraining Data\nThe training dataset includes images from 228 fashion attribute classes with multiple ground truth labels for each image. It includes a total of 1,014,544 images for training and 10,586 images for validation and 42,590 images for testing.\n\nAll train/validation/test sets have the same format as shown below:\n{\"images\" : [image],\n\"annotations\" : [annotation],\n}\nimage{\n\"image_id\" : int,\n\"url\": [string]\n}\nannotation{\n\"image_id\" : int,\n\"label_id\" : [int]\n}\n\nNote that for each image, we only provide URLs instead of the image content. Users need to download the images by themselves. Note that the image urls are hosted by Wish so they are expected to be stable.\nThis year, we omit the names of the labels to avoid hand labeling the test images.\n\nTesting data and submissions\nThe testing data only has images as shown below:\n{\"images\" : [image],\n}\n{\"image_id\" : int,\n\"url\" : [string],\n}\nWe also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format.\n\nSubject to Competition Rules",
      "docker_challenge_path": "/data/imaterialist-challenge-fashion-2018",
      "competition_description": "Challenge description:\niMaterialist Challenge (Fashion) at FGVC5\nImage classification of fashion products.\n\nAs shoppers move online, it would be a dream come true to have products in photos classified automatically. But, automatic product recognition is tough because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, royal blue vs turquoise in color. Many of today\u2019s general-purpose recognition machines simply cannot perceive such subtle differences between photos, yet these differences could be important for shopping decisions.\nTackling issues like this is why theConference on Computer Vision and Pattern Recognition (CVPR)has put together a workshop specifically for data scientists focused on fine-grained visual categorization called theFGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Wish, and Malong Technologies to challenge the data science community to help push the state of the art in automatic image classification.\nIn this competition, FGVC workshop organizers with Wish and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product detection \u2013 to accurately assign attribute labels for fashion images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop.\nKaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
      "evaluation_metric": "For this competition each image has multiple ground truth labels. We will use Mean F1 score (micro-averaged, see detailshere) to measure the algorithm quality. The metric is also known as the example based F-score in the multi-label learning literature.\nThe F1 metric weights recall and precision equally, and a good recognition algorithm will maximize both precision and recall simultaneously. Thus, moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.",
      "dataset_description": "Data description:\nImage classification of fashion products.\niMaterialist Challenge (Fashion) at FGVC5\n\nDataset Description\nAll the data described below are txt files in JSON format.\n\nOverview\nsample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format\ntest.json: images of which the participants need to generate predictions. Only image URLs are provided.\ntrain.json: training data with image urls and labels\nvalidation.json: validation data with the same format as train.json\n\nTraining Data\nThe training dataset includes images from 228 fashion attribute classes with multiple ground truth labels for each image. It includes a total of 1,014,544 images for training and 10,586 images for validation and 42,590 images for testing.\n\nAll train/validation/test sets have the same format as shown below:\n{\"images\" : [image],\n\"annotations\" : [annotation],\n}\nimage{\n\"image_id\" : int,\n\"url\": [string]\n}\nannotation{\n\"image_id\" : int,\n\"label_id\" : [int]\n}\n\nNote that for each image, we only provide URLs instead of the image content. Users need to download the images by themselves. Note that the image urls are hosted by Wish so they are expected to be stable.\nThis year, we omit the names of the labels to avoid hand labeling the test images.\n\nTesting data and submissions\nThe testing data only has images as shown below:\n{\"images\" : [image],\n}\n{\"image_id\" : int,\n\"url\" : [string],\n}\nWe also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format.\n\nSubject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "multi-label classification",
          "images",
          "cnn transfer-learning augmentation",
          "fashion retail",
          "f1-micro"
        ]
      }
    },
    {
      "challenge_name": "imaterialist-challenge-furniture-2018",
      "description": "Challenge description:\nDescription\nAs shoppers move online, it\u2019d be a dream come true to have products in photos classified automatically. But, automatic product recognition is challenging because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, ball chair vs egg chair for furniture, or dutch oven vs french oven for cookware. Many of today\u2019s general-purpose recognition machines simply can\u2019t perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Malong Technologies and Wish to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC5 workshop organizers and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product recognition \u2013 to accurately assign category labels for furniture and home goods images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.\n\nEvaluation\nFor this competition each image has one ground truth label. An algorithm to be evaluated will produce 1 label per image. If the predicted label is the same as the groundtruth label, then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images.\nSubmission File\nFor each image in the test set, you must predict 1 class label. The csv file should contain a header and have the following format:\nid,predicted\n12345,0\n67890,83\netc.\n\nCVPR 2018\nThis competition is part of the FGVC5 workshop at CVPR 2018. Top submissions for the competition will be invited to give talks at the workshop. Attending the workshop is not required to participate in the competition, however only teams that are attending the workshop will be considered to present their work. Attendees presenting in person are responsible for all costs associated with travel, expenses, and fees to attend CVPR 2018.\n\nPrizes\n1st place - $1,200\n2nd place - $800\n3rd place - $500\nThe prize is provided by Malong Technologies.\n\nTimeline\nMay 23, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMay 23, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nMay 30, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nCitation\nandypassion, RavenClaw, and Wendy Kan. iMaterialist Challenge (Furniture) at FGVC5. https://kaggle.com/competitions/imaterialist-challenge-furniture-2018, 2018. Kaggle.\n\nCompetition Host\nWendy Kan\n\nPrizes & Awards\n$2,500\nDoes not award Points or Medals\n\nParticipation\n2,937 Entrants\n569 Participants\n426 Teams\n5,214 Submissions\n\nData description:\niMaterialist Challenge (Furniture) at FGVC5Image Classification of Furniture & Home Goods.\nAll the data described below are txt files in JSON format.\ntrain.json: training data with image urls and labels\nvalidation.json: validation data with the same format as train.json\ntest.json: images of which the participants need to generate predictions. Only image URLs are provided.\nsample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format\nTraining Data\nThe training dataset includes images from 128 furniture and home goods classes with one ground truth label for each image. It includes a total of 194,828 images for training and 6,400 images for validation and 12,800 images for testing.\nTrain and validation sets have the same format as shown below:\n\"images\" : [image],\n\"annotations\" : [annotation],\n\"image_id\" : int,\n\"url\": [string]\nannotation{\n\"image_id\" : int,\n\"label_id\" : int\nNote that for each image, we only provide URL instead of the image content. Users need to download the images by themselves. Note that the image urls may become unavailable over time. Therefore we suggest that the participants start downloading the images as early as possible. We are considering image hosting service in order to handle unavailable URLs. We'll update here if that could be finalized.\nThis year, we omit the names of the labels to avoid hand labeling the test images.\nTesting data and submissions\nThe testing data only has images as shown below:\n\"images\" : [image],\n\"image_id\" : int,\n\"url\" : [string],\nWe also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format.\nFiles4 filesSize48.59 MBTypejson, csvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/imaterialist-challenge-furniture-2018",
      "competition_description": "Challenge description:\nDescription\nAs shoppers move online, it\u2019d be a dream come true to have products in photos classified automatically. But, automatic product recognition is challenging because for the same product, a picture can be taken in different lighting, angles, backgrounds, and levels of occlusion. Meanwhile different fine-grained categories may look very similar, for example, ball chair vs egg chair for furniture, or dutch oven vs french oven for cookware. Many of today\u2019s general-purpose recognition machines simply can\u2019t perceive such subtle differences between photos, yet these differences could be important for shopping decisions. Tackling issues like this is why the Conference on Computer Vision and Pattern Recognition (CVPR) has put together a workshop specifically for data scientists focused on fine-grained visual categorization called the FGVC5 workshop. As part of this workshop, CVPR is partnering with Google, Malong Technologies and Wish to challenge the data science community to help push the state of the art in automatic image classification. In this competition, FGVC5 workshop organizers and Malong Technologies challenge you to develop algorithms that will help with an important step towards automatic product recognition \u2013 to accurately assign category labels for furniture and home goods images. Individuals/Teams with top submissions will be invited to present their work live at the FGVC5 workshop. Kaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
      "evaluation_metric": "Evaluation\nFor this competition each image has one ground truth label. An algorithm to be evaluated will produce 1 label per image. If the predicted label is the same as the groundtruth label, then the error for that image is 0, otherwise it is 1. The final score is the error averaged across all images.",
      "dataset_description": "Data description:\niMaterialist Challenge (Furniture) at FGVC5Image Classification of Furniture & Home Goods.\nAll the data described below are txt files in JSON format.\ntrain.json: training data with image urls and labels\nvalidation.json: validation data with the same format as train.json\ntest.json: images of which the participants need to generate predictions. Only image URLs are provided.\nsample_submission_randomlabel.csv: example submission file with random predictions to illustrate the submission file format\nTraining Data\nThe training dataset includes images from 128 furniture and home goods classes with one ground truth label for each image. It includes a total of 194,828 images for training and 6,400 images for validation and 12,800 images for testing.\nTrain and validation sets have the same format as shown below:\n\"images\" : [image],\n\"annotations\" : [annotation],\n\"image_id\" : int,\n\"url\": [string]\nannotation{\n\"image_id\" : int,\n\"label_id\" : int\nNote that for each image, we only provide URL instead of the image content. Users need to download the images by themselves. Note that the image urls may become unavailable over time. Therefore we suggest that the participants start downloading the images as early as possible. We are considering image hosting service in order to handle unavailable URLs. We'll update here if that could be finalized.\nThis year, we omit the names of the labels to avoid hand labeling the test images.\nTesting data and submissions\nThe testing data only has images as shown below:\n\"images\" : [image],\n\"image_id\" : int,\n\"url\" : [string],\nWe also provide a sample submission csv file as an example. The evaluation section has a more detailed description of the submission format.\nFiles4 filesSize48.59 MBTypejson, csvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "classification",
          "images",
          "transfer_learning",
          "retail",
          "accuracy"
        ]
      }
    },
    {
      "challenge_name": "inclusive-images-challenge",
      "description": "Challenge description:\n# Inclusive Images Challenge\n\n## Description\n\nMaking products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world.\n\nToday, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn't represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach.\n\nGoogle AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on.\n\nBy finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere.\n\nNote: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates.\n\nThis competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop.\n\nShankar et al. \"No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\" NIPS 2017 Workshop on Machine Learning for the Developing World\n\n## Evaluation\n\nFor this competition each image has multiple ground truth labels. We will use Mean F2 score to measure the algorithm quality. The metric is also known as the example based F-score with a beta of 2.\n\nThe F2 metric weights recall more heavily than precision, but a good recognition algorithm will still balance precision and recall. Moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.\n\n### Submission File\nFor every image in the dataset, submission files should contain two columns: image id and predicted labels. Labels should be a space-delimited list. Note that if the algorithm doesn't predict anything, the column can be left blank. The file must have a header and should look like the following:\n\nimage_id,labels\n2b2b327132556c767a736b3d,/m/0sgh53y /m/0g4cd0\n2b2b394755692f303963553d,/m/0sgh70d /m/0g44a\netc\n\n## Timeline\n\nOctober 29, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nOctober 29, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nNovember 5, 2018 - Stage 1 ends & Model upload deadline*.\nNovember 6, 2018 - Stage 2 begins. New test set uploaded.\nNovember 12, 2018 - Stage 2 ends & Final submission deadline.\nNovember 26, 2018 - Solutions & Other Winners Obligations due from winners.\nDecember 3-8, 2018 - NIPS 2018 Conference in Montreal, Quebec, Canada. Competition Workshop Track on Dec 7-8.\n\n* In order to be eligible for Stage 2, each team's Stage 1 submission must include the model uploaded, via Team -> Your Model, per the Competition Rules. This model should match that which was used to generate the 1 final submission selected for scoring. Be aware that if you do not select a final submission (via 'My Submissions'), the platform will auto-select your best-scoring model on the Stage 1 public leaderboard. The deadline for model upload is firmly the end of Stage 1.\n\nThis requirement is in place for the host team to verify the performance of the uploaded models matches the Stage 2 submission file. Compliance with the above will be verified by the host team. Submitters who fail to upload their model by the Stage 1 deadline, or are found not to be in compliance, may be disqualified from Stage 2 and removed from the final leaderboard.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Travel Grant Prizes\n\nThe top 5 competitors will be given funding to support attendance at the NIPS workshops from December 7 - December 8, 2018. They will also be given the opportunity to present their Inclusive Images solution as part of the NIPS 2018 competition track.\n\n1st Place - $5,000\n2nd Place - $5,000\n3rd Place - $5,000\n4th Place - $5,000\n5th Place - $5,000\n\n## Inclusive Images FAQ\n\n### Questions about the Competition Framework\n\n**Q. What is this competition about?**\nThis competition is about developing models that do well at image classification tasks even when the data on which they are evaluated is drawn from a very different set of geographical locations than the data on which they are trained.\n\n**Q. What are the goals of this competition?**\nThe main goal of this competition is to encourage and celebrate new research and methods that can do well in the challenging area of distributional skew. We hope that highlighting this key problem area can help to spur additional advances in the research community, and provide useful verification of their results in an open, rigorous evaluation.\n\n**Q. What is the importance of distributional skew?**\nOne of the key assumptions in traditional supervised machine learning is that the test set is drawn from the same distribution as the training set. However, in real world systems, it is often the reality that training data are collected in a way that does not represent the full diversity of individuals who will interact with the system once it is deployed. As a result, models are applied to data that is quite different from their training distributions. And indeed, from a geographical perspective, one's local distribution will always differ from a global distribution. Developing models and methods that are robust to distributional skew is one way to help develop models that may be more inclusive and more fair in real world settings.\n\n**Q. Why is the competition called InclusiveImages?**\nThe Challenge datasets in this competition provide a stress test for the geographical inclusivity of trained models. Models that do a better job of making predictions on data from a broad range of the world's geographical regions are likely to do much better in this competition than those that are specialized to work well on (for example) images drawn mostly from North America and Western Europe. Thus, we frame this competition with the notion of a \"stress test\", outlined below, where certain unrevealed geographic locations that are less well-represented in the training data are significantly more well-represented in the Challenge datasets.\n\n**Q. Why don't we just get more data to solve this sort of issue?**\nFor real world systems, it is definitely a best practice to collect more data to fill in under-represented portions of the data space whenever possible. (For more on this, see Google's recommendations for best practices for responsible AI.) In this competition, we are focused on the research challenge posed when such data collection is not possible for one reason or another. One important scenario for this exists when some areas of the data space are represented very rarely or are completely inaccessible under our current data-collection methodology.\n\n**Q. What does it mean for this to be a \"stress test\"?**\nThe idea of a stress test is that we give an algorithm a difficult challenge and see if it can handle it well. If the algorithm does not do well, that's a sign that it might not be good for settings that are similar to the stress test setting -- in this case, settings that involve strong distributional skew. If an algorithm does well on a stress test, that's an encouraging result, but we caution that this competition is just one test out of many possible stress tests. Ideally researchers perform a wide range of stress tests on their algorithms before making conclusions about their reliability.\n\n**Q. Isn't there more to algorithmic fairness and inclusion than just making algorithmic changes?**\nAbsolutely. Addressing such issues in depth is not simply a machine learning problem or a technology problem. The best work in this area is inclusive of a broad range of disciplines, people, and perspectives. (For more background, see Google's recommendations for best practices for responsible AI.) For the purposes of this competition, we do narrow the focus to look at the issue of distributional skew and encourage research that can help address this key problem area as one of many ways to help advance the field. But we also expect that the results of competition may serve to highlight areas in which algorithmic changes on their own may still fall short.\n\n**Q. Why are competitors not allowed to augment their datasets with additional images or other data sources?**\nAs a community, we already have a good understanding that adding additional data helps a great deal whenever possible. We want to make sure that this competition focuses research attention on the more difficult setting in which gathering fully representative data is not feasible in the given setting. This can happen, for example, because of privacy reasons or because certain exemplars are difficult to access under current data-collection methodologies.\n\n**Q. How might results from this competition inform work on types of data other than images?**\nWhile we are starting this effort off with images, we hope that algorithmic methods that do well on this competition may be applicable in other domains as well. For example, any new objective functions, regularization methods, or methods of incorporating multi-modal data (such as the Wikipedia side information allowed in this competition) may apply widely beyond image data.\n\n**Q. Are the winning solutions likely to be absolutely fair in all respects?**\nNot necessarily. If a method does well on the final test set, which is drawn from an un-revealed mix of geographic locations, then it is likely that this result was not due simply to chance or overfitting. This does not mean that such a method is absolutely fair in all respects. For one thing, there are a wide variety of definitions of fairness, some of which may be in tension with each other. But even more importantly, doing well on any one stress test is far from a complete certificate of fairness. In an ideal world, researchers test their methods on a wide range of stress tests before making conclusions about their reliability.\n\n**Q. How does the 2-stage design work?**\nThe competition will proceed in two stages. In Stage 1, competitors will train their models on a subset of the OpenImages dataset (as specified on the Competition Data Page), a widely used publicly available benchmark dataset for image classification. During this period, competitors will have access to the Challenge Stage 1 dataset and can compete on a public leaderboard. Competitors will upload their final models at the end of Stage 1. In Stage 2, competitors will run their final models on the Challenge Stage 2 dataset. Both Challenge datasets have un-revealed and distinct geographical distributions (see figure on the description page for an illustration). In this way, models are stress-tested for their ability to operate inclusively beyond their training data.\n\n### Questions about Participating in the Competition\n\n**Q. What are the important dates for this competition?**\nClick here for the full list of dates.\n\n**Q. Is this competition only open to those who can afford a lot of computational resources?**\nWe very much hope to be inclusive in participation as well as in data. To that end, we are providing $500 in Google Cloud credit for computational resources for this competition to the first 500 entrants who submit & meet eligibility requirements of the request form. We hope that this will make participation accessible to a wide range of interested people and groups. Check the competition forums for information that will be posted with the request form.\n\n**Q. What's the best way to get started?**\nIf you choose to use Google Cloud, the best way to get started is to import the training data into your own cloud bucket and start up a GPU-equipped instance as mentioned HERE.\n\nIf not using Google Cloud, please see the instructions below to access the datasets on your local machine. The Open Images training data resides on AWS, so you can work within that environment as well.\n\n**Q. How do I access the data sets?**\nPlease see the data download portion of the instructions HERE.\n\n**Q. Are there prizes?**\nThis challenge is primarily a research challenge, aimed at encouraging and celebrating new methods and techniques for handling problems of distributional skew. We are providing a set of travel grants to top competitors to help cover the cost of attending the associated NIPS workshop.\n\n**Q. Will grant winners be able to register for the NIPS Workshops?**\nWe will have access to a small number of reserved registrations for the NIPS workshops, which will be held for one representative from each of the 5 top-placing teams.\n\n**Q. If I win a prize but am not able to travel to the NIPS workshop, can I still accept the travel grant?**\nThe travel grants will be awarded on the basis of final leaderboard ranking, even if you are unable to attend the conference. However, winners are strongly encouraged (and expected) to attend the workshop to present their findings and share with the community.\n\n### Questions about the Data Sets\n\n**Q. What are the different datasets in this competition for training and testing?**\n\n**Training on Open Images**\nOpen Images training data: 1,743,042 images with both image-level and bounding-box annotations. Information on downloading this data is available here. Note that this is a subset of the \"full\" Open Images data set, which is large enough to be prohibitively expensive for many competitors to download.\n\nTuning data: 1000 images from the Challenge Stage 1 dataset with labels for competitors to get a sense of the dataset and labels, tune thresholds, etc.\n\n**Challenge Datasets for Testing**\nWe have prepared two challenge datasets for the two stages of the competition. They will differ significantly in their geographic distribution and potentially along other dimensions as well.\n\nChallenge Stage 1: Competitors will have a chance to compete with a leaderboard using one sample of the challenge dataset.\n\nChallenge Stage 2: (Final test set) competitors are scored based on their predictions on a final test set.\n\n**Optional External Data**\nWikipedia text: Text data from wikipedia that can optionally be used to improve training.\n\nNote that, per the competition rules, any other form of external data or pre-trained models is not permitted. Competitors are strictly prohibited from using other images or data outside of what has been explicitly approved.\n\n**Q. How was the data collected for the Challenge datasets?**\nOur goal was to make sure that images were provided by local people in their local environments, reflecting their daily world as they saw it. To this end, we collected image data for the challenge via Google's Crowdsource app, which is used by people around the world to help make ML training data more representative of their community. It is described in this article. We used data both from volunteers who chose to take pictures and donate them under a Creative Commons CC-BY license with the app on their mobile devices, and also from paid contractors from each local area. The app blurred any faces that it was able to detect in the photo on device. Each image was labeled in natural language by the person who donated it, which was then resolved to standard OpenImages-based class labels and verified by expert human raters.\n\n**Q. What is the Wikipedia Data for?**\nBecause creating models that generalize well to distributions that differ significantly from a model's training distribution is a hard research challenge, we wanted to provide some additional source of information that might be useful for creative solutions. For example, there might be opportunities to use this additional information for transfer learning, or for informative joint embeddings. This is an opportunity to be creative. But it's strictly optional -- there may be very good solutions to this challenge that do not touch this data source at all.\n\n**Q. Do we have to use the Wikipedia Data?**\nNo, the use of this data is purely optional.\n\n**Q. Who donated the images in the Challenge data sets?**\nImages were donated by thousands of Crowdsource app users in the targeted various geographical locations; additional images were supplied by dozens of paid contractors in these same locations.\n\n**Q. Why use image donation rather than images already existing on the web?**\nOur primary goal was to make sure that the images in this data set were indeed representative of the local areas and the people who live there. For the purposes of competition, we also wanted to ensure that the images had not previously appeared in other data sets or were not otherwise discoverable on the internet.\n\n**Q. How were the image labels in the data set verified?**\nThe class labels for each image were extracted from the natural language text entered by the people who took the pictures using the Crowdsource App, and were augmented for coverage by a human-verified labeling pipeline. All labels were verified by expert human raters, with images that contain pictures of people were assigned to multiple raters to reduce the chance of inadvertent error.\n\n**Q. Why were these particular labels chosen?**\nWe chose the labels to line up with the open images label set. We did change the trainable labels to remove specifically gendered tags, preferring e.g., \"person\" to \"man\" or \"woman\". Similar to the face-blurring discussed below. This was done because attempting to predict identity attributes such as gender of people in this competition is explicitly out of scope.\n\n**Q. Why not work to improve the data itself in OpenImages or release a new and improved highly inclusive dataset?**\nThis is obviously a great idea, independent of any of the research goals of this competition. We're continuing to collect images from all over the world through the Crowdsource app. It's a global effort that will continue well past the end of this competition. Watch this space for further updates.\n\n**Q. Are the Challenge datasets in InclusiveImages completely unbiased?**\nWe do not claim that these data sets are completely free of bias. Indeed, we have worked hard to make sure that they are strongly geographically skewed towards specific geographical regions that have been under-represented in other open-source image data sets. Doing well on these data sets is not evidence that an algorithm is completely free of bias, but doing well on these data sets under the format of the competition is one interesting piece of evidence about one challenging stress test.\n\n**Q. What do we know about biases in the Challenge Stage 1 dataset?**\nWhile we have targeted specific geographical locations in the collection of the Challenge Stage 1 dataset, it does have some particular areas of over and under representation that we found in preliminary analysis and wish to describe briefly here. These include:\n\n- Images of people tend to under-represent people who appear to be elderly.\n- Images tagged Child tend to be seen mostly in the context of play.\n- Some Person-related categories, including Bartender, Police Officer, and several sports related tags, appear to be predominantly (but by no means entirely) male.\n- Some Person-related categories, including Teacher, appear to be predominantly (but by no means entirely) female.\n- Images with people seem to be taken predominantly in urban rather than rural areas.\n- Images of people in traditional locale-specific dress such as Sari's in India are relatively under-represented in this Challenge Stage 1 data set.\n- In images tagged Wedding, there does not appear to be representation of same-sex marriages.\n\nNote that all of these listed qualities are characteristics we have found through analysis of images in the Challenge Stage 1 dataset. Competitors should expect that these distributional qualities may differ significantly in the Challenge Stage 2 final test set.\n\nAdditionally, we note a few peculiarities of the labels that may be useful background, and do not appear to be location / distribution specific:\n\n- Not all tags marked Person are necessarily images of humans. Images marked Person also include images of drawings, paintings, and figurines that might broadly considered a representation of a person.\n- The Transport tag is predominantly applied to means of public transportation (including bus, taxi, and rickshaw), rather than the more generic tag of Vehicle.\n- Tags related to categories such as Transport or Car might include that tag even when the object is not present in the image. For example, an image showing a car rental agency might be tagged as Car, and a train ticket counter might be tagged as Transport.\n\n**Q. From which geographical regions has the Challenge data been drawn?**\nIn general, we have sought to draw data largely from countries that are not largely represented in the OpenImages V4 data set that is used as training data in this competition. Because that data set drew largely from countries in North America and Western Europe, we have focused on countries in Asia, Africa, and South America for these new data sets. Specific lists of countries included will be made public after the conclusion of the competition.\n\n**Q. How will the Challenge Stage 2 (final test set) differ from the Challenge Stage 1 dataset?**\nThis competition is about distributional skew between training sets and test sets. To this end, we make sure that the Challenge Stage 1 dataset, which is used by competitors to help guide model development and for the public \"leaderboard\" before the final test, has a very different geographical distribution than the Challenge Stage 2 final test set. There may also be some differences in the specific label distributions and in the distributions of the subjects of the images. Note that the lack of detail on these differences is intentional at this time as part of the competition framework.\n\n**Q. Why is the distribution of the Challenge Stage 2 (final test set) kept hidden, and different from the Challenge Stage 1?**\nThe goal is that competitors should seek to do well on images distributions that differ from the training distribution in general, rather than seek to fit the specific distribution they see in the Challenge Stage 1 dataset. By keeping the distribution of the Challenge Stage 2 final test set hidden until the end of the competition, this allows us to reason that if a model does well on this un-revealed stress test distribution, then it is unlikely to have done so by chance or because of over-fitting to the Challenge Stage 1 distribution.\n\n**Q. Are there recognizable faces in the Challenge datasets?**\nNo, to the best of our ability we have applied facial blurring to all non-occluded frontal-view faces that appear in this data set. If you believe that an image with a non-occluded visible face was accidentally included in the dataset, please contact inclusive-images-nips@google.com and we will review it for removal from the dataset.\n\n**Q. Why are faces in the Challenge data set blurred?**\nWe take privacy very seriously and chose to blur faces in these data sets with this in mind. Additional reasons why we chose to blur faces in this data set include anonymization and ensuring that this competition was explicitly not on the topic of facial recognition.\n\n**Q. At what stage in the collection and donation process was facial blurring applied?**\nImage donation was performed via a mobile app, and blurring was applied on-device in that app at the time each picture was taken. Each individual who chose to donate an image was able to see the blurred image before deciding to donate it to this data set. We then engaged expert human raters to confirm that this blurring had been effectively applied in each image.\n\n**Q. Won't the blurring of faces impact the quality of predictions?**\nAs part of the preliminary testing for this competition framework, we ran tests that showed while the absolute performance of models may be slightly lower on the prediction tasks in OpenImages after facial blurring has been applied, the relative ranking of different models remains unchanged. Thus, we believe that for the purposes of competition the blurring of faces will not have a material effect on the final rankings of submissions.\n\n**Q. Are competitors allowed to try and attempt methods that are targeted towards unblurring the blurred faces?**\nNo, this is not permitted under the competition rules, as is any attempt to de-anonymize the data or otherwise use the data for purposes outside the framework of this competition.\n\n**Q. What if someone tries to create a Person detector by looking for blurred portions of an image and guessing there's a person there?**\nCompetitors should expect that the test set may protect against this sort of strategy in a variety of ways. We recommend not trying this sort of thing, or any other method that relies primarily on peculiarities of the Challenge Stage 1 dataset.\n\n**Q. How have images of people been processed in the Challenge datasets?**\nWe have attempted to include a class label of Person in all images that include visible images of people in them. These labels were originally drawn from the image descriptions provided by the image donators, and have been verified and augmented by paid expert human raters. As described above, we have to the best of our ability blurred all non-occluded faces of people in this data set. To avoid inferring a person's gender from an image, we have explicitly not included any gender-related identity terms in the Challenge datasets. Thus, unlike OpenImages V4, we do not include labels for Woman, Man, Girl, or Boy in this data. (Competitors are welcome to make use of this difference in label sets to avoid outputting these labels.)\n\n**Q. What licenses apply to the data sets in this competition?**\nThe Challenge datasets for this competition are each made available under Creative Commons Attribution 4.0 International license (\"CC BY 4.0\"). Please see the Creative Commons website (CC BY 4.0) for details regarding the details of this license.\n\nThe training set for this competition is Open Images V4. The annotations for this dataset are licensed by Google Inc. under a CC BY 4.0 license. The images are listed as having a CC BY 2.0 license. Note: While Google has made best effort to identify images in Open Images V4 that are licensed under a Creative Commons Attribution license, Google makes no representations or warranties regarding the license status of each image in Open Images V4.\n\n**Q. Will the data sets in this competition be open sourced after the competition finishes?**\nYes, we're planning on open sourcing the Challenge Stage 1 and Challenge Stage 2 datasets in full (including labels) after the competition closes.\n\n### Other Questions\n\n**Q. What is Google's involvement in this project?**\nAs described in Google's AI Principles, Google is committed to investing in responsible AI. As part of this, we want to support the broader academic and developer community in driving forward the cutting edge of research in this space. Google is proud to organize and sponsor this competition in partnership with NIPS!\n\n**Q. Why was F2 measure chosen as the evaluation metric?**\nBecause precision and recall are two metrics that are often in tension, it is important to consider both of these measures together in the evaluation metric. The F1 metric is one option, which weighs precision and recall equally. The F2 metric weights recall more heavily than precision, which we chose because the ground truth labels for this competition were generated based on text written by the person taking the photograph. As a result, there may be some objects in the photograph that were not labeled. Using F2 weighs the metric towards ensuring that the labels provided by the donator are included in the predicted labels, while still giving some weight toward avoiding completely spurious predictions.\n\n**Q. How will winning models be verified?**\nCompetitors with top submissions will be required to provide a means to the competition organizers to reproduce their results using their locked-in models and the allowed training data. We will also request access to model source code.\n\n**Q. Who can we contact for more information?**\nQuestions should be posted to the competition forum.\n\n## Model Eligibility Requirements\n\n### MODEL ELIGIBILITY REQUIREMENTS\nPer the Competition Rules, models must abide by these requirements. The Competition Host will verify eligibility of winning models and may, at its discretion, disqualify submissions which fail to meet these eligibility requirements:\n\n- The sole contribution of a submission must be a modeling technique (as opposed to a new auxiliary labeled image dataset).\n- Final submissions must contain only machine-generated labels.\n- Competitors with top submissions will be required to provide a means to reproduce their results using their locked-in models and the allowed training data.\n- Competitors are allowed to use the data as described on the competition page. No other data may be used for training.\n- Competitors are not permitted to warm-start their models using pretrained models, or otherwise use pretrained models in the training of their models.\n- Models must make their predictions based on image input only. Associated metadata such as the image id or the creator's name are not allowed to be used as inputs at inference time.\n\n### MODEL UPLOAD REQUIREMENT\nIn order to be eligible for Stage 2, each team's Stage 1 submission must include the model uploaded, via Team -> Your Model, per the Competition Rules. This model should match that which was used to generate the 1 final submission selected for scoring. Be aware that if you do not select a final submission (via 'My Submissions'), the platform will auto-select your best-scoring model on the Stage 1 public leaderboard. The deadline for model upload is firmly the end of Stage 1.\n\nThis requirement is in place for the host team to verify the performance of the uploaded models matches the Stage 2 submission file. Compliance with the above will be verified by the host team. Submitters who fail to upload their model by the Stage 1 deadline, or are found not to be in compliance, may be disqualified from Stage 2 and removed from the final leaderboard.\n\n## Data Download & Getting Started\n\nGetting the right data from Open Images dataset to train on is a little bit tricky, so please read closely.\n\nThe contest rules explicitly prohibit using images from outside of the dedicated training set and you will be asked to affirm that you followed the instructions and only used the specified images during training.\n\n### Getting the open images training image ids\nDownload the image ids for the training set from the open images website. We will be working with the training subset of the full open images dataset known as the \"Subset with bounding boxes.\" This subset contains 1,743,042 training images. Other than data provided on the Kaggle site, these are the only images you will be allowed to use in training.\n\n### Getting the images\nFollow the instructions to download the set of CVDF hosted images with bounding box annotations. Use only the train images. There should be 1,743,042 images in this dataset (513GB) and the ids should match those in the image ids file you downloaded in the previous step.\n\n### Using Google Cloud Bucket\nTo upload the data to the Google cloud bucket follow the steps below:\n\nCreate a Google Storage Bucket by following the instructions here:\n- Without the command line utility: https://cloud.google.com/storage/docs/quickstart-console\n- With the command line utility gsutil: https://cloud.google.com/storage/docs/quickstart-gsutil\n\nOnce you have gsutil installed and have given permission, If you haven't already, create a bucket:\ngsutil mb -l \"US\" gs://<bucket-name>\n\nCopy files in parallel:\ngsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp -r <bigfilename> gs://<bucket-name>\n\nWe recommend using the command line utility since it makes it easier to upload large folders.\nSee here for more information on accessing data stored in Google Cloud bucket.\n\nThings to not miss:\n- Once you install the SDK, you will have to authorize it using ./google-cloud-sdk/bin/gcloud init\n- You will have to assign an appropriate billing agency. Once you have created a project under manage resources and selected the appropriate billing entity, proceed to creating a Google cloud bucket as detailed above.\n- Select the type of bucket based on the amount of replication etc that you need as detailed here\n\n### Attaching Google Cloud Bucket to Google Compute Instance\nTo attach the Google Cloud Bucket to your Google Compute Instance, you will need to use gcs-fuse. https://cloud.google.com/storage/docs/gcs-fuse#using_feat_name\n\nFirst select your Google cloud Instance from here. To create an instance, go to your Google cloud platform \u2192 VM Instances \u2192 Create an Instance. Unlike AWS instances, here you can attach a GPU to any machine type, by first selecting the number of CPUs and then use the 'Customize' link to the right of the CPU selection to select the number of GPUs to add.\n\nSee here for more information on quotas and configuring GPUs. We would recommend using the DeepLearning VM from Marketplace.\n\nThe SSH key is automatically created and stored on the instance which is accessible via a browser.\n\nIn the instance terminal, authorise it to use your credentials by running gcloud auth login\n\nThen follow the steps to install gcsfuse and mount a Google Cloud Bucket as described:\n- INSTALL: https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/installing.md\n- MOUNT: https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/mounting.md\n\nTo mount the directory use:\ngcsfuse --implicit-dirs <bucket-name> <mountpoint>\n\nPlease note that all paths are relative and the directories won't be seen unless the subdirectories already exist without using the implicit-dirs flag.\nSee (https://stackoverflow.com/questions/38311036/folders-not-showing-up-in-bucket-storage)\n\n### Additional Resources\nUse Google Cloud Platform with Tensorflow\n\n## Citation\n\nAlex D'Amour, D. Sculley, James Atwood, Pallavi, Sohier Dane, Tulsee Doshi, Yoni Halpern, and Yufeng Guo. Inclusive Images Challenge. https://kaggle.com/competitions/inclusive-images-challenge, 2018. Kaggle.\n\nData description:\n# Inclusive Images Challenge\n\nStress test image classifiers across new geographic distributions\n\n## Dataset Description\n\n### Image Data Access\nAll of the stage 1 and 2 image and attributions and solutions files are now available at https://console.cloud.google.com/storage/browser/inclusive-images-public.\n\n### Data description\nThis competition uses a portion of the Open Images dataset as the training set. Getting the right subset of data from Open Images site is a bit tricky; please see this guide for details.\n\nYou may also choose to use this Wikipedia text data to augment the training images. For example, there might be opportunities to use this additional information for transfer learning, or for informative joint embeddings. This is an opportunity to be creative. But it's strictly optional -- there may be very good solutions to this challenge that do not touch this data source at all. The contents of the text data archive are documented here.\n\n### File descriptions\n- stage_2_attributions.csv - The credits for the stage 2 images.\n- stage_2_sample_submission.csv - A sample submission file for stage 2 in the correct format.\n- stage_2_test_images.zip - The stage 2 test set. The original images were released under the CC-BY 4.0 license. They have had their exif metadata removed and been rescaled to have a longest dimension of 1024 pixels.\n- stage_1_attributions.csv - The credits for the stage 1 images.\n- stage_1_sample_submission.csv - The old sample submission file.\n- stage_1_test_images.zip - The stage 1 test set. We aren't releasing the labels for stage 1 at this time as models must be locked down for stage 2. The original images were released under the CC-BY 4.0 license. They have had their exif metadata removed and been rescaled to have a longest dimension of 1024 pixels.\n- train_bounding_boxes.csv - The bounding boxes for the training set.\n- train_human_labels.csv - The human generated labels for the training set. Excludes the negative labels provided in the core Open Images dataset, as this competition does not use the concept of negative labels.\n- train_machine_labels.csv - The machine generated labels for the training set. Excludes the negative labels provided in the core Open Images dataset, as this competition does not use the concept of negative labels.\n- tuning_labels.csv - The labels for 1,000 images from the stage 1 test set. These are provided to make it easier to tune models to perform well on the test set.\n- class-descriptions.csv - A short description of each label. These labels largely overlap with the standard Open Images labels, but with several specifically gendered tags removed. See the FAQ for more detail.\n- classes-trainable.csv - The set of classes that have enough examples in the full Open Images dataset that it is feasible to train a model to detect them. As with the class descriptions, the gendered tags have been removed; see the FAQ for more detail.\n\n### Dataset statistics\n- Files: 131,902 files\n- Size: 15.88 GB\n- Type: jpg, csv\n- License: Subject to Competition Rules",
      "docker_challenge_path": "/data/inclusive-images-challenge",
      "competition_description": "## Description\n\nMaking products that work for people all over the globe is an important value at Google AI. In the field of classification, this means developing models that work well for regions all over the world.\n\nToday, the dataset a model is trained on greatly dictates the performance of that model. A system trained on a dataset that doesn't represent a broad range of localities could perform worse on images drawn from geographic regions underrepresented in the training data. Google and the industry at large are working to create more diverse & representative datasets. But it is also important for the field to make progress in understanding how to build models when the data available may not cover all audiences a model is meant to reach.\n\nGoogle AI is challenging Kagglers to develop models that are robust to blind spots that might exist in a data set, and to create image recognition systems that can perform well on test images drawn from different geographic distributions than the ones they were trained on.\n\nBy finding ways to teach image classifiers to generalize to new geographic and cultural contexts, we hope the community will make even more progress in inclusive machine learning that benefits everyone, everywhere.\n\nNote: This competition is run in two stages. Refer to the FAQ for an explanation of how this works & the Timeline for specific dates.\n\nThis competition is a part of the NIPS 2018 competition track. Winners will be invited to attend and present their solutions at the workshop.\n\nShankar et al. \"No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World\" NIPS 2017 Workshop on Machine Learning for the Developing World",
      "evaluation_metric": "## Evaluation\n\nFor this competition each image has multiple ground truth labels. We will use Mean F2 score to measure the algorithm quality. The metric is also known as the example based F-score with a beta of 2.\n\nThe F2 metric weights recall more heavily than precision, but a good recognition algorithm will still balance precision and recall. Moderately good performance on both will be favored over extremely good performance on one and poor performance on the other.",
      "dataset_description": "## Dataset Description\n\n### Image Data Access\nAll of the stage 1 and 2 image and attributions and solutions files are now available at https://console.cloud.google.com/storage/browser/inclusive-images-public.\n\n### Data description\nThis competition uses a portion of the Open Images dataset as the training set. Getting the right subset of data from Open Images site is a bit tricky; please see this guide for details.\n\nYou may also choose to use this Wikipedia text data to augment the training images. For example, there might be opportunities to use this additional information for transfer learning, or for informative joint embeddings. This is an opportunity to be creative. But it's strictly optional -- there may be very good solutions to this challenge that do not touch this data source at all. The contents of the text data archive are documented here.\n\n### File descriptions\n- stage_2_attributions.csv - The credits for the stage 2 images.\n- stage_2_sample_submission.csv - A sample submission file for stage 2 in the correct format.\n- stage_2_test_images.zip - The stage 2 test set. The original images were released under the CC-BY 4.0 license. They have had their exif metadata removed and been rescaled to have a longest dimension of 1024 pixels.\n- stage_1_attributions.csv - The credits for the stage 1 images.\n- stage_1_sample_submission.csv - The old sample submission file.\n- stage_1_test_images.zip - The stage 1 test set. We aren't releasing the labels for stage 1 at this time as models must be locked down for stage 2. The original images were released under the CC-BY 4.0 license. They have had their exif metadata removed and been rescaled to have a longest dimension of 1024 pixels.\n- train_bounding_boxes.csv - The bounding boxes for the training set.\n- train_human_labels.csv - The human generated labels for the training set. Excludes the negative labels provided in the core Open Images dataset, as this competition does not use the concept of negative labels.\n- train_machine_labels.csv - The machine generated labels for the training set. Excludes the negative labels provided in the core Open Images dataset, as this competition does not use the concept of negative labels.\n- tuning_labels.csv - The labels for 1,000 images from the stage 1 test set. These are provided to make it easier to tune models to perform well on the test set.\n- class-descriptions.csv - A short description of each label. These labels largely overlap with the standard Open Images labels, but with several specifically gendered tags removed. See the FAQ for more detail.\n- classes-trainable.csv - The set of classes that have enough examples in the full Open Images dataset that it is feasible to train a model to detect them. As with the class descriptions, the gendered tags have been removed; see the FAQ for more detail.\n\n### Dataset statistics\n- Files: 131,902 files\n- Size: 15.88 GB\n- Type: jpg, csv\n- License: Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "multi-label classification",
          "images",
          "transfer learning and domain adaptation",
          "geospatial",
          "f2 (f-score)"
        ]
      }
    },
    {
      "challenge_name": "liverpool-ion-switching",
      "description": "Challenge description:\nUniversity of Liverpool - Ion Switching\nIdentify the number of channels open at each time point\n\nDescription:\nThink you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.\nTechnology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.\nAcknowledgements: This would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).\n\nEvaluation:\nSubmissions are evaluated using the macro F1 score.\nF1 is calculated as follows:\n$$F_1 = 2 * \\frac{precision * recall}{precision + recall}$$\n$$precision = \\frac{TP}{TP + FP}$$\n$$recall = \\frac{TP}{TP + FN}$$\nIn \"macro\" F1 a separate F1 score is calculated for each open_channels value and then averaged.\nSubmission File\nFor each time value in the test set, you must predict open_channels. The files must have a header and should look like the following:\ntime,open_channels\n500.0000,0\n500.0001,2\netc.\n\nTimeline:\nMay 18, 2020 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMay 18, 2020 - Team Merger deadline. This is the last day participants may join or merge teams.\nMay 25, 2020 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nPrizes:\n1st Place - $12,000\n2nd Place - $8,000\n3rd Place - $5,000\n\nCitation:\nNuman Celik, Richard Barrett-Jolley, Walter Reade, and Addison Howard. University of Liverpool - Ion Switching. https://kaggle.com/competitions/liverpool-ion-switching, 2020. Kaggle.\n\nCompetition Host:\nUniversity of Liverpool\n\nData description:\nUniversity of Liverpool - Ion Switching\nIdentify the number of channels open at each time point\n\nIn this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\nIMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\nYou can find detailed information about the data from the paper Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data.\n\nFiles\ntrain.csv - the training set\ntest.csv - the test set; you will be predicting open_channels from the signal data in this file\nsample_submission.csv - a sample submission file in the correct format\n\nFiles 3 files\nSize 146.08 MB\nType csv\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/liverpool-ion-switching",
      "competition_description": "Challenge description:\nUniversity of Liverpool - Ion Switching\nIdentify the number of channels open at each time point\n\nDescription:\nThink you can use your data science skills to make big predictions at a submicroscopic level? Many diseases, including cancer, are believed to have a contributing factor in common. Ion channels are pore-forming proteins present in animals and plants. They encode learning and memory, help fight infections, enable pain signals, and stimulate muscle contraction. If scientists could better study ion channels, which may be possible with the aid of machine learning, it could have a far-reaching impact.\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research. Scientists hope that technology could enable rapid automatic detection of ion channel current events in raw data.\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for your help. In this competition, you\u2019ll use ion channel data to better model automatic identification methods. If successful, you\u2019ll be able to detect individual ion channel events in noisy raw signals. The data is simulated and injected with real world noise to emulate what scientists observe in laboratory experiments.\nTechnology to analyze electrical data in cells has not changed significantly over the past 20 years. If we better understand ion channel activity, the research could impact many areas related to cell health and migration. From human diseases to how climate change affects plants, faster detection of ion channels could greatly accelerate solutions to major world problems.\nAcknowledgements: This would not be possible without the help of the Biotechnology and Biological Sciences Research Council (BBSRC).",
      "evaluation_metric": "Evaluation:\nSubmissions are evaluated using the macro F1 score.\nF1 is calculated as follows:\n$$F_1 = 2 * \\frac{precision * recall}{precision + recall}$$\n$$precision = \\frac{TP}{TP + FP}$$\n$$recall = \\frac{TP}{TP + FN}$$\nIn \"macro\" F1 a separate F1 score is calculated for each open_channels value and then averaged.",
      "dataset_description": "Data description:\nUniversity of Liverpool - Ion Switching\nIdentify the number of channels open at each time point\n\nIn this competition, you will be predicting the number of open_channels present, based on electrophysiological signal data.\nIMPORTANT: While the time series appears continuous, the data is from discrete batches of 50 seconds long 10 kHz samples (500,000 rows per batch). In other words, the data from 0.0001 - 50.0000 is a different batch than 50.0001 - 100.0000, and thus discontinuous between 50.0000 and 50.0001.\nYou can find detailed information about the data from the paper Deep-Channel uses deep neural networks to detect single-molecule events from patch-clamp data.\n\nFiles\ntrain.csv - the training set\ntest.csv - the test set; you will be predicting open_channels from the signal data in this file\nsample_submission.csv - a sample submission file in the correct format\n\nFiles 3 files\nSize 146.08 MB\nType csv\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "classification",
          "time_series",
          "signal_processing",
          "biology",
          "f1_macro"
        ]
      }
    },
    {
      "challenge_name": "m5-forecasting-accuracy",
      "description": "Challenge description:\nNote: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out itscompanion competitionHow much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you\u2019re also challenged to use machine learning to improve forecast accuracy.The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.AcknowledgementsAdditional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.EvaluationThis competition uses aWeighted Root Mean Squared Scaled Error(RMSSE). Extensive details about the metric, scaling, and weighting can be found in theM5 Participants Guide.Submission FileEach row contains anidthat is a concatenation of anitem_idand astore_id, which is eithervalidation(corresponding to the Public leaderboard), orevaluation(corresponding to the Private leaderboard). You are predicting 28 forecast days (F1-F28) of items sold for each row. For thevalidationrows, this corresponds tod_1914 - d_1941, and for theevaluationrows, this corresponds tod_1942 - d_1969. (Note: a month before the competition close, the ground truth for thevalidationrows will be provided.)The files must have a header and should look like the following:id,F1,...F28HOBBIES_1_001_CA_1_validation,0,...,2HOBBIES_1_002_CA_1_validation,2,...,11...HOBBIES_1_001_CA_1_evaluation,3,...,7HOBBIES_1_002_CA_1_evaluation,1,...,4TimelineJune 1, 2020-  Full Training Labels Released. Participants will be provided with the actual values of the 28 days of data used for scoring performance (Public test set). The private test set will remain unchanged, and will still be used to determine the winners.June 23, 2020- Entry deadline. You must accept the competition rules before this date in order to compete.June 23, 2020- Team Merger deadline. This is the last day participants may join or merge teams.June 30, 2020- Final submission deadline.All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.Prizes1st Place - $25,0002nd Place - $10,0003rd Place - $5,0004th Place - $3,0005th Place - $2,000An additional $5,000 will be granted to the highest performing student team on the leaderboard at the end of the competition. A student team is one for which at least 50% of team members are current full-time students. If interested append_STUto your team name.Prizes will be distributed during the M5 Conference in December 2020, held in New York City, NY, USA.CitationAddison Howard, inversion, Spyros Makridakis, and vangelis. M5 Forecasting - Accuracy. https://kaggle.com/competitions/m5-forecasting-accuracy, 2020. Kaggle.Competition HostUniversity of NicosiaPrizes & Awards$50,000Awards Points & MedalsParticipation31,968 Entrants7,022 Participants5,558 Teams88,741 SubmissionsTagsTime Series AnalysisCustom Metric\n\nData description:\nM5 Forecasting - Accuracy\nEstimate the unit sales of Walmart retail goods\n\nDataset Description\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in theM5 Participants Guide.\n\nFiles\ncalendar.csv- Contains information about the dates on which the products are sold.\nsales_train_validation.csv- Contains the historical daily unit sales data per product and store[d_1 - d_1913]\nsample_submission.csv- The correct format for submissions. Reference theEvaluationtab for more info.\nsell_prices.csv- Contains information about the price of the products sold per store and date.\nsales_train_evaluation.csv- Includes sales[d_1 - d_1941](labels used for the Public leaderboard)\n\nFiles5 files\nSize450.47 MB\nTypecsv\nLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/m5-forecasting-accuracy",
      "competition_description": "Challenge description:\nNote: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the point forecasts of the unit sales of various products sold in the USA by Walmart? If you are interested in estimating the uncertainty distribution of the realized values of the same series, be sure to check out itscompanion competitionHow much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses.  In this competition, in addition to traditional forecasting methods you\u2019re also challenged to use machine learning to improve forecast accuracy.The Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.In this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.If successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.AcknowledgementsAdditional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",
      "evaluation_metric": "EvaluationThis competition uses aWeighted Root Mean Squared Scaled Error(RMSSE). Extensive details about the metric, scaling, and weighting can be found in theM5 Participants Guide.",
      "dataset_description": "Data description:\nM5 Forecasting - Accuracy\nEstimate the unit sales of Walmart retail goods\n\nDataset Description\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in theM5 Participants Guide.\n\nFiles\ncalendar.csv- Contains information about the dates on which the products are sold.\nsales_train_validation.csv- Contains the historical daily unit sales data per product and store[d_1 - d_1913]\nsample_submission.csv- The correct format for submissions. Reference theEvaluationtab for more info.\nsell_prices.csv- Contains information about the price of the products sold per store and date.\nsales_train_evaluation.csv- Includes sales[d_1 - d_1941](labels used for the Public leaderboard)\n\nFiles5 files\nSize450.47 MB\nTypecsv\nLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "forecasting",
          "time_series",
          "hierarchical forecasting",
          "retail",
          "rmsse"
        ]
      }
    },
    {
      "challenge_name": "m5-forecasting-uncertainty",
      "description": "Challenge description:\n# M5 Forecasting - Uncertainty\n\nEstimate the uncertainty distribution of Walmart unit sales.\n\nNote: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition.\n\nHow much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you're also challenged to use machine learning to improve forecast accuracy.\n\nThe Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.\n\nIn this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world's largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\nIf successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.\n\nAcknowledgements: Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.\n\n## Evaluation\n\nThis competition uses a Weighted Scaled Pinball Loss (WSPL). Extensive details about the metric, scaling, and weighting can be found in the M5 Participants Guide.\n\n## Submission File\n\nSimilar to the point forecast competition, each row contains an id that is a concatenation of an item_id, a store_id, a quartile, and the prediction interval, which is either validation (corresponding to the Public leaderboard), or evaluation (corresponding to the Private leaderboard).\n\nIn addition, this competition has rows that have been aggregated at different levels. An X indicates the absence of a second aggregation level.\n\nYou are predicting 28 forecast days (F1-F28) of items sold for each row. For the validation rows, this corresponds to d_1914 - d_1941, and for the evaluation rows, this corresponds to d_1942 - d_1969. (Note: a month before the competition close, the ground truth for the validation rows will be provided.)\n\nThe files must have a header and should look like the following:\n\nid,F1,...F28\nTotal_X_0.005_validation,53,...,201\nHOBBIES_1_001_CA_1_0.005_validation,0,...,2\nHOBBIES_1_002_CA_1_0.005_validation,2,...,11\n...\nHOBBIES_1_001_CA_1_0.995_evaluation,3,...,7\nHOBBIES_1_002_CA_1_0.995_evaluation,1,...,4\n\n## Timeline\n\nJune 1, 2020 - Full Training Labels Released. Participants will be provided with the actual values of the 28 days of data used for scoring performance (Public test set). The private test set will remain unchanged, and will still be used to determine the winners.\n\nJune 23, 2020 - Entry deadline. You must accept the competition rules before this date in order to compete.\n\nJune 23, 2020 - Team Merger deadline. This is the last day participants may join or merge teams.\n\nJune 30, 2020 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $25,000\n2nd Place - $10,000\n3rd Place - $5,000\n4th Place - $3,000\n5th Place - $2,000\n\nAn additional $5,000 will be granted to the highest performing student team on the leaderboard at the end of the competition. A student team is one for which at least 50% of team members are current full-time students. If interested append _STU to your team name.\n\nPrizes will be distributed during the M5 Conference in December 2020, held in New York City, NY, USA.\n\n## Citation\n\nAddison Howard, inversion, Spyros Makridakis, and vangelis. M5 Forecasting - Uncertainty. https://kaggle.com/competitions/m5-forecasting-uncertainty, 2020. Kaggle.\n\n## Competition Host\n\nUniversity of Nicosia\n\nData description:\nUniversity of Nicosia\nM5 Forecasting - Uncertainty\nEstimate the uncertainty distribution of Walmart unit sales.\n\nDataset Description\nIn the challenge, you are predicting 9 quartiles of item sales at stores in various locations for two 28-day time periods. Information about the data is found in theM5 Participants Guide.\n\nFiles:\ncalendar.csv - Contains information about the dates on which the products are sold.\nsales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\nsample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\nsell_prices.csv - Contains information about the price of the products sold per store and date.\nsales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n\nFiles: 5 files\nSize: 515.48 MB\nType: csv\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/m5-forecasting-uncertainty",
      "competition_description": "Challenge description:\n# M5 Forecasting - Uncertainty\n\nEstimate the uncertainty distribution of Walmart unit sales.\n\nNote: This is one of the two complementary competitions that together comprise the M5 forecasting challenge. Can you estimate, as precisely as possible, the uncertainty distribution of the unit sales of various products sold in the USA by Walmart? This specific competition is the first of its kind, opening up new directions for both academic research and how uncertainty could be assessed and used in organizations. If you are interested in providing point (accuracy) forecasts for the same series, be sure to check out its companion competition.\n\nHow much camping gear will one store sell each month in a year? To the uninitiated, calculating sales at this level may seem as difficult as predicting the weather. Both types of forecasting rely on science and historical data. While a wrong weather forecast may result in you carrying around an umbrella on a sunny day, inaccurate business forecasts could result in actual or opportunity losses. In this competition, in addition to traditional forecasting methods you're also challenged to use machine learning to improve forecast accuracy.\n\nThe Makridakis Open Forecasting Center (MOFC) at the University of Nicosia conducts cutting-edge forecasting research and provides business forecast training. It helps companies achieve accurate predictions, estimate the levels of uncertainty, avoiding costly mistakes, and apply best forecasting practices. The MOFC is well known for its Makridakis Competitions, the first of which ran in the 1980s.\n\nIn this competition, the fifth iteration, you will use hierarchical sales data from Walmart, the world's largest company by revenue, to forecast daily sales for the next 28 days and to make uncertainty estimates for these forecasts. The data covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n\nIf successful, your work will continue to advance the theory and practice of forecasting. The methods used can be applied in various business areas, such as setting up appropriate inventory or service levels. Through its business support and training, the MOFC will help distribute the tools and knowledge so others can achieve more accurate and better calibrated forecasts, reduce waste and be able to appreciate uncertainty and its risk implications.\n\nAcknowledgements: Additional thanks go to other partner organizations and prize sponsors, National Technical University of Athens (NTUA), INSEAD, Google, Uber and IIF.",
      "evaluation_metric": "## Evaluation\n\nThis competition uses a Weighted Scaled Pinball Loss (WSPL). Extensive details about the metric, scaling, and weighting can be found in the M5 Participants Guide.",
      "dataset_description": "Data description:\nUniversity of Nicosia\nM5 Forecasting - Uncertainty\nEstimate the uncertainty distribution of Walmart unit sales.\n\nDataset Description\nIn the challenge, you are predicting 9 quartiles of item sales at stores in various locations for two 28-day time periods. Information about the data is found in theM5 Participants Guide.\n\nFiles:\ncalendar.csv - Contains information about the dates on which the products are sold.\nsales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\nsample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\nsell_prices.csv - Contains information about the price of the products sold per store and date.\nsales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n\nFiles: 5 files\nSize: 515.48 MB\nType: csv\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "forecasting",
          "time_series",
          "quantile_regression",
          "retail_sales",
          "weighted_scaled_pinball_loss"
        ]
      }
    },
    {
      "challenge_name": "march-machine-learning-mania-2023",
      "description": "Challenge description:\nForecast the 2023 NCAA Basketball Tournaments\nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our ninth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.\nYou are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2023 edition, with points, medals, prizes, and basketball glory at stake.\nWe have made several updates to the competition format compared to prior editions:\nWe have also launched a companion warmup competition, which is setup as a practice leaderboard covering the previous five tournaments. Because its only for practice and the ground historical game outcomes are public information, the warmup competition does not count for points/medals and will be taken down once it has served its purpose. Prior to the start of the tournaments, the leaderboard of this competition will reflect all zero scores. Kaggle will periodically fill in the outcomes and rescore once games begin.\nGood luck and happy forecasting!\nThe evaluation methodology for 2023 has changed from prior editions of this competition. Submissions are now evaluated on the Brier score between the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context). This change was made to reduce the competitive \"distractions\" caused by the 0 and 1 boundaries of the previous log-loss metric (e.g. submitting rounded predictions to gamble on a given upset, or caring deeply about the 0.99 vs 0.999 distinction that log loss would reward/punish).\nThe submission file format also has a revised format for 2023:\nAs with prior years, each game has a unique ID created by concatenating the season in which the game was played and the two team's respective TeamIds. For example, \"2023_1101_1102\" indicates a hypothetical matchup between team 1101 and 1102 in the year 2023. You must predict the probability that the team with the lower TeamId beats the team with the higher TeamId. Note that the men's teams and women's TeamIds do not overlap.\nThe resulting submission format looks like the following, where Pred represents the predicted probability that the first team will win:\nID,Pred\n2023_1101_1102,0.5\n2023_1101_1103,0.5\n2023_1101_1104,0.5\n...\nYour 2023 submissions will score 0.0 if you have submitted predictions in the right format. The leaderboard of this competition will be only meaningful once the 2023 tournaments begin and Kaggle rescores your predictions!\nFebruary 15, 2022 - Start Date\nMarch 16, 2023 4PM UTC - Final Submission Deadline. Note that Kaggle will release updated data at least once in advance of the deadline in order to include as much of the current season's data as possible.\nMarch 16 - April 3 - Watch your tournament results play out! Kaggle will refresh the leaderboard throughout the tournaments.\nThe organizers reserve the right to update the contest timeline if they deem it necessary.\nJeff Sonas, Maggie, and Will Cukierski. March Machine Learning Mania 2023. https://kaggle.com/competitions/march-machine-learning-mania-2023, 2023. Kaggle.\nAwards Points & Medals\n4,446 Entrants\n1,098 Participants\n1,033 Teams\n1,821 Submissions\nMarch Machine Learning Mania 2023\nCompetition Host\nPrizes & Awards\nParticipation\nDescription\nGoal of the Competition\nSubmission File\nGoal of the Competition\nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our ninth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.\nContext\nYou are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2023 edition, with points, medals, prizes, and basketball glory at stake.\nWe have made several updates to the competition format compared to prior editions:\nThere is a change in evaluation metric from log loss to Brier scores. See the Evaluation Page for full details.\nWe are combining the Men's and Women's tournament into one single competition, instead of running separate tracks. The competition will award full points/medals as a result.\nWe have changed the prediction format so that you may forecast the 2023 tournaments right away, instead of having to wait to see which teams are selected for the tournament.\nWe have also launched a companion warmup competition, which is setup as a practice leaderboard covering the previous five tournaments. Because its only for practice and the ground historical game outcomes are public information, the warmup competition does not count for points/medals and will be taken down once it has served its purpose. Prior to the start of the tournaments, the leaderboard of this competition will reflect all zero scores. Kaggle will periodically fill in the outcomes and rescore once games begin.\nGood luck and happy forecasting!\nEvaluation\nThe evaluation methodology for 2023 has changed from prior editions of this competition. Submissions are now evaluated on the Brier score between the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context). This change was made to reduce the competitive \"distractions\" caused by the 0 and 1 boundaries of the previous log-loss metric (e.g. submitting rounded predictions to gamble on a given upset, or caring deeply about the 0.99 vs 0.999 distinction that log loss would reward/punish).\nSubmission File\nThe submission file format also has a revised format for 2023:\nWe have combined the Men's and Women's tournaments into one single competition. Your submission file should contain predictions for both.\nYou will now be predicting the hypothetical results for every possible team matchup, not just teams that are selected for the NCAA tournament.\nThis change was enacted to provide a longer time window to submit predictions for the 2023 tournament. Previously, the short time between Selection Sunday and the tournament tipoffs would require participants to quickly turn around updated predictions. By forecasting every possible outcome between every team, you can now submit a valid prediction at any point leading up to the tournaments.\nYou may submit as many times as you wish before the tournaments start, but make sure to select the two submissions you want to count towards scoring. Do not rely on automatic selection to pick your submissions, as there is no public leaderboard score and the system will select your earliest two submissions.\nAs with prior years, each game has a unique ID created by concatenating the season in which the game was played and the two team's respective TeamIds. For example, \"2023_1101_1102\" indicates a hypothetical matchup between team 1101 and 1102 in the year 2023. You must predict the probability that the team with the lower TeamId beats the team with the higher TeamId. Note that the men's teams and women's TeamIds do not overlap.\nThe resulting submission format looks like the following, where Pred represents the predicted probability that the first team will win:\nID,Pred\n2023_1101_1102,0.5\n2023_1101_1103,0.5\n2023_1101_1104,0.5\n...\nYour 2023 submissions will score 0.0 if you have submitted predictions in the right format. The leaderboard of this competition will be only meaningful once the 2023 tournaments begin and Kaggle rescores your predictions!\nTimeline\nFebruary 15, 2022 - Start Date\nMarch 16, 2023 4PM UTC - Final Submission Deadline. Note that Kaggle will release updated data at least once in advance of the deadline in order to include as much of the current season's data as possible.\nMarch 16 - April 3 - Watch your tournament results play out! Kaggle will refresh the leaderboard throughout the tournaments.\nThe organizers reserve the right to update the contest timeline if they deem it necessary.\nPrizes\n1st Place - $10,000\n2nd Place - $8,000\n3rd Place - $7,000\n4th - 8th Place(s) - $5,000\nCitation\nJeff Sonas, Maggie, and Will Cukierski. March Machine Learning Mania 2023. https://kaggle.com/competitions/march-machine-learning-mania-2023, 2023. Kaggle.\n\nData description:\n# March Machine Learning Mania 2023: Forecast the 2023 NCAA Basketball Tournaments\n\nEach season there are thousands of NCAA basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia pages for the men's and women's tournaments before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nPlease note that in previous years, there were separate competitions for predicting the men's tournament games or the women's tournament games. In this year's competition, you will be submitting combined prediction files that include predictions for both the men's tournament and the women's tournament. Thus the data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences, and these files do not start with an M prefix or a W prefix.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nWarmup Competition - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2017-2019 and 2021-2022). Note that there was no tournament held in 2020.\n\n2023 Competition - You should submit predicted probabilities for every possible matchup before the 2023 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the competition data files. All of the files are complete through February 7th of the current season. As we get closer to the tournament in mid-March, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nSpecial note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first regular season games were played in November 2022 and the national championship games will be played in April 2023. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2023 season, not the 2022 season or the 2022-23 season or the 2022-2023 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: MTeams.csv and WTeams.csv**\n\nThese files identify the different college teams present in the dataset (MTeams is for the men's teams and WTeams is for the women's teams). Each school is uniquely identified by a 4 digit id number. Men's team id's start with a 1 and women's team id's start with a 3, and typically there is exactly a difference of 2000 between the men's and women's team id's for a given school. For example, the men's Arizona State team id is 1113 and the women's Arizona State team id is 3113. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 363 teams currently in Men's Division-I and 361 teams currently in Women's Division-I. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs. So there will be some teams listed in the data only for historical seasons and not for the current season, and thus there are more than 363 men's teams and more than 361 women's teams listed.\n\n- TeamID - a 4 digit id number, uniquely identifying each NCAA\u00ae men's or women's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the men's 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985. This column is only present in the men's data, so it is not found in WTeams.csv.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2023. Again, this column is only present in the men's data, so it is not found in WTeams.csv.\n\n**Data Section 1 file: MSeasons.csv and WSeasons.csv**\n\nThese files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons).\n\n- Season - indicates the year in which the tournament was played. Remember that the current season counts as 2023.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the men's national semifinals are always on DayNum=152, the \"play-in\" games are on days 134-135, men's Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011. Also note that the men's and women's data share the same DayZero each season, although the women's championship game is not necessarily played on DayNum=154\n- RegionW, RegionX, Region Y, Region Z - by our competitions' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files, even if the region names change from year to year. For instance, during the 2012 men's tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the MNCAATourneySeeds file that year with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until the brackets are announced on Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv**\n\nThese files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 12, 2023 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the MNCAATourneySeeds file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"MSeasons.csv\" file was the East region).\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv or WTeams.csv file\n\n**Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv**\n\nThese files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv, namely the year in which the final tournament occurs. For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" or \"WSeasons.csv\" file. For example, the first game in the \"MRegularSeasonCompactResults.csv\" file was DayNum=20. Combined with the fact from the \"MSeasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the men's SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" or \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team. Please note that in this case the \"W\" in \"WTeamID does not refer to women's data; the \"W\" is for \"winning\". Both the men's data and women's data will identify the winning team id by this WTeamID column. The same note applies to WScore and WLoc below - these are \"W\" for \"winning\" and not for \"women's\".\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting (or \"away\") team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv**\n\nThese files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding RegularSeasonCompactResults data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics. Note that this tournament game data also includes the play-in games for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a game was, depending on the exact DayNum. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files (only for men's data) within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionWarmup.csv and SampleSubmission2023**\n\nThese files illustrate the submission file format for the warmup competition and the 2023 competition. They reflect the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. Please remember that this file includes predictions for both men's games and women's games, and that it includes all Division I teams, not just the tournament teams.\n\nA submission file lists every possible tournament matchup between Division-I teams for one or more years. In the 2023 competition, you will be asked to make predictions for the current season (2023).\n\nThere are two very important differences between this year's submission file and the submission files used in previous years' competitions.\n\nIMPORTANT DIFFERENCE #1: The competition is a blend of men's predictions and women's predictions. Thus your score for a given season will be your overall score across the 126 games (63 men's and 63 women's) that were played among the final 64 teams in each tournament. You cannot submit predictions just for the men's competition or just for the women's competition; the submission file must include predictions for both.\n\nIMPORTANT DIFFERENCE #2: The competition supports submissions even before tournament brackets are announced. Therefore, a submission file includes all possible combinations of two teams, rather than all possible combinations of two tournament teams. For example, there are 363 men's Division-I teams this season. A submission file will need to include predictions for all possible pairs of those 363 men's teams, and there are 363*362/2 = 65,703 possible combinations. Similarly, on the women's side, there are 361 Division-I teams this season, which corresponds to 361*360/2=64,980 possible combinations. So a submission file for the 2023 season would have 65,703+64,980=130,683 data rows. Most prediction rows will be discarded because one or both teams failed to make the tournament, or didn't play each other in the tournament.\n\nIf you want to know which predictions are needed, you can simply parse the data rows in the sample submission file, or you can calculate the combinations yourself. You will need to remember that not all teams in the MTeams or WTeams files are active each year. The easiest way to get a master list of the active teams each year is to look at the MTeamConferences and WTeamConferences data files, which will have one record each season for each active Division-I team, along with telling you which conference they were in. For example, you will find 363 data rows in the MTeamConferences file for the 2023 season, and you will find 361 data rows in the WTeamConferences file for the 2023 season.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 men's tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2017_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2018 women's tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2018_3181_3314,0.516\n\nAlso note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke.\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season (men) or since the 2009-10 season (women).\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv**\n\nThese files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file.\n\n**Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv**\n\nThese files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or MNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: MGameCities.csv and WGameCities.csv**\n\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (men's data only), are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the corresponding Regular Season Compact Results and Regular Season Detailed Results files. If it is NCAA, you can find more about the game in the corresponding NCAA Tourney Compact Results and NCAA Tourney Detailed Results files. If it is Secondary, you can find more about the game in the MSecondaryTourneyCompactResults file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MMasseyOrdinals.csv**\n\nThis file lists out rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which generally start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n**Data Section 5 file: MTeamCoaches.csv**\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the calendar year in which the final tournament occurs)\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 5 file: Conferences.csv**\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n**Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv**\n\nThese files indicate the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. These files tracks this information historically, for men's and women's teams separately.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv or WTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 5 file: MConferenceTourneyGames.csv**\n\nThis file indicates which games were part of each year's post-season men's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the MRegularSeasonCompactResults and/or MRegularSeasonDetailedResults files.\n\n**Data Section 5 file: MSecondaryTourneyTeams.csv**\n\nThis file identifies the teams that participated in post-season men's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, Vegas 16 (V16), and The Basketball Classic (TBC) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv).\n\n**Data Section 5 file: MSecondaryTourneyCompactResults.csv**\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n\n**Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv**\n\nThese files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv or WTeams.csv).\n\n**Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots**\n\nThese files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. They can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs). Please note that in recent years, the women's tournament has expanded from 64 to 68 teams, which means this Tourney Slots information is no longer the same every year. Previously there was an WNCAATourneySlots file without a Season column, but starting this year we have switched over to making the WNCAATourneySlots file match the format of the men's file, so now it will have a Season column and there is a complete set of Tourney Slots data in the file for each season listed. The 2022 women's tournament was the first one with play-in games, and we expect they may be distributed differently this season, and so there is a need for season-specific Tourney Slots data rather than just one global set that could always be used for the 64-team brackets.\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of the men's file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 5 file: MNCAATourneySeedRoundSlots.csv**\n\nThis file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the MNCAATourneySlots file.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "docker_challenge_path": "/data/march-machine-learning-mania-2023",
      "competition_description": "Challenge description:\nForecast the 2023 NCAA Basketball Tournaments\nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our ninth annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.\nYou are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2023 edition, with points, medals, prizes, and basketball glory at stake.\nWe have made several updates to the competition format compared to prior editions:\nWe have also launched a companion warmup competition, which is setup as a practice leaderboard covering the previous five tournaments. Because its only for practice and the ground historical game outcomes are public information, the warmup competition does not count for points/medals and will be taken down once it has served its purpose. Prior to the start of the tournaments, the leaderboard of this competition will reflect all zero scores. Kaggle will periodically fill in the outcomes and rescore once games begin.\nGood luck and happy forecasting!",
      "evaluation_metric": "The evaluation methodology for 2023 has changed from prior editions of this competition. Submissions are now evaluated on the Brier score between the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context). This change was made to reduce the competitive \"distractions\" caused by the 0 and 1 boundaries of the previous log-loss metric (e.g. submitting rounded predictions to gamble on a given upset, or caring deeply about the 0.99 vs 0.999 distinction that log loss would reward/punish).",
      "dataset_description": "Data description:\n# March Machine Learning Mania 2023: Forecast the 2023 NCAA Basketball Tournaments\n\nEach season there are thousands of NCAA basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia pages for the men's and women's tournaments before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nPlease note that in previous years, there were separate competitions for predicting the men's tournament games or the women's tournament games. In this year's competition, you will be submitting combined prediction files that include predictions for both the men's tournament and the women's tournament. Thus the data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences, and these files do not start with an M prefix or a W prefix.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings and WTeamSpellings files, which are listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nWarmup Competition - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2017-2019 and 2021-2022). Note that there was no tournament held in 2020.\n\n2023 Competition - You should submit predicted probabilities for every possible matchup before the 2023 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the competition data files. All of the files are complete through February 7th of the current season. As we get closer to the tournament in mid-March, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nSpecial note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first regular season games were played in November 2022 and the national championship games will be played in April 2023. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2023 season, not the 2022 season or the 2022-23 season or the 2022-2023 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: MTeams.csv and WTeams.csv**\n\nThese files identify the different college teams present in the dataset (MTeams is for the men's teams and WTeams is for the women's teams). Each school is uniquely identified by a 4 digit id number. Men's team id's start with a 1 and women's team id's start with a 3, and typically there is exactly a difference of 2000 between the men's and women's team id's for a given school. For example, the men's Arizona State team id is 1113 and the women's Arizona State team id is 3113. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 363 teams currently in Men's Division-I and 361 teams currently in Women's Division-I. Each year, some teams might start being Division-I programs, and others might stop being Division-I programs. So there will be some teams listed in the data only for historical seasons and not for the current season, and thus there are more than 363 men's teams and more than 361 women's teams listed.\n\n- TeamID - a 4 digit id number, uniquely identifying each NCAA\u00ae men's or women's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the men's 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985. This column is only present in the men's data, so it is not found in WTeams.csv.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2023. Again, this column is only present in the men's data, so it is not found in WTeams.csv.\n\n**Data Section 1 file: MSeasons.csv and WSeasons.csv**\n\nThese files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons).\n\n- Season - indicates the year in which the tournament was played. Remember that the current season counts as 2023.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the men's national semifinals are always on DayNum=152, the \"play-in\" games are on days 134-135, men's Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011. Also note that the men's and women's data share the same DayZero each season, although the women's championship game is not necessarily played on DayNum=154\n- RegionW, RegionX, Region Y, Region Z - by our competitions' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files, even if the region names change from year to year. For instance, during the 2012 men's tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the MNCAATourneySeeds file that year with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until the brackets are announced on Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv**\n\nThese files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 12, 2023 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the MNCAATourneySeeds file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"MSeasons.csv\" file was the East region).\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv or WTeams.csv file\n\n**Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv**\n\nThese files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv, namely the year in which the final tournament occurs. For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" or \"WSeasons.csv\" file. For example, the first game in the \"MRegularSeasonCompactResults.csv\" file was DayNum=20. Combined with the fact from the \"MSeasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the men's SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" or \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team. Please note that in this case the \"W\" in \"WTeamID does not refer to women's data; the \"W\" is for \"winning\". Both the men's data and women's data will identify the winning team id by this WTeamID column. The same note applies to WScore and WLoc below - these are \"W\" for \"winning\" and not for \"women's\".\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting (or \"away\") team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv**\n\nThese files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding RegularSeasonCompactResults data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics. Note that this tournament game data also includes the play-in games for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a game was, depending on the exact DayNum. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files (only for men's data) within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionWarmup.csv and SampleSubmission2023**\n\nThese files illustrate the submission file format for the warmup competition and the 2023 competition. They reflect the simplest possible submission: a 50% winning percentage is predicted for each possible matchup. Please remember that this file includes predictions for both men's games and women's games, and that it includes all Division I teams, not just the tournament teams.\n\nA submission file lists every possible tournament matchup between Division-I teams for one or more years. In the 2023 competition, you will be asked to make predictions for the current season (2023).\n\nThere are two very important differences between this year's submission file and the submission files used in previous years' competitions.\n\nIMPORTANT DIFFERENCE #1: The competition is a blend of men's predictions and women's predictions. Thus your score for a given season will be your overall score across the 126 games (63 men's and 63 women's) that were played among the final 64 teams in each tournament. You cannot submit predictions just for the men's competition or just for the women's competition; the submission file must include predictions for both.\n\nIMPORTANT DIFFERENCE #2: The competition supports submissions even before tournament brackets are announced. Therefore, a submission file includes all possible combinations of two teams, rather than all possible combinations of two tournament teams. For example, there are 363 men's Division-I teams this season. A submission file will need to include predictions for all possible pairs of those 363 men's teams, and there are 363*362/2 = 65,703 possible combinations. Similarly, on the women's side, there are 361 Division-I teams this season, which corresponds to 361*360/2=64,980 possible combinations. So a submission file for the 2023 season would have 65,703+64,980=130,683 data rows. Most prediction rows will be discarded because one or both teams failed to make the tournament, or didn't play each other in the tournament.\n\nIf you want to know which predictions are needed, you can simply parse the data rows in the sample submission file, or you can calculate the combinations yourself. You will need to remember that not all teams in the MTeams or WTeams files are active each year. The easiest way to get a master list of the active teams each year is to look at the MTeamConferences and WTeamConferences data files, which will have one record each season for each active Division-I team, along with telling you which conference they were in. For example, you will find 363 data rows in the MTeamConferences file for the 2023 season, and you will find 361 data rows in the WTeamConferences file for the 2023 season.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 men's tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2017_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2018 women's tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2018_3181_3314,0.516\n\nAlso note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke.\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season (men) or since the 2009-10 season (women).\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv**\n\nThese files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file.\n\n**Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv**\n\nThese files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or MNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: MGameCities.csv and WGameCities.csv**\n\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (men's data only), are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the corresponding Regular Season Compact Results and Regular Season Detailed Results files. If it is NCAA, you can find more about the game in the corresponding NCAA Tourney Compact Results and NCAA Tourney Detailed Results files. If it is Secondary, you can find more about the game in the MSecondaryTourneyCompactResults file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MMasseyOrdinals.csv**\n\nThis file lists out rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which generally start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n**Data Section 5 file: MTeamCoaches.csv**\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the calendar year in which the final tournament occurs)\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 5 file: Conferences.csv**\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n**Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv**\n\nThese files indicate the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. These files tracks this information historically, for men's and women's teams separately.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv or WTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 5 file: MConferenceTourneyGames.csv**\n\nThis file indicates which games were part of each year's post-season men's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the MRegularSeasonCompactResults and/or MRegularSeasonDetailedResults files.\n\n**Data Section 5 file: MSecondaryTourneyTeams.csv**\n\nThis file identifies the teams that participated in post-season men's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, Vegas 16 (V16), and The Basketball Classic (TBC) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv).\n\n**Data Section 5 file: MSecondaryTourneyCompactResults.csv**\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n\n**Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv**\n\nThese files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv or WTeams.csv).\n\n**Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots**\n\nThese files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. They can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs). Please note that in recent years, the women's tournament has expanded from 64 to 68 teams, which means this Tourney Slots information is no longer the same every year. Previously there was an WNCAATourneySlots file without a Season column, but starting this year we have switched over to making the WNCAATourneySlots file match the format of the men's file, so now it will have a Season column and there is a complete set of Tourney Slots data in the file for each season listed. The 2022 women's tournament was the first one with play-in games, and we expect they may be distributed differently this season, and so there is a need for season-specific Tourney Slots data rather than just one global set that could always be used for the 64-team brackets.\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of the men's file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 5 file: MNCAATourneySeedRoundSlots.csv**\n\nThis file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the MNCAATourneySlots file.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "forecasting",
          "tabular",
          "feature_engineering",
          "sports",
          "brier_score"
        ]
      }
    },
    {
      "challenge_name": "march-machine-learning-mania-2025",
      "description": "Challenge description:\nForecast the 2025 NCAA Basketball Tournaments\nYou will be forecasting the outcomes of both the men's and women's 2025 collegiate basketball tournaments, by submitting predictions for every possible tournament matchup.\n\nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In oureleventhannual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.\nYou are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2025 edition, with points, medals, prizes, and basketball glory at stake.\nWe have reverted back to the formatfrom 2023where you are making predictions about every possible matchup in the tournament, evaluated using the Brier score. See theEvaluation Pagefor full details.\nPrior to the start of the tournaments, the leaderboard of this competition will reflect scores from 2021-2024 only. Kaggle will periodically fill in the outcomes and rescore once the 2025 games begin.\nGood luck and happy forecasting!\n\nSubmissions are evaluated on theBrier scorebetween the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context).\n\nThe submission file format also has a revised format for 2025:\nWe have combined the Men's and Women's tournaments into one single competition.\nYour submission file should contain predictions for both.\nYou will now be predicting the hypothetical results for every possible team matchup, not just teams that are selected for the NCAA tournament.\nThis change was enacted to provide a longer time window to submit predictions for the 2025 tournament. Previously, the short time between Selection Sunday and the tournament tipoffs would require participants to quickly turn around updated predictions. By forecasting every possible outcome between every team, you can now submit a valid prediction at any point leading up to the tournaments.\nYou may submit as many times as you wish before the tournaments start, butmake sure to select the two submissions you want to count towards scoring. Do not rely on automatic selection to pick your submissions.\nAs with prior years, each game has a uniqueIDcreated by concatenating the season in which the game was played and the two team's respectiveTeamIds. For example, \"2025_1101_1102\" indicates a hypothetical matchup between team 1101 and 1102 in the year 2025. You must predict the probability that the team with the lowerTeamIdbeats the team with the higherTeamId. Note that the men's teams and women'sTeamIds do not overlap.\nThe resulting submission format looks like the following, wherePredrepresents the predicted probability that the first team will win:\nID,Pred\n2025_1101_1102,0.5\n2025_1101_1103,0.5\n2025_1101_1104,0.5\n...\nYour 2025 submissions will score 0.0 if you have submitted predictions in the right format. The leaderboard of this competition will be only meaningful once the 2025 tournaments begin and Kaggle rescores your predictions!\n\nFebruary 10, 2025- Start Date\nWeek of February 18-21, 2025- 2025 Tournament Submission File Available\nMarch 20, 2025  4PM UTC- Final Submission Deadline. Note that Kaggle will release updated data at least once in advance of the deadline in order to include as much of the current season's data as possible.\nMarch 20 - April 8- Watch your tournament results play out! Kaggle will refresh the leaderboard throughout the tournaments.\nThe organizers reserve the right to update the contest timeline if they deem it necessary.\n\n1st Place - $10,000\n2nd Place - $8,000\n3rd Place - $7,000\n4th - 8th Place(s) - $5,000\n\nJeff Sonas, Paul Mooney, Addison Howard, and Will Cukierski. March Machine Learning Mania 2025. https://kaggle.com/competitions/march-machine-learning-mania-2025, 2025. Kaggle.\n\nData description:\n# March Machine Learning Mania 2025: Forecast the 2025 NCAA Basketball Tournaments\n\nEach season there are thousands of NCAA\u00ae basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the national championship men's and women's tournaments that run from mid-March until their championship games in early April. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes.\n\nThe data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences. The MTeamSpellings and WTeamSpellings files may help you map external team references into our own Team ID structure.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## File descriptions\n\nBelow we describe the format and fields of the competition data files. All of the files are complete through January 28th of the current season. As we get closer to the tournament in mid-March, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n## Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nBy convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in.\n\n### Data Section 1 file: MTeams.csv and WTeams.csv\n\nThese files identify the different college teams present in the dataset.\n\n- TeamID - a 4 digit id number, uniquely identifying each NCAA\u00ae men's or women's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. The men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. This column is only present in the men's data, so it is not found in WTeams.csv.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2025. Again, this column is only present in the men's data, so it is not found in WTeams.csv.\n\n### Data Section 1 file: MSeasons.csv and WSeasons.csv\n\nThese files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons).\n\n- Season - indicates the year in which the tournament was played.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the men's national semifinals are always on DayNum=152, the men's \"play-in\" games are on days 134-135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011. Also note that the men's and women's data share the same DayZero each season, although the women's championship game is not necessarily played on DayNum=154.\n- RegionW, RegionX, Region Y, Region Z - by our competitions' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions in a standardized way in other files, even if the region names change from year to year.\n\n### Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv\n\nThese files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 16, 2025 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3-character or 4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically.\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv or WTeams.csv file\n\n### Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv\n\nThese files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself).\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv, namely the year in which the final tournament occurs.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" or \"WSeasons.csv\" file.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" or \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team. Both the men's data and women's data will identify the winning team id by this WTeamID column. The same note applies to WScore and WLoc below - these are \"W\" for \"winning\" and not for \"women's\".\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting (or \"away\") team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\".\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n### Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv\n\nThese files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding Regular Season Compact Results data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a men's game was, just by looking at its day number. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\n### Data Section 1 file: SampleSubmissionStage1.csv\n\nThis file illustrates the submission file format for the \"warmup\" Stage 1 competition. It reflects the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nIf you want to know which predictions are needed, you can simply parse the data rows in the sample submission file.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\n## Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2003 season (men) or since the 2010 season (women).\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as (2*FGM) + FGM3 + FTM.\n\n### Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv\n\nThese files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. However, earlier women's data is challenging to collect, and so approximately 1.5% of women's games in the 2010, 2011, and 2012 seasons are unavailable in the data. All games from 2013 to the present should have detailed results present. In previous years, there was also a small amount of detailed results missing from the women's seasons 2013 and 2014, but those have been brought up to 100% completion in this year's data. Also, there was a nagging problem in recent years where many games during the 2021 season showed zero personal fouls for both teams; the correct personal foul numbers have been reconstructed this year and so there are no games with zero personal fouls anymore from 2021, just like in other seasons.\n\n### Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv\n\nThese files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or WNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file.\n\n## Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2010 season.\n\n### Data Section 3 file: Cities.csv\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n### Data Section 3 file: MGameCities.csv and WGameCities.csv\n\nThese files identify all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (known as secondary tournaments), are all listed together. Games from the 2009 season and before are not listed in this file. Almost all games played in women's seasons 2010, 2011, and 2012 are listed, although approximately 1%-2% of games from those seasons are not easily available and are not listed. In previous seasons, there were also missing women's game cities from some games during seasons 2013-2018, but in this year's data we now have 100% coverage for women's game cities for all seasons since 2013.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the corresponding Regular Season Compact Results and Regular Season Detailed Results files. If it is NCAA, you can find more about the game in the corresponding NCAA Tourney Compact Results and NCAA Tourney Detailed Results files. If it is Secondary, you can find more about the game in the Secondary Tourney Compact Results file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n## Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2003 season.\n\n### Data Section 4 file: MMasseyOrdinals.csv\n\nThis file lists ordinal rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his rankings page.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which generally start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey rankings page (linked to above), you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline.\n\n## Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n### Data Section 5 file: MTeamCoaches.csv\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team.\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces, and all letters being lowercase.\n\n### Data Section 5 file: Conferences.csv\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n### Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv\n\nThese files indicate the conference affiliations for each team during each season. Some conferences have added or dropped teams from year to year, and these files track this information historically, for men's and women's teams separately.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv or WTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n### Data Section 5 file: MConferenceTourneyGames.csv and WConferenceTourneyGames.csv\n\nThis file indicates which games were part of each year's post-season men's and women's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season for men's data and the 2002 season for women's data.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the Regular Season Compact Results and/or Regular Season Detailed Results files.\n\n### Data Section 5 file: MSecondaryTourneyTeams.csv and WSecondaryTourneyTeams\n\nThis file identifies the teams that participated in post-season men's or women's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament).\n\n- Season - this is the year of the associated entry in the Seasons file (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, such as NIT or WNIT.\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv or WTeams.csv).\n\n### Data Section 5 file: MSecondaryTourneyCompactResults.csv and WSecondaryTourneyCompactResults\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the Regular Season Compact Results file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n\n### Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv\n\nThese files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv or WTeams.csv).\n\n### Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots\n\nThese files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs).\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of the men's file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n### Data Section 5 file: MNCAATourneySeedRoundSlots.csv\n\nThis file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on. Many tournament rounds span two days' time and so the expected day number for a game to be played on would be one of those two days.",
      "docker_challenge_path": "/data/march-machine-learning-mania-2025",
      "competition_description": "Challenge description:\nForecast the 2025 NCAA Basketball Tournaments\nYou will be forecasting the outcomes of both the men's and women's 2025 collegiate basketball tournaments, by submitting predictions for every possible tournament matchup.\n\nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In oureleventhannual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's college basketball tournaments. Unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.\nYou are provided data of historical NCAA games to forecast the outcomes of the Division 1 Men's and Women's basketball tournaments. This competition is the official 2025 edition, with points, medals, prizes, and basketball glory at stake.\nWe have reverted back to the formatfrom 2023where you are making predictions about every possible matchup in the tournament, evaluated using the Brier score. See theEvaluation Pagefor full details.\nPrior to the start of the tournaments, the leaderboard of this competition will reflect scores from 2021-2024 only. Kaggle will periodically fill in the outcomes and rescore once the 2025 games begin.\nGood luck and happy forecasting!",
      "evaluation_metric": "Submissions are evaluated on theBrier scorebetween the predicted probabilities and the actual game outcomes (this is equivalent to mean squared error in this context).",
      "dataset_description": "Data description:\n# March Machine Learning Mania 2025: Forecast the 2025 NCAA Basketball Tournaments\n\nEach season there are thousands of NCAA\u00ae basketball games played between Division I college basketball teams, culminating in March Madness\u00ae, the national championship men's and women's tournaments that run from mid-March until their championship games in early April. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes.\n\nThe data files incorporate both men's data and women's data. The files that pertain only to men's data will start with the letter prefix M, and the files that pertain only to women's data will start with the letter prefix W. Some files span both men's and women's data, such as Cities and Conferences. The MTeamSpellings and WTeamSpellings files may help you map external team references into our own Team ID structure.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## File descriptions\n\nBelow we describe the format and fields of the competition data files. All of the files are complete through January 28th of the current season. As we get closer to the tournament in mid-March, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n## Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nBy convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in.\n\n### Data Section 1 file: MTeams.csv and WTeams.csv\n\nThese files identify the different college teams present in the dataset.\n\n- TeamID - a 4 digit id number, uniquely identifying each NCAA\u00ae men's or women's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. The men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. This column is only present in the men's data, so it is not found in WTeams.csv.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2025. Again, this column is only present in the men's data, so it is not found in WTeams.csv.\n\n### Data Section 1 file: MSeasons.csv and WSeasons.csv\n\nThese files identify the different seasons included in the historical data, along with certain season-level properties. There are separate files for men's data (MSeasons) and women's data (WSeasons).\n\n- Season - indicates the year in which the tournament was played.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the men's national semifinals are always on DayNum=152, the men's \"play-in\" games are on days 134-135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011. Also note that the men's and women's data share the same DayZero each season, although the women's championship game is not necessarily played on DayNum=154.\n- RegionW, RegionX, Region Y, Region Z - by our competitions' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions in a standardized way in other files, even if the region names change from year to year.\n\n### Data Section 1 file: MNCAATourneySeeds.csv and WNCAATourneySeeds.csv\n\nThese files identify the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday/Friday of the first week (by definition, that is DayNum=136/137 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 16, 2025 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3-character or 4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically.\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv or WTeams.csv file\n\n### Data Section 1 file: MRegularSeasonCompactResults.csv and WRegularSeasonCompactResults.csv\n\nThese files identify the game-by-game results for many seasons of historical data, starting with the 1985 season for men (the first year the NCAA\u00ae had a 64-team men's tournament) and the 1998 season for women. For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself).\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv, namely the year in which the final tournament occurs.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" or \"WSeasons.csv\" file.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" or \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team. Both the men's data and women's data will identify the winning team id by this WTeamID column. The same note applies to WScore and WLoc below - these are \"W\" for \"winning\" and not for \"women's\".\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting (or \"away\") team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\".\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n### Data Section 1 file: MNCAATourneyCompactResults.csv and WNCAATourneyCompactResults.csv\n\nThese files identify the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the corresponding Regular Season Compact Results data. All men's games will show up as neutral site (so WLoc is always N) and some women's games will show up as neutral site, depending on the specifics.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can generally tell what round a men's game was, just by looking at its day number. However, the men's 2021 tournament scheduling was slightly different, and the women's scheduling has varied a lot. Nevertheless, in general the men's schedule will be:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\n### Data Section 1 file: SampleSubmissionStage1.csv\n\nThis file illustrates the submission file format for the \"warmup\" Stage 1 competition. It reflects the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nIf you want to know which predictions are needed, you can simply parse the data rows in the sample submission file.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\n## Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2003 season (men) or since the 2010 season (women).\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as (2*FGM) + FGM3 + FTM.\n\n### Data Section 2 file: MRegularSeasonDetailedResults.csv and WRegularSeasonDetailedResults.csv\n\nThese files provide team-level box scores for many regular seasons of historical data, starting with the 2003 season (men) or starting with the 2010 season (women). All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file, and similarly, all games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file. However, earlier women's data is challenging to collect, and so approximately 1.5% of women's games in the 2010, 2011, and 2012 seasons are unavailable in the data. All games from 2013 to the present should have detailed results present. In previous years, there was also a small amount of detailed results missing from the women's seasons 2013 and 2014, but those have been brought up to 100% completion in this year's data. Also, there was a nagging problem in recent years where many games during the 2021 season showed zero personal fouls for both teams; the correct personal foul numbers have been reconstructed this year and so there are no games with zero personal fouls anymore from 2021, just like in other seasons.\n\n### Data Section 2 file: MNCAATourneyDetailedResults.csv and WNCAATourneyDetailedResults.csv\n\nThese files provide team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season (men) or starting with the 2010 season (women). Similarly, all games listed in the MNCAATourneyCompactResults or WNCAATourneyCompactResults file for those seasons should exactly be present in the corresponding MNCAATourneyDetailedResults or WNCAATourneyDetailedResults file.\n\n## Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2010 season.\n\n### Data Section 3 file: Cities.csv\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two datasets. Also note that if you created any supplemental data in previous years on cities (latitude/longitude, altitude, city-to-city distances, etc.), the CityID's match between previous years and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n### Data Section 3 file: MGameCities.csv and WGameCities.csv\n\nThese files identify all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments (known as secondary tournaments), are all listed together. Games from the 2009 season and before are not listed in this file. Almost all games played in women's seasons 2010, 2011, and 2012 are listed, although approximately 1%-2% of games from those seasons are not easily available and are not listed. In previous seasons, there were also missing women's game cities from some games during seasons 2013-2018, but in this year's data we now have 100% coverage for women's game cities for all seasons since 2013.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the corresponding Regular Season Compact Results and Regular Season Detailed Results files. If it is NCAA, you can find more about the game in the corresponding NCAA Tourney Compact Results and NCAA Tourney Detailed Results files. If it is Secondary, you can find more about the game in the Secondary Tourney Compact Results file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n## Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings (men's teams only) for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2003 season.\n\n### Data Section 4 file: MMasseyOrdinals.csv\n\nThis file lists ordinal rankings (e.g. #1, #2, #3, ..., #N) of men's teams going back to the 2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his rankings page.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which generally start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey rankings page (linked to above), you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline.\n\n## Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n### Data Section 5 file: MTeamCoaches.csv\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team.\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces, and all letters being lowercase.\n\n### Data Section 5 file: Conferences.csv\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n### Data Section 5 files: MTeamConferences.csv and WTeamConferences.csv\n\nThese files indicate the conference affiliations for each team during each season. Some conferences have added or dropped teams from year to year, and these files track this information historically, for men's and women's teams separately.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv or WTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n### Data Section 5 file: MConferenceTourneyGames.csv and WConferenceTourneyGames.csv\n\nThis file indicates which games were part of each year's post-season men's and women's conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season for men's data and the 2002 season for women's data.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the Regular Season Compact Results and/or Regular Season Detailed Results files.\n\n### Data Section 5 file: MSecondaryTourneyTeams.csv and WSecondaryTourneyTeams\n\nThis file identifies the teams that participated in post-season men's or women's tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament).\n\n- Season - this is the year of the associated entry in the Seasons file (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, such as NIT or WNIT.\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv or WTeams.csv).\n\n### Data Section 5 file: MSecondaryTourneyCompactResults.csv and WSecondaryTourneyCompactResults\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the Regular Season Compact Results file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, V16 (which stands for Vegas 16), or TBC (which stands for The Basketball Classic).\n\n### Data Section 5 files: MTeamSpellings.csv and WTeamSpellings.csv\n\nThese files indicate alternative spellings of many team names. They are intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv or WTeams.csv).\n\n### Data Section 5 files: MNCAATourneySlots and WNCAATourneySlots\n\nThese files identify the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds.\n\n- Season - this is the year of the associated entry in MSeasons.csv or WSeasons.csv (the year in which the final tournament occurs).\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of the men's file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv or WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n### Data Section 5 file: MNCAATourneySeedRoundSlots.csv\n\nThis file helps to represent the men's bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. The women's scheduling has varied a lot more and does not lend itself to this common structure and so there is not a corresponding file for the women's data. Also note that the 2021 men's tournament had unusual scheduling and did not follow the traditional assignment of DayNums for each round.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on. Many tournament rounds span two days' time and so the expected day number for a game to be played on would be one of those two days.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "forecasting",
          "tabular",
          "feature engineering",
          "sports/basketball",
          "brier"
        ]
      }
    },
    {
      "challenge_name": "mens-machine-learning-competition-2018",
      "description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2018-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nGoogle Cloud and NCAA\u00ae have teamed up to bring you this year's version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness\u00ae during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results.\n\nThis page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.\n\n## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\nwhere:\n- $n$ is the number of games played\n- $\\hat{y}_i$ is the predicted probability of team 1 beating team 2\n- $y_i$ is 1 if team 1 wins, 0 if team 2 wins\n- $\\log()$ is the natural (base e) logarithm\n\nA smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored).\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.\n\n### Submission File\n\nThe file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2018 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 = 2278 matchups.\n\nEach game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2013_1104_1129\" indicates team 1104 played team 1129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.\n\nThe resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win:\n```\nid,pred\n2013_1103_1107,0.5\n2013_1103_1112,0.5\n2013_1103_1125,0.5\n...\n```\n\n## FAQs\n\n**Is one tournament enough to decide the best basketball algorithm?**\nIt's better to be lucky than good! Besides, when was the last time your office held a cross validation pool?\n\n**Can I update my predictions after the tournament starts?**\nNo changes are permitted once the tournament begins.\n\n**How is the leaderboard going to work when the event being scored hasn't yet happened?**\nYou won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change.\n\n**Why do we have to predict every match up? Why 68 teams and not 64?**\nThis was done for timing purposes. Predicting every possible matchup for the 68 teams announced on Selection Sunday gives participants the most time to get their current year predictions ready in time. There is a small \"play-in\" round, sometimes called the first round, where the 68 are narrowed to 64. While you are asked to predict these games (and you may be predicting them after they occur), we will not be scoring them.\n\n**Why don't predictions in later rounds count for more?**\nWhile it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets.\n\n## Prizes\n\n- 1st Place - $25,000\n- 2nd Place - $15,000\n- 3rd Place - $10,000\n\nStage 1 will not count towards Kaggle rankings/points.\nStage 2 is weighted to award 50% of the usual competition points. It does not count towards tiers.\n\n## Timeline\n\n**Stage 1 - Model Building**\n- March 10 - Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes.\n\n**Stage 2 - Championship**\n- Sunday, March 11 - Selection Sunday\u00ae (68 teams announced)\n- Monday, March 12 - Kaggle begins to accept 2018 predictions. Release of up-to-date 2017-2018 season data.\n- Thursday, March 15 3PM UTC - Final deadline to submit 2018 predictions.\n- March 15 onward - Watch your tournament results play out!\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Tutorials & Getting Started\n\nCheck out our starter tutorial notebook in Kaggle Kernels.\n\nGoogle Cloud Platform: Work with this competition's data on Google Cloud Platform (GCP). New users can receive $300 in free trial credits.\n\n## Citation\n\nAddison Howard, cpirrotta, Danielle Notaro, Eric Schmidt, inversion, Jeff Sonas, Julia Elliott, Lauren McCann, and Will Cukierski. Google Cloud & NCAA\u00ae ML Competition 2018-Men's. https://kaggle.com/competitions/mens-machine-learning-competition-2018, 2018. Kaggle.\n\n## Competition Details\n\n- **Competition Host**: Google Cloud\n- **Total Prize Pool**: $50,000\n- **Participation**: 6,526 Entrants, 1,061 Participants, 933 Teams, 1,655 Submissions\n- **Tags**: Basketball, Log Loss\n\nData description:\n# Google Cloud & NCAA\u00ae ML Competition 2018-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". And this year the seeds file is NCAATourneySeeds.csv rather than TourneySeeds.csv.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nStage 1- You should submit predicted probabilities for every possible matchup in the past 4 NCAA\u00ae tournaments (2014-2017).\n\nStage 2- You should submit predicted probabilities for every possible matchup before the 2018 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:** the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 10th, 2017 and the men's national championship game will be played on April 2nd, 2018. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2018 season, not the 2017 season or the 2017-18 season or the 2017-2018 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: Teams.csv**  \nThis file identifies the different college teams present in the dataset. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 351 teams currently in Division-I, and an overall total of 364 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). Each team has a 4 digit id number.\n\n- TeamID- a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName- a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season- the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n- LastD1Season- the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2018, and you can confirm there are 351 such teams. It has been a few years since any teams stopped being Division-I; the last was Centenary whose final Division-I year was 2011.\n\n**Data Section 1 file: Seasons.csv**  \nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season- indicates the year in which the tournament was played\n- DayZero- tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that the championship game of the final tournament is on daynum=154. Working backward, the national semifinals are always on daynum=152, the \"play-in\" games are on days 134/135, Selection Sunday is on day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you really want to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z- by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the NCAATourneySeeds file with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: NCAATourneySeeds.csv**  \nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with eight \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 11, 2018.\n\n- Season- the year that the tournament was played in\n- Seed- this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tells you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. For example, the first record in the file is seed W01, which means we are looking at the #1 seed in the W region (which we can see from the \"Seasons.csv\" file was the East region).\n- TeamID- this identifies the id number of the team, as specified in the Teams.csv file\n\n**Data Section 1 file: RegularSeasonCompactResults.csv**  \nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- DayNum- this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"Seasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"Seasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID- this identifies the id number of the team that won the game, as listed in the \"Teams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore- this identifies the number of points scored by the winning team.\n- LTeamID- this identifies the id number of the team that lost the game.\n- LScore- this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- NumOT- this indicates the number of overtime periods in the game, an integer 0 or higher.\n- WLoc- this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\n**Data Section 1 file: NCAATourneyCompactResults.csv**  \nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionStage1.csv**  \nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past four NCAA\u00ae tournaments (seasons 2014, 2015, 2016, and 2017). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2018).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*4=9,112 data rows.\n\n- ID- this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred- this contains the predicted winning percentage for the first team identified in the ID field\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2012_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2012_1181_1314,0.516\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM- field goals made (by the winning team)\n- WFGA- field goals attempted (by the winning team)\n- WFGM3- three pointers made (by the winning team)\n- WFGA3- three pointers attempted (by the winning team)\n- WFTM- free throws made (by the winning team)\n- WFTA- free throws attempted (by the winning team)\n- WOR- offensive rebounds (pulled by the winning team)\n- WDR- defensive rebounds (pulled by the winning team)\n- WAst- assists (by the winning team)\n- WTO- turnovers committed (by the winning team)\n- WStl- steals (accomplished by the winning team)\n- WBlk- blocks (accomplished by the winning team)\n- WPF- personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: RegularSeasonDetailedResults.csv**  \nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file.\n\n**Data Section 2 file: NCAATourneyDetailedResults.csv**  \nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**  \nThis file provides a master list of cities that have been locations for games played.\n- CityID- a four-digit ID number uniquely identifying a city.\n- City- the text name of the city.\n- State- the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: GameCities.csv**  \nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType- this can be either Regular or NCAA\u00ae or Secondary. If it is Regular, you can find more about the game in the RegularSeasonCompactResults.csv and RegularSeasonDetailedResults.csv files. If it is NCAA\u00ae, you can find more about the game in the NCAATourneyCompactResults.csv and NCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the SecondaryTourneyCompactResults file.\n- CityID- the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv**  \nThis zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- RankingDayNum- this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the Seasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName- this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID- this is the ID of the team being ranked, as described in Teams.csv.\n- OrdinalRank- this is the overall ranking of the team in the underlying system. Most systems provide a complete ranking from #1 through #351 (currently), but sometimes there are ties and sometimes only a smaller set of rankings is provided, as with the AP's top 25.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Play-by-play\n\nThis section provides play-by-play event logs for 99% of regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season - including plays by individual players.\n\n**Data Section 5 files: PlayByPlay_201X.zip containing Events_201X.csv and Players_201X.csv**  \nEach zip file (PlayByPlay_2010.zip, PlayByPlay_2011.zip, ..., PlayByPlay_2017.zip) contains two CSV files, listing the play-by-play event logs for almost all games from that season. Each event is assigned to either a team or one of the team's players (by name). The players are listed by PlayerID within the Players csv file for that year, and the play-by-play events are listed (including a PlayerID) within the Events csv file for that year.\n\n**Data Section 5 file: Events_201X.csv**  \n- EventID- this is a unique ID for each logged event. The EventID's are different within each year, as are the PlayerID's. The events are sorted in approximate chronological order within each game, based on clock time, although when multiple events happen within the same clock time, the tiebreak for sorting is just the text EventType, so in some cases it may be impossible to determine the exact sequence of several tip-in attempts or free-throw-attempts that all happened at the same clock time.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. The games are a mix of Regular Season, NCAA\u00ae Tourney, and Secondary Tourney games.\n- WPoints, LPoints- whenever a scoring play happens (1 point, 2 points, or 3 points) the updated score is provided (from the perspective of the winning team (WPoints) and the losing team (LPoints), although of course during the game we didn't know yet that they were the winning team or losing team.\n- ElapsedSeconds- this is the number of seconds that have elapsed from the start of the game until the event occurred. With a 20-minue half, that means that an ElapsedSeconds value from 0 to 1200 represents an event in the first half, a value from 1200 to 2400 represents and event in the second half, and a value above 2400 represents an event in overtime.\n- EventTeamID- this is the ID of the team that the event is logged for, which will either be the WTeamID or the LTeamID.\n- EventPlayerID- this is the ID of the player that the event is logged for, as described in the corresponding Players file.\n- EventType- this is the type of the event that was logged (see listing below).\n\nEvent Types:\n- assist- an assist was credited on a made shot\n- block- a blocked shot was recorded\n- steal- a steal was recorded\n- turnover- a turnover was recorded\n- timeout, timeout_tv- a regular timeout or TV timeout was called\n- foul_pers, foul_tech- a personal foul or technical foul was committed\n- reb_off, reb_def, reb_dead- an offensive rebound, defensive rebound, or dead-ball rebound was recorded\n- sub_in, sub_out- a player entered or exited the game via a substitution\n- made1_free, miss1_free- a one-point free throw was made or missed\n- made2_dunk, miss2_dunk- a two-point field goal (dunk) was made or missed\n- made2_tip, miss2_tip- a two-point field goal (tip-in) was made or missed\n- made2_lay, miss2_lay- a two-point field goal (layup) was made or missed\n- made2_jump, miss2_jump- a two-point field goal (jump shot) was made or missed\n- made3_jump, miss3_jump- a three-point field goal (assumed to be a jump shot) was made or missed\n\n**Data Section 5 file: Players_201X.csv**  \n- PlayerID- this is a unique ID for each distinct player name. The PlayerID's are different within each year, as are the EventID's.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this is the TeamID of the player's team, as described in Teams.csv.\n- PlayerName- this is a text representation of the player's full name, in the format LAST_FIRST, with underscores substituted in for spaces.\n\nNote: there are errors within the events, in that they don't necessarily add up to the final stats for the game. Nevertheless, this was the highest quality data we could achieve for the near-complete set of games.\n\n### Data Section 6 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments\n\n**Data Section 6 file: TeamCoaches.csv**  \nThis file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire year, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many years, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding year.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this is the TeamID of the team that was coached, as described in Teams.csv.\n- FirstDayNum, LastDayNum- this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned.\n- CoachName- this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 6 file: Conferences.csv**  \nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name.\n- ConfAbbrev- this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description- this is a longer text name for the conference.\n\n**Data Section 6 file: TeamConferences.csv**  \nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this identifies the TeamID (as described in Teams.csv).\n- ConfAbbrev- this identifies the conference (as described in Conferences.csv).\n\n**Data Section 6 file: ConferenceTourneyGames.csv**  \nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n- ConfAbbrev- this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the RegularSeasonCompactResults and/or RegularSeasonDetailedResults files.\n\n**Data Section 6 file: SecondaryTourneyTeams.csv**  \nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney- this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n- TeamID- this identifies the TeamID that participated in the tournament (as described in Teams.csv).\n\n**Data Section 6 file: SecondaryTourneyCompactResults.csv**  \nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney.\n- SecondaryTourney- this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 6 file: TeamSpellings.csv**  \nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n- TeamNameSpelling- this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID- this identifies the TeamID for the team that has the alternative spelling (as described in Teams.csv).\n\n**Data Section 6 file: NCAATourneySlots**  \nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- Slot- this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed- this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed- this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the TourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 6 file: NCAATourneySeedRoundSlots.csv**  \nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n- Seed- this is the tournament seed of the team.\n- GameRound- this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot- this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the NCAATourneySlots file.\n- EarlyDayNum, LateDayNum- these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "docker_challenge_path": "/data/mens-machine-learning-competition-2018",
      "competition_description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2018-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nGoogle Cloud and NCAA\u00ae have teamed up to bring you this year's version of the Kaggle machine learning competition. Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness\u00ae during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible match-ups in the 2018 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2018 results.\n\nThis page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\nwhere:\n- $n$ is the number of games played\n- $\\hat{y}_i$ is the predicted probability of team 1 beating team 2\n- $y_i$ is 1 if team 1 wins, 0 if team 2 wins\n- $\\log()$ is the natural (base e) logarithm\n\nA smaller log loss is better. Games which are not played are ignored in the scoring. Play-in games are also ignored (only the games among the final 64 teams are scored).\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add infinite to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.",
      "dataset_description": "Data description:\n# Google Cloud & NCAA\u00ae ML Competition 2018-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\". And this year the seeds file is NCAATourneySeeds.csv rather than TourneySeeds.csv.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nStage 1- You should submit predicted probabilities for every possible matchup in the past 4 NCAA\u00ae tournaments (2014-2017).\n\nStage 2- You should submit predicted probabilities for every possible matchup before the 2018 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:** the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 10th, 2017 and the men's national championship game will be played on April 2nd, 2018. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2018 season, not the 2017 season or the 2017-18 season or the 2017-2018 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: Teams.csv**  \nThis file identifies the different college teams present in the dataset. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 351 teams currently in Division-I, and an overall total of 364 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). Each team has a 4 digit id number.\n\n- TeamID- a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName- a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season- the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n- LastD1Season- the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2018, and you can confirm there are 351 such teams. It has been a few years since any teams stopped being Division-I; the last was Centenary whose final Division-I year was 2011.\n\n**Data Section 1 file: Seasons.csv**  \nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season- indicates the year in which the tournament was played\n- DayZero- tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that the championship game of the final tournament is on daynum=154. Working backward, the national semifinals are always on daynum=152, the \"play-in\" games are on days 134/135, Selection Sunday is on day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you really want to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z- by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the NCAATourneySeeds file with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: NCAATourneySeeds.csv**  \nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with eight \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 11, 2018.\n\n- Season- the year that the tournament was played in\n- Seed- this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tells you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. For example, the first record in the file is seed W01, which means we are looking at the #1 seed in the W region (which we can see from the \"Seasons.csv\" file was the East region).\n- TeamID- this identifies the id number of the team, as specified in the Teams.csv file\n\n**Data Section 1 file: RegularSeasonCompactResults.csv**  \nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- DayNum- this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"Seasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"Seasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID- this identifies the id number of the team that won the game, as listed in the \"Teams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore- this identifies the number of points scored by the winning team.\n- LTeamID- this identifies the id number of the team that lost the game.\n- LScore- this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- NumOT- this indicates the number of overtime periods in the game, an integer 0 or higher.\n- WLoc- this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\n**Data Section 1 file: NCAATourneyCompactResults.csv**  \nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionStage1.csv**  \nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past four NCAA\u00ae tournaments (seasons 2014, 2015, 2016, and 2017). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2018).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*4=9,112 data rows.\n\n- ID- this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred- this contains the predicted winning percentage for the first team identified in the ID field\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2012_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2012_1181_1314,0.516\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM- field goals made (by the winning team)\n- WFGA- field goals attempted (by the winning team)\n- WFGM3- three pointers made (by the winning team)\n- WFGA3- three pointers attempted (by the winning team)\n- WFTM- free throws made (by the winning team)\n- WFTA- free throws attempted (by the winning team)\n- WOR- offensive rebounds (pulled by the winning team)\n- WDR- defensive rebounds (pulled by the winning team)\n- WAst- assists (by the winning team)\n- WTO- turnovers committed (by the winning team)\n- WStl- steals (accomplished by the winning team)\n- WBlk- blocks (accomplished by the winning team)\n- WPF- personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: RegularSeasonDetailedResults.csv**  \nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file.\n\n**Data Section 2 file: NCAATourneyDetailedResults.csv**  \nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**  \nThis file provides a master list of cities that have been locations for games played.\n- CityID- a four-digit ID number uniquely identifying a city.\n- City- the text name of the city.\n- State- the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: GameCities.csv**  \nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType- this can be either Regular or NCAA\u00ae or Secondary. If it is Regular, you can find more about the game in the RegularSeasonCompactResults.csv and RegularSeasonDetailedResults.csv files. If it is NCAA\u00ae, you can find more about the game in the NCAATourneyCompactResults.csv and NCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the SecondaryTourneyCompactResults file.\n- CityID- the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv**  \nThis zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- RankingDayNum- this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the Seasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName- this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID- this is the ID of the team being ranked, as described in Teams.csv.\n- OrdinalRank- this is the overall ranking of the team in the underlying system. Most systems provide a complete ranking from #1 through #351 (currently), but sometimes there are ties and sometimes only a smaller set of rankings is provided, as with the AP's top 25.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Play-by-play\n\nThis section provides play-by-play event logs for 99% of regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season - including plays by individual players.\n\n**Data Section 5 files: PlayByPlay_201X.zip containing Events_201X.csv and Players_201X.csv**  \nEach zip file (PlayByPlay_2010.zip, PlayByPlay_2011.zip, ..., PlayByPlay_2017.zip) contains two CSV files, listing the play-by-play event logs for almost all games from that season. Each event is assigned to either a team or one of the team's players (by name). The players are listed by PlayerID within the Players csv file for that year, and the play-by-play events are listed (including a PlayerID) within the Events csv file for that year.\n\n**Data Section 5 file: Events_201X.csv**  \n- EventID- this is a unique ID for each logged event. The EventID's are different within each year, as are the PlayerID's. The events are sorted in approximate chronological order within each game, based on clock time, although when multiple events happen within the same clock time, the tiebreak for sorting is just the text EventType, so in some cases it may be impossible to determine the exact sequence of several tip-in attempts or free-throw-attempts that all happened at the same clock time.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. The games are a mix of Regular Season, NCAA\u00ae Tourney, and Secondary Tourney games.\n- WPoints, LPoints- whenever a scoring play happens (1 point, 2 points, or 3 points) the updated score is provided (from the perspective of the winning team (WPoints) and the losing team (LPoints), although of course during the game we didn't know yet that they were the winning team or losing team.\n- ElapsedSeconds- this is the number of seconds that have elapsed from the start of the game until the event occurred. With a 20-minue half, that means that an ElapsedSeconds value from 0 to 1200 represents an event in the first half, a value from 1200 to 2400 represents and event in the second half, and a value above 2400 represents an event in overtime.\n- EventTeamID- this is the ID of the team that the event is logged for, which will either be the WTeamID or the LTeamID.\n- EventPlayerID- this is the ID of the player that the event is logged for, as described in the corresponding Players file.\n- EventType- this is the type of the event that was logged (see listing below).\n\nEvent Types:\n- assist- an assist was credited on a made shot\n- block- a blocked shot was recorded\n- steal- a steal was recorded\n- turnover- a turnover was recorded\n- timeout, timeout_tv- a regular timeout or TV timeout was called\n- foul_pers, foul_tech- a personal foul or technical foul was committed\n- reb_off, reb_def, reb_dead- an offensive rebound, defensive rebound, or dead-ball rebound was recorded\n- sub_in, sub_out- a player entered or exited the game via a substitution\n- made1_free, miss1_free- a one-point free throw was made or missed\n- made2_dunk, miss2_dunk- a two-point field goal (dunk) was made or missed\n- made2_tip, miss2_tip- a two-point field goal (tip-in) was made or missed\n- made2_lay, miss2_lay- a two-point field goal (layup) was made or missed\n- made2_jump, miss2_jump- a two-point field goal (jump shot) was made or missed\n- made3_jump, miss3_jump- a three-point field goal (assumed to be a jump shot) was made or missed\n\n**Data Section 5 file: Players_201X.csv**  \n- PlayerID- this is a unique ID for each distinct player name. The PlayerID's are different within each year, as are the EventID's.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this is the TeamID of the player's team, as described in Teams.csv.\n- PlayerName- this is a text representation of the player's full name, in the format LAST_FIRST, with underscores substituted in for spaces.\n\nNote: there are errors within the events, in that they don't necessarily add up to the final stats for the game. Nevertheless, this was the highest quality data we could achieve for the near-complete set of games.\n\n### Data Section 6 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments\n\n**Data Section 6 file: TeamCoaches.csv**  \nThis file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire year, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many years, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding year.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this is the TeamID of the team that was coached, as described in Teams.csv.\n- FirstDayNum, LastDayNum- this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned.\n- CoachName- this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 6 file: Conferences.csv**  \nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name.\n- ConfAbbrev- this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description- this is a longer text name for the conference.\n\n**Data Section 6 file: TeamConferences.csv**  \nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- TeamID- this identifies the TeamID (as described in Teams.csv).\n- ConfAbbrev- this identifies the conference (as described in Conferences.csv).\n\n**Data Section 6 file: ConferenceTourneyGames.csv**  \nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n- ConfAbbrev- this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the RegularSeasonCompactResults and/or RegularSeasonDetailedResults files.\n\n**Data Section 6 file: SecondaryTourneyTeams.csv**  \nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney- this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n- TeamID- this identifies the TeamID that participated in the tournament (as described in Teams.csv).\n\n**Data Section 6 file: SecondaryTourneyCompactResults.csv**  \nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney.\n- SecondaryTourney- this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 6 file: TeamSpellings.csv**  \nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n- TeamNameSpelling- this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID- this identifies the TeamID for the team that has the alternative spelling (as described in Teams.csv).\n\n**Data Section 6 file: NCAATourneySlots**  \nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n- Season- this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n- Slot- this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed- this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed- this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the TourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 6 file: NCAATourneySeedRoundSlots.csv**  \nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n- Seed- this is the tournament seed of the team.\n- GameRound- this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot- this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the NCAATourneySlots file.\n- EarlyDayNum, LateDayNum- these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "classification",
          "tabular",
          "feature engineering",
          "sports",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "mens-machine-learning-competition-2019",
      "description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nAs a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.\n\nAs the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!\n\nThis page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.\n\n## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\n\nwhere\nn is the number of games played\n$\\hat{y}_i$ is the predicted probability of team 1 beating team 2\n$y_i$ is 1 if team 1 wins, 0 if team 2 wins\n$\\log()$ is the natural (base e) logarithm\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.\n\n### Submission File\n\nThe file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams (64 + 4 play-in teams), you will predict (68*67)/2 = 2,278 matchups.\n\nEach game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2014_1107_1110\" indicates team 1107 potentially played team 1110 in the year 2014. You must predict the probability that the team with the lower id beats the team with the higher id.\n\nThe resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win:\nid,pred\n2014_1107_1110,0.5\n2014_1107_1112,0.5\n2014_1107_1113,0.5\n...\n\n## FAQs\n\nIs one tournament enough to decide the best basketball algorithm?\nIt's better to be lucky than good! Besides, when was the last time your office held a cross validation pool?\n\nCan I update my predictions after the tournament starts?\nNo changes are permitted once the tournament begins.\n\nHow is the leaderboard going to work when the event being scored hasn't yet happened?\nYou won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change.\n\nWhy do we have to predict every match up? Why 68 teams and not 64?\nThis was done for timing purposes. Predicting every possible matchup for the 68 teams announced on Selection Sunday gives participants the most time to get their current year predictions ready in time. During the First Four, the 68 are narrowed to 64. While you are asked to predict these games (and you may be predicting them after they occur), we will not be scoring them.\n\nWhy don't predictions in later rounds count for more?\nWhile it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets.\n\n## Prizes\n\n1st Place - $10,000\n2nd Place - $7,000\n3rd Place - $5,000\n4th Place - $2,000\n5th Place - $1,000\n\nKaggle Swag (e.g. t-shirt, coffee mug, stickers), will be awarded to the top three most upvoted Kernels, as of the close of the end of the competition. Only Kernels created/forked after the start of the competition will be eligible for a prize.\n\nNCAA/Google Cloud Swag will be given for the best Competitiveness Score submissions. See https://www.kaggle.com/c/mens-machine-learning-competition-2019/overview/competitiveness-score for more information\n\nStage 1 will not count towards Kaggle rankings/points.\nStage 2 will count toward Kaggle rankings/points.\n\n## Timeline\n\nStage 1 - Model Building\nSaturday, Mar 16\n- Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes.\n\nStage 2 - Championship\nSunday, Mar 17\n- Selection Sunday\u00ae (68 teams announced)\nMonday, Mar 18\n- Kaggle begins to accept 2019 predictions. Release of up-to-date 2018-2019 season data.\nThursday, Mar 21 3PM UTC\n- Final deadline to submit 2019 predictions.\nMar 21 onward\n- Watch your tournament results play out! The Kaggle Team will refresh the leaderboard throughout the competition as games are finalized.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Tutorials & Getting Started\n\nCheck out our starter tutorial notebook in Kaggle Kernels.\n\nGoogle Cloud Platform: Work with this competition's data on Google Cloud Platform (GCP). New users can receive $300 in free trial credits.\n\nInterested in learning more about Cloud Machine Learning Engine or accessing data in Cloud Bucket? Check out our tutorial here.\n\n## Competitiveness Score\n\nThere's a reason why it's called March Madness\u00ae. Upsets happen, underdogs become \"cinderellas\" and games that analysts expected to be blowouts become nail-biters through the final seconds. A team's competitiveness is what keeps games exciting and the tournament truly \"mad.\"\n\nIn addition to the predictive modeling competition, we are hosting a separate competition for kernels that present an exploratory analysis of a competitiveness score. To enter this complementary challenge, all you have to do is publish a public kernel before the final day of the competition, and add \"Competitiveness\" in the title. Entries will be judged by the Google Cloud team in the weeks following the competition and the top three winners will receive a piece of Google Cloud/NCAA\u00ae swag! You may submit as many entries as you wish!\n\nWhat is competitiveness? The ability of team to \"stay in the game\" and possibly have a chance to win late in the contest\u2026\n\nThis is more than likely not a scalar metric, but possibly a clustering of types of competitiveness and then a rating within each. Does this metric have predictive power? The interpretation is up to you.\n\nA few examples:\n- Expected performance relative to match up. Typically this is communicated as point spread. Team A is a 13 point favorite over Team B (the betting line from oddsmakers built to entice 50 percent of wagering on each side -- Team A minus 13 points or Team B plus 13 points)\n- Time/score benchmark toward end of game. This might be an arbitrary mark such as 5 minutes left in regulation relative to initial performance expectation. Ex: Team B (underdog) only trails Team A (favorite) by 3 points with 4:23 remaining (as an original 13 point underdog) = competitive. Team B is trailing Team A by 18 points with 4:23 remaining (as original 13 point underdog) = non-competitive\n- Safety net or threshold for competitiveness not completely dependent on final margin. Ex: Team B trailed Team A by 18 with 4:23 remaining (as original 13 point underdog) but scored the final 10 points of the game -- after the outcome had been \"secured\" -- during \"garbage\" time. They only lost by 8 points. Did they cover the spread? Yes. Were they truly competitive? No.\n- Performance in road and neutral floor games relative to opponent/travel-- i.e. how does neutral floor NCAA Tournament setting affect competitiveness?\n- \"Effort\" category performance-- rebounding, blocks, steals, etc -- relative to seasonal averages weighted against caliber of opponent and opponent's \"effort\" category performance expectation going into the game\n- Leading scorers and/or most significant players performance relative to expectation and/or season averages. Did the best players fail to match, match, or exceed expectations? Or - did a team effectively neutralize the opponent's leading scorers and/or most significant players?\n\nHere's some examples to what factors that would be used in making a better submission. Please make all Kernels public and add \"[Competitiveness]\" to the front of the Kernel's name.\n\nKernels must be made by the end of the tournament, and judging will begin shortly thereafter.\n\nWinners will receive NCAA/Google Cloud swag.\n\nHappy Modeling!\n\n## Citation\n\nAddison Howard, Danielle Notaro, Eric Schmidt, Jeff Sonas, Jen Raulli, Rachel Ahn, Tiffany Martin, and Will Cukierski. Google Cloud & NCAA\u00ae ML Competition 2019-Men's. https://kaggle.com/competitions/mens-machine-learning-competition-2019, 2019. Kaggle.\n\nData description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\n## Note for Stage 2:\nAll of the data files have been updated through the end of the current regular season. You can find everything you need in these four files:\n1) Stage2DataFiles.zip (this contains the same type of information as the DataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n2) MasseyOrdinals_thru_2019_day_128.zip (this contains the same type of information as MasseyOrdinals.zip did for Stage 1, but it now includes 2019 data as well). For the absolute latest version of the Massey Ordinals, see the Discussion thread \"Massey Ordinals Day 133 Thread\".\n3) PlayByPlay_2019.zip (this contains the same type of information as PlayByPlay_2018.zip, etc., but it is play-by-play for the current 2019 season games)\n4) SampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)\n\nYou can just disregard the Stage 1 files and the Prelim files at this point - they are completely superseded by the above release. The only exceptions are that the play-by-play data for earlier years (PlayByPlay_2010, PlayByPlay_2011, \u2026, PlayByPlay_2018) is still useful, and there will be ongoing releases of the latest Massey Ordinals as they become available.\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\".\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018).\n\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nSpecial note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 6th, 2018 and the men's national championship game will be played on April 8th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: Teams.csv**\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=1465) and North Alabama (TeamID=1466), and so you will not see any historical data for these teams prior to the current season.\n\nTeamID - a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n\nTeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n\nFirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n\nLastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2019, and you can confirm there are 353 such teams. It has been a few years since any teams stopped being Division-I; the last was Centenary whose final Division-I year was 2011.\n\n**Data Section 1 file: Seasons.csv**\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\nSeason - indicates the year in which the tournament was played. Remember that the current season counts as 2019.\n\nDayZero - tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the championship game of the men's tournament is on daynum=154. Working backward, the national semifinals are always on daynum=152, the \"play-in\" games are on days 134/135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n\nRegionW, RegionX, Region Y, Region Z - by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the NCAATourneySeeds file with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: NCAATourneySeeds.csv**\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 17, 2019.\n\nSeason - the year that the tournament was played in\n\nSeed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"Seasons.csv\" file was the East region).\n\nTeamID - this identifies the id number of the team, as specified in the Teams.csv file\n\n**Data Section 1 file: RegularSeasonCompactResults.csv**\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n\nDayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"Seasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"Seasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n\nWTeamID - this identifies the id number of the team that won the game, as listed in the \"Teams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n\nWScore - this identifies the number of points scored by the winning team.\n\nLTeamID - this identifies the id number of the team that lost the game.\n\nLScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n\nWLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\nNumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: NCAATourneyCompactResults.csv**\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\nDayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\nDayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\nDayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\nDayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\nDayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\nDayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\nDayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionStage1.csv**\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows.\n\nID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n\nPred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2012_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2012_1181_1314,0.516\n\n### Data Section 2 - Team Box Scores\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n\nWFGM - field goals made (by the winning team)\nWFGA - field goals attempted (by the winning team)\nWFGM3 - three pointers made (by the winning team)\nWFGA3 - three pointers attempted (by the winning team)\nWFTM - free throws made (by the winning team)\nWFTA - free throws attempted (by the winning team)\nWOR - offensive rebounds (pulled by the winning team)\nWDR - defensive rebounds (pulled by the winning team)\nWAst - assists (by the winning team)\nWTO - turnovers committed (by the winning team)\nWStl - steals (accomplished by the winning team)\nWBlk - blocks (accomplished by the winning team)\nWPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: RegularSeasonDetailedResults.csv**\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file.\n\n**Data Section 2 file: NCAATourneyDetailedResults.csv**\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\nThis file provides a master list of cities that have been locations for games played.\n\nCityID - a four-digit ID number uniquely identifying a city.\nCity - the text name of the city.\nState - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: GameCities.csv**\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n\nCRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the RegularSeasonCompactResults.csv and RegularSeasonDetailedResults.csv files. If it is NCAA, you can find more about the game in the NCAATourneyCompactResults.csv and NCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the SecondaryTourneyCompactResults file.\n\nCityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv**\nThis zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nRankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the Seasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n\nSystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n\nTeamID - this is the ID of the team being ranked, as described in Teams.csv.\n\nOrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but sometimes there are ties and sometimes only a smaller set of rankings is provided, as with the AP's top 25. This year they will typically go up to #353 because there are two new teams in Division I.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Play-by-play\nThis section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA\u00ae tournament, and secondary tournament games since the 2014-15 season - including plays by individual players.\n\nThis year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible.\n\n**Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv**\nEach MEvents file lists the play-by-play event logs for more than 99.5% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the MPlayers.csv file.\n\nEventID - this is a unique ID for each logged event. The EventID's are different within each year and uniquely identify each play-by-play event. They ought to be listed in chronological order for the events within their game.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. The games are a mix of Regular Season, NCAA\u00ae Tourney, and Secondary Tourney games.\n\nWFinalScore, LFinalScore - these two columns match the WScore and LScore numbers as found elsewhere in the Compact Results and Detailed Results files. They are provided here to indicate the final score at the end of the game. Note that the event-by-event totals are not guaranteed to add up to the final scores, due to possible data recording errors in the play-by-play.\n\nWCurrentScore, LCurrentScore - whenever a scoring play happens (1 point, 2 points, or 3 points) the updated score is provided, from the perspective of the winning team (WPoints) and the losing team (LPoints), although of course during the game we didn't know yet that they were the winning team or losing team. Note that in the earlier years of the play-by-play data from this source, the running WCurrentScore and LCurrentScore were not calculated, and so they show up as zero throughout the event log for those years. However, they can still be calculated manually when looping through the game events that are present, by watching for rows like \"made1\", \"made2\", and \"made3\" that represent scoring events.\n\nElapsedSeconds - this is the number of seconds that have elapsed from the start of the game until the event occurred. With a 20-minute half, that means that an ElapsedSeconds value from 0 to 1200 represents an event in the first half, a value from 1200 to 2400 represents an event in the second half, and a value above 2400 represents an event in overtime. For example, since overtime periods are five minutes long (that's 300 seconds), a value of 2699 would represent one second left in the first overtime.\n\nEventTeamID - this is the ID of the team that the event is logged for, which will either be the WTeamID or the LTeamID.\n\nEventPlayerID - this is the ID of the player that the event is logged for, as described in the MPlayers.csv file.\n\nEventType, EventSubType - these indicate the type of the event that was logged (see listing below).\n\nEvent Types and Subtypes:\nassist - an assist was credited on a made shot\nblock - a blocked shot was recorded\nsteal - a steal was recorded\nsub - a substitution occurred, with one of the following subtypes:\n      in=player entered the game; \n      out=player exited the game; \n      start=player started the game\ntimeout - a timeout was called, with one of the following subtypes:\n      unk=unknown type of timeout; \n      comm=commercial timeout; \n      full=full timeout; \n      short= short timeout\nturnover - a turnover was recorded, with one of the following subtypes:\n      unk=unknown type of turnover; \n      10sec=10 second violation; \n      3sec=3 second violation; \n      5sec=5 second violation; \n      bpass=bad pass turnover; \n      dribb=dribbling turnover; \n      lanev=lane violation; \n      lostb=lost ball; \n      offen=offensive turnover (?); \n      offgt=offensive goaltending; \n      other=other type of turnover; \n      shotc=shot clock violation; \n      trav=travelling\nfoul - a foul was committed, with one of the following subtypes:\n      unk=unknown type of foul; \n      admT=administrative technical; \n      benT=bench technical; \n      coaT=coach technical; \n      off=offensive foul; \n      pers=personal foul; \n      tech=technical foul\nfouled - a player was fouled\nreb - a rebound was recorded, with one of the following subtypes:\n      deadb=a deadball rebound; \n      def=a defensive rebound; \n      defdb=a defensive deadball rebound; \n      off=an offensive rebound; \n      offdb=an offensive deadball rebound\nmade1, miss1 - a one-point free throw was made or missed, with one of the following subtypes:\n      1of1=the only free throw of the trip to the line; \n      1of2=the first of two free throw attempts; \n      2of2=the second of two free throw attempts; \n      1of3=the first of three free throw attempts; \n      2of3=the second of three free throw attempts; \n      3of3=the third of three free throw attempts; \n      unk=unknown what the free throw sequence is\nmade2, miss2 - a two-point field goal was made or missed, with one of the following subtypes:\n      unk=unknown type of two-point shot; \n      dunk=dunk; \n      lay=layup; \n      tip=tip-in; \n      jump=jump shot; \n      alley=alley-oop; \n      drive=driving layup; \n      hook=hook shot; \n      stepb=step-back jump shot; \n      pullu=pull-up jump shot; \n      turna=turn-around jump shot; \n      wrong=wrong basket\nmade3, miss3 - a three-point field goal was made or missed, with one of the following subtypes:\n      unk=unknown type of three-point shot; \n      jump=jump shot; \n      stepb=step-back jump shot; \n      pullu=pull-up jump shot; \n      turna=turn-around jump shot; \n      wrong=wrong basket\njumpb - a jumpball was called or resolved, with one of the following subtypes:\n      start=start period; \n      block=block tie-up; \n      heldb=held ball; \n      lodge=lodged ball; \n      lost=jump ball lost; \n      outof=out of bounds; \n      outrb=out of bounds rebound; \n      won=jump ball won\nX, Y - for games where it is available, this describes an X/Y position on the court where the lower-left corner of the court is (0,0), the upper-right corner of the court is (100,100), and so on. The X/Y position is provided for fouls, turnovers, and field-goal attempts (either 2-point or 3-point).\n\nArea - for events where an X/Y position is provided, this position is more generally categorized into one of 13 \"areas\" of the court, as follows: \n1=under basket; \n2=in the paint; \n3=inside right wing; \n4=inside right; \n5=inside center; \n6=inside left; \n7=inside left wing; \n8=outside right wing; \n9=outside right; \n10=outside center; \n11=outside left; \n12=outside left wing; \n13=backcourt\n\n**Data Section 5 file: MPlayers.csv**\nPlayerID - this is a unique ID for each distinct player name/team combination. If the same player name spelling appears on a team for multiple seasons, it should stay as one player record rather than a separate one for each year.\n\nLastName, FirstName - these are the player's last and first names.\n\nTeamID - this is the TeamID of the player's team, as described in Teams.csv.\n\nNote: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games.\n\n### Data Section 6 - Supplements\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments\n\n**Data Section 6 file: TeamCoaches.csv**\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nTeamID - this is the TeamID of the team that was coached, as described in Teams.csv.\n\nFirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n\nCoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 6 file: Conferences.csv**\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams.\n\nConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n\nDescription - this is a longer text name for the conference.\n\n**Data Section 6 file: TeamConferences.csv**\nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nTeamID - this identifies the TeamID (as described in Teams.csv).\n\nConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 6 file: ConferenceTourneyGames.csv**\nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\nConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the RegularSeasonCompactResults and/or RegularSeasonDetailedResults files.\n\n**Data Section 6 file: SecondaryTourneyTeams.csv**\nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the Secondary Tourney Compact Results file, but is presented in this file as well, for your convenience.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the post-season tournament was played)\n\nSecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\nTeamID - this identifies the TeamID that participated in the tournament (as described in Teams.csv).\n\n**Data Section 6 file: SecondaryTourneyCompactResults.csv**\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney.\n\nSecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 6 file: TeamSpellings.csv**\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\nTeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n\nTeamID - this identifies the TeamID for the team that has the alternative spelling (as described in Teams.csv).\n\n**Data Section 6 file: NCAATourneySlots**\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nSlot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n\nStrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n\nWeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 6 file: NCAATourneySeedRoundSlots.csv**\nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n\nSeed - this is the tournament seed of the team.\n\nGameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n\nGameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the NCAATourneySlots file.\n\nEarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "docker_challenge_path": "/data/mens-machine-learning-competition-2019",
      "competition_description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nAs a result of the continued collaboration between Google Cloud and the NCAA, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.\n\nAs the official public cloud provider of the NCAA, Google is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes, and more than 19,000 teams. Game on!\n\nThis page is for the NCAA Division I Men's tournament. Check out the NCAA Division I Women's tournament here.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\n\nwhere\nn is the number of games played\n$\\hat{y}_i$ is the predicted probability of team 1 beating team 2\n$y_i$ is 1 if team 1 wins, 0 if team 2 wins\n$\\log()$ is the natural (base e) logarithm\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.",
      "dataset_description": "# Google Cloud & NCAA\u00ae ML Competition 2019-Men's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\n## Note for Stage 2:\nAll of the data files have been updated through the end of the current regular season. You can find everything you need in these four files:\n1) Stage2DataFiles.zip (this contains the same type of information as the DataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n2) MasseyOrdinals_thru_2019_day_128.zip (this contains the same type of information as MasseyOrdinals.zip did for Stage 1, but it now includes 2019 data as well). For the absolute latest version of the Massey Ordinals, see the Discussion thread \"Massey Ordinals Day 133 Thread\".\n3) PlayByPlay_2019.zip (this contains the same type of information as PlayByPlay_2018.zip, etc., but it is play-by-play for the current 2019 season games)\n4) SampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)\n\nYou can just disregard the Stage 1 files and the Prelim files at this point - they are completely superseded by the above release. The only exceptions are that the play-by-play data for earlier years (PlayByPlay_2010, PlayByPlay_2011, \u2026, PlayByPlay_2018) is still useful, and there will be ongoing releases of the latest Massey Ordinals as they become available.\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The TeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, so if you are re-using code from previous instances of this contest, you may need to adjust for this. For example, we are universally referencing Team ID columns with a spelling of \"TeamID\" rather than \"team_id\".\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018).\n\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\nSpecial note about \"Season\" numbers: the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 6th, 2018 and the men's national championship game will be played on April 8th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: Teams.csv**\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=1465) and North Alabama (TeamID=1466), and so you will not see any historical data for these teams prior to the current season.\n\nTeamID - a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n\nTeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n\nFirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n\nLastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2019, and you can confirm there are 353 such teams. It has been a few years since any teams stopped being Division-I; the last was Centenary whose final Division-I year was 2011.\n\n**Data Section 1 file: Seasons.csv**\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\nSeason - indicates the year in which the tournament was played. Remember that the current season counts as 2019.\n\nDayZero - tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the championship game of the men's tournament is on daynum=154. Working backward, the national semifinals are always on daynum=152, the \"play-in\" games are on days 134/135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n\nRegionW, RegionX, Region Y, Region Z - by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the NCAATourneySeeds file with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: NCAATourneySeeds.csv**\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week. We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 17, 2019.\n\nSeason - the year that the tournament was played in\n\nSeed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"Seasons.csv\" file was the East region).\n\nTeamID - this identifies the id number of the team, as specified in the Teams.csv file\n\n**Data Section 1 file: RegularSeasonCompactResults.csv**\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n\nDayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"Seasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"Seasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n\nWTeamID - this identifies the id number of the team that won the game, as listed in the \"Teams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n\nWScore - this identifies the number of points scored by the winning team.\n\nLTeamID - this identifies the id number of the team that lost the game.\n\nLScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n\nWLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\nNumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: NCAATourneyCompactResults.csv**\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the RegularSeasonCompactResults data. Note that these games also include the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\nDayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\nDayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\nDayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\nDayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\nDayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\nDayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\nDayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as fairly similar to NCAA\u00ae tournament games.\n\n**Data Section 1 file: SampleSubmissionStage1.csv**\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows.\n\nID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n\nPred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2012 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2012_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2012 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2012_1181_1314,0.516\n\n### Data Section 2 - Team Box Scores\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n\nWFGM - field goals made (by the winning team)\nWFGA - field goals attempted (by the winning team)\nWFGM3 - three pointers made (by the winning team)\nWFGA3 - three pointers attempted (by the winning team)\nWFTM - free throws made (by the winning team)\nWFTA - free throws attempted (by the winning team)\nWOR - offensive rebounds (pulled by the winning team)\nWDR - defensive rebounds (pulled by the winning team)\nWAst - assists (by the winning team)\nWTO - turnovers committed (by the winning team)\nWStl - steals (accomplished by the winning team)\nWBlk - blocks (accomplished by the winning team)\nWPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: RegularSeasonDetailedResults.csv**\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the RegularSeasonCompactResults file since the 2003 season should exactly be present in the RegularSeasonDetailedResults file.\n\n**Data Section 2 file: NCAATourneyDetailedResults.csv**\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the NCAATourneyCompactResults file since the 2003 season should exactly be present in the NCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\nThis file provides a master list of cities that have been locations for games played.\n\nCityID - a four-digit ID number uniquely identifying a city.\nCity - the text name of the city.\nState - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: GameCities.csv**\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n\nCRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the RegularSeasonCompactResults.csv and RegularSeasonDetailedResults.csv files. If it is NCAA, you can find more about the game in the NCAATourneyCompactResults.csv and NCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the SecondaryTourneyCompactResults file.\n\nCityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MasseyOrdinals.zip containing MasseyOrdinals.csv**\nThis zip file contains a large CSV file, listing out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his College Basketball Ranking Composite page.\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nRankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the Seasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n\nSystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n\nTeamID - this is the ID of the team being ranked, as described in Teams.csv.\n\nOrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but sometimes there are ties and sometimes only a smaller set of rankings is provided, as with the AP's top 25. This year they will typically go up to #353 because there are two new teams in Division I.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Play-by-play\nThis section provides play-by-play event logs for more than 99.5% of each year's regular season, NCAA\u00ae tournament, and secondary tournament games since the 2014-15 season - including plays by individual players.\n\nThis year we are transitioning to a different play-by-play source, which includes data since the 2014-2015 season rather than since the 2009-2010 season (that's what we had previously for men's data). However, we are now able to provide play-by-play for both men's and women's data, and there is locational play-by-play detail starting with games from the 2018-2019 season. This includes an X/Y location (ranging from 0 to 100 in each dimension) on the court for each shot attempt, turnover, and foul for many games, as well as an overall categorization of the area on the court that the shot or turnover or foul occurred in (inside left wing, outside right wing, under the basket, etc.) Some games in these recent seasons still lack the locational detail. The data from last year (2019 season) matches what you can expect for the current year (2020 season) as we approach the postseason. Despite the 99.5% coverage, there are still a few games missing annually, and we will try to bring those in as well, if possible.\n\n**Data Section 5 file: MEvents2015.csv, MEvents2016.csv, MEvent2017.csv, MEvents2018.csv, MEvents2019.csv**\nEach MEvents file lists the play-by-play event logs for more than 99.5% of games from that season. Each event is assigned to either a team or a single one of the team's players. Thus if a basket is made by one player and an assist is credited to a second player, that would show up as two separate records. The players are listed by PlayerID within the MPlayers.csv file.\n\nEventID - this is a unique ID for each logged event. The EventID's are different within each year and uniquely identify each play-by-play event. They ought to be listed in chronological order for the events within their game.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. The games are a mix of Regular Season, NCAA\u00ae Tourney, and Secondary Tourney games.\n\nWFinalScore, LFinalScore - these two columns match the WScore and LScore numbers as found elsewhere in the Compact Results and Detailed Results files. They are provided here to indicate the final score at the end of the game. Note that the event-by-event totals are not guaranteed to add up to the final scores, due to possible data recording errors in the play-by-play.\n\nWCurrentScore, LCurrentScore - whenever a scoring play happens (1 point, 2 points, or 3 points) the updated score is provided, from the perspective of the winning team (WPoints) and the losing team (LPoints), although of course during the game we didn't know yet that they were the winning team or losing team. Note that in the earlier years of the play-by-play data from this source, the running WCurrentScore and LCurrentScore were not calculated, and so they show up as zero throughout the event log for those years. However, they can still be calculated manually when looping through the game events that are present, by watching for rows like \"made1\", \"made2\", and \"made3\" that represent scoring events.\n\nElapsedSeconds - this is the number of seconds that have elapsed from the start of the game until the event occurred. With a 20-minute half, that means that an ElapsedSeconds value from 0 to 1200 represents an event in the first half, a value from 1200 to 2400 represents an event in the second half, and a value above 2400 represents an event in overtime. For example, since overtime periods are five minutes long (that's 300 seconds), a value of 2699 would represent one second left in the first overtime.\n\nEventTeamID - this is the ID of the team that the event is logged for, which will either be the WTeamID or the LTeamID.\n\nEventPlayerID - this is the ID of the player that the event is logged for, as described in the MPlayers.csv file.\n\nEventType, EventSubType - these indicate the type of the event that was logged (see listing below).\n\nEvent Types and Subtypes:\nassist - an assist was credited on a made shot\nblock - a blocked shot was recorded\nsteal - a steal was recorded\nsub - a substitution occurred, with one of the following subtypes:\n      in=player entered the game; \n      out=player exited the game; \n      start=player started the game\ntimeout - a timeout was called, with one of the following subtypes:\n      unk=unknown type of timeout; \n      comm=commercial timeout; \n      full=full timeout; \n      short= short timeout\nturnover - a turnover was recorded, with one of the following subtypes:\n      unk=unknown type of turnover; \n      10sec=10 second violation; \n      3sec=3 second violation; \n      5sec=5 second violation; \n      bpass=bad pass turnover; \n      dribb=dribbling turnover; \n      lanev=lane violation; \n      lostb=lost ball; \n      offen=offensive turnover (?); \n      offgt=offensive goaltending; \n      other=other type of turnover; \n      shotc=shot clock violation; \n      trav=travelling\nfoul - a foul was committed, with one of the following subtypes:\n      unk=unknown type of foul; \n      admT=administrative technical; \n      benT=bench technical; \n      coaT=coach technical; \n      off=offensive foul; \n      pers=personal foul; \n      tech=technical foul\nfouled - a player was fouled\nreb - a rebound was recorded, with one of the following subtypes:\n      deadb=a deadball rebound; \n      def=a defensive rebound; \n      defdb=a defensive deadball rebound; \n      off=an offensive rebound; \n      offdb=an offensive deadball rebound\nmade1, miss1 - a one-point free throw was made or missed, with one of the following subtypes:\n      1of1=the only free throw of the trip to the line; \n      1of2=the first of two free throw attempts; \n      2of2=the second of two free throw attempts; \n      1of3=the first of three free throw attempts; \n      2of3=the second of three free throw attempts; \n      3of3=the third of three free throw attempts; \n      unk=unknown what the free throw sequence is\nmade2, miss2 - a two-point field goal was made or missed, with one of the following subtypes:\n      unk=unknown type of two-point shot; \n      dunk=dunk; \n      lay=layup; \n      tip=tip-in; \n      jump=jump shot; \n      alley=alley-oop; \n      drive=driving layup; \n      hook=hook shot; \n      stepb=step-back jump shot; \n      pullu=pull-up jump shot; \n      turna=turn-around jump shot; \n      wrong=wrong basket\nmade3, miss3 - a three-point field goal was made or missed, with one of the following subtypes:\n      unk=unknown type of three-point shot; \n      jump=jump shot; \n      stepb=step-back jump shot; \n      pullu=pull-up jump shot; \n      turna=turn-around jump shot; \n      wrong=wrong basket\njumpb - a jumpball was called or resolved, with one of the following subtypes:\n      start=start period; \n      block=block tie-up; \n      heldb=held ball; \n      lodge=lodged ball; \n      lost=jump ball lost; \n      outof=out of bounds; \n      outrb=out of bounds rebound; \n      won=jump ball won\nX, Y - for games where it is available, this describes an X/Y position on the court where the lower-left corner of the court is (0,0), the upper-right corner of the court is (100,100), and so on. The X/Y position is provided for fouls, turnovers, and field-goal attempts (either 2-point or 3-point).\n\nArea - for events where an X/Y position is provided, this position is more generally categorized into one of 13 \"areas\" of the court, as follows: \n1=under basket; \n2=in the paint; \n3=inside right wing; \n4=inside right; \n5=inside center; \n6=inside left; \n7=inside left wing; \n8=outside right wing; \n9=outside right; \n10=outside center; \n11=outside left; \n12=outside left wing; \n13=backcourt\n\n**Data Section 5 file: MPlayers.csv**\nPlayerID - this is a unique ID for each distinct player name/team combination. If the same player name spelling appears on a team for multiple seasons, it should stay as one player record rather than a separate one for each year.\n\nLastName, FirstName - these are the player's last and first names.\n\nTeamID - this is the TeamID of the player's team, as described in Teams.csv.\n\nNote: there are data collection errors within the events, in that they don't necessarily add up to the final stats for the game. In addition, the player name spellings may vary over the course of a season or career for the same player, which would lead to the same player showing up with different PlayerID values. Nevertheless, this was the highest quality data we could manage for play-by-play with the near-complete set of games.\n\n### Data Section 6 - Supplements\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, game results for NIT and other postseason tournaments\n\n**Data Section 6 file: TeamCoaches.csv**\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNums to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nTeamID - this is the TeamID of the team that was coached, as described in Teams.csv.\n\nFirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n\nCoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 6 file: Conferences.csv**\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams.\n\nConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n\nDescription - this is a longer text name for the conference.\n\n**Data Section 6 file: TeamConferences.csv**\nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nTeamID - this identifies the TeamID (as described in Teams.csv).\n\nConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 6 file: ConferenceTourneyGames.csv**\nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the RegularSeasonCompactResults and RegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\nConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n\nSeason, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the RegularSeasonCompactResults and/or RegularSeasonDetailedResults files.\n\n**Data Section 6 file: SecondaryTourneyTeams.csv**\nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the Secondary Tourney Compact Results file, but is presented in this file as well, for your convenience.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the post-season tournament was played)\n\nSecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\nTeamID - this identifies the TeamID that participated in the tournament (as described in Teams.csv).\n\n**Data Section 6 file: SecondaryTourneyCompactResults.csv**\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney.\n\nSecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 6 file: TeamSpellings.csv**\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\nTeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n\nTeamID - this identifies the TeamID for the team that has the alternative spelling (as described in Teams.csv).\n\n**Data Section 6 file: NCAATourneySlots**\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\nSeason - this is the year of the associated entry in Seasons.csv (the year in which the final tournament occurs)\n\nSlot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n\nStrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n\nWeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the NCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 6 file: NCAATourneySeedRoundSlots.csv**\nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n\nSeed - this is the tournament seed of the team.\n\nGameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n\nGameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the NCAATourneySlots file.\n\nEarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "forecasting",
          "tabular",
          "feature engineering",
          "sports",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "mens-march-mania-2022",
      "description": "Challenge description:\nMarch Machine Learning Mania 2022 - Men\u2019s  \nPredict the 2022 College Men's Basketball Tournament  \n\nOverview  \nStart: Feb 18, 2022  \nClose: Apr 4, 2022  \n\nDescription  \nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our *eighth* annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.  \n\nYou're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge. In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.  \n\nAnd don't forget to take a look at our [companion competition](https://www.kaggle.com/c/34542) that looks to predict the outcome of the US women's college basketball tournament!  \n\n### Acknowledgments  \nBanner image by Ben Hershey on Unsplash  \n\nEvaluation  \nSubmissions are scored on the log loss:  \n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right],$$  \nwhere  \n\\( n \\) is the number of games played  \n\\( \\hat{y}_i \\) is the predicted probability of team 1 beating team 2  \n\\( y_i  \\) is 1 if team 1 wins, 0 if team 2 wins  \n\\( \\log \\) is the natural logarithm  \n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.  \n\nSubmission File  \nThe file you submit will depend on whether the competition is in stage 1--historical model building--or stage 2--the 2022 tournament. Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 68 teams, you will predict \\( (68*67)/2  = 2,278\\) matchups.  \n\nEach game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2016_1107_1110\" indicates team 1107 potentially played team 1110 in the year 2016. You must predict the probability that the team with the lower id beats the team with the higher id.  \n\nThe resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win:  \nID,Pred  \n2016_1107_1110,0.5  \n2016_1107_1112,0.5  \n2016_1107_1113,0.5  \n\nTimeline  \nStage 1 - Model Building  \nSaturday, Mar 12  \n- Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes.  \n\nStage 2 - Championship  \nSunday, Mar 13  \n- Selection Sunday\u00ae (68 teams announced)  \nMonday, Mar 14  \n- Kaggle begins to accept 2022 predictions. Release of up-to-date 2021-2022 season data.  \nThursday, Mar 17 4PM UTC  \n- Final deadline to submit 2022 predictions.  \nMar 17 onward  \n- Watch your tournament results play out! The Kaggle Team will refresh the leaderboard throughout the competition as games are finalized.  \n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary.  \n\nPrizes  \n1st Place - $5,000  \n2nd Place - $5,000  \n3rd Place - $5,000  \n4th Place - $5,000  \n5th Place - $5,000  \n\nCitation  \nAddison Howard, Jeff Sonas, and Will Cukierski. March Machine Learning Mania 2022 - Men\u2019s. https://kaggle.com/competitions/mens-march-mania-2022, 2022. Kaggle.  \n\nCompetition Host  \nKaggle  \n\nPrizes & Awards  \n$25,000  \n\nAwards Points & Medals  \n\nParticipation  \n4,157 Entrants  \n1,025 Participants  \n930 Teams  \n1,681 Submissions  \n\nTags  \nBasketball  \nSports  \nLog Loss\n\nData description:\n# March Machine Learning Mania 2022 - Men's: Predict the 2022 College Men's Basketball Tournament\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the [wikipedia page](https://en.wikipedia.org/wiki/NCAA_Men's_Division_I_Basketball_Tournament) before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2016-2019 and 2021). Note that there was no tournament held in 2020.\n\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2022 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the contest data files. All of the files are complete through February 7th of the current season. At the start of Stage 2, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:** the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 9th, 2021 and the men's national championship game will be played on April 4th, 2022. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2022 season, not the 2021 season or the 2021-22 season or the 2021-2022 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: MTeams.csv**\n\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 358 teams currently in Division-I, and an overall total of 372 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs).\n\n- TeamID - a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2022, and you can confirm there are 358 such teams.\n\n**Data Section 1 file: MSeasons.csv**\n\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season - indicates the year in which the tournament was played. Remember that the current season counts as 2022.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the national semifinals are always on DayNum=152, the \"play-in\" games are on days 135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z - by our contests' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files, even if the region names change from year to year. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the MNCAATourneySeeds file that year with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: MNCAATourneySeeds.csv**\n\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 13, 2022 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"MSeasons.csv\" file was the East region).\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv file\n\n**Data Section 1 file: MRegularSeasonCompactResults.csv**\n\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"MSeasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: MNCAATourneyCompactResults.csv**\n\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.\n\n**Data Section 1 file: MSampleSubmissionStage1.csv**\n\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2016, 2017, 2018, 2019, and 2021). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2022).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2017_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2018_1181_1314,0.516\n\nAlso note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke.\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: MRegularSeasonDetailedResults.csv**\n\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file.\n\n**Data Section 2 file: MNCAATourneyDetailedResults.csv**\n\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: MGameCities.csv**\n\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the MRegularSeasonCompactResults.csv and MRegularSeasonDetailedResults.csv files. If it is NCAA, you can find more about the game in the MNCAATourneyCompactResults.csv and MNCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the MSecondaryTourneyCompactResults file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MMasseyOrdinals.csv**\n\nThis file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his [College Basketball Ranking Composite page](http://www.masseyratings.com/cb/compare.htm).\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n**Data Section 5 file: MTeamCoaches.csv**\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the calendar year in which the final tournament occurs)\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 5 file: Conferences.csv**\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n**Data Section 5 file: MTeamConferences.csv**\n\nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 5 file: MConferenceTourneyGames.csv**\n\nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the MRegularSeasonCompactResults and/or MRegularSeasonDetailedResults files.\n\n**Data Section 5 file: MSecondaryTourneyTeams.csv**\n\nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv).\n\n**Data Section 5 file: MSecondaryTourneyCompactResults.csv**\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 5 file: MTeamSpellings.csv**\n\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv).\n\n**Data Section 5 file: MNCAATourneySlots**\n\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 5 file: MNCAATourneySeedRoundSlots.csv**\n\nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the MNCAATourneySlots file.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "docker_challenge_path": "/data/mens-march-mania-2022",
      "competition_description": "Description  \nAnother year, another chance to predict the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. In our *eighth* annual March Machine Learning Mania competition, Kagglers will once again join the millions of fans who attempt to predict the outcomes of this year's US men's college basketball tournament. But unlike most fans, you will pick the winners and losers using a combination of rich historical data and computing power, while the ground truth unfolds on television.  \n\nYou're provided data of historical NCAA games and are encouraged to use other sources of publicly available data to gain a winning edge. In stage one of this two-stage competition, participants will build and test their models against previous tournaments. In the second stage, participants will predict the outcome of the 2022 tournament. You don\u2019t need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2022 results.  \n\nAnd don't forget to take a look at our [companion competition](https://www.kaggle.com/c/34542) that looks to predict the outcome of the US women's college basketball tournament!",
      "evaluation_metric": "Evaluation  \nSubmissions are scored on the log loss:  \n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right],$$  \nwhere  \n\\( n \\) is the number of games played  \n\\( \\hat{y}_i \\) is the predicted probability of team 1 beating team 2  \n\\( y_i  \\) is 1 if team 1 wins, 0 if team 2 wins  \n\\( \\log \\) is the natural logarithm  \n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.",
      "dataset_description": "Data description:\n# March Machine Learning Mania 2022 - Men's: Predict the 2022 College Men's Basketball Tournament\n\nEach season there are thousands of NCAA basketball games played between Division I men's teams, culminating in March Madness\u00ae, the 68-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the NCAA\u00ae tournament, we encourage reading the [wikipedia page](https://en.wikipedia.org/wiki/NCAA_Men's_Division_I_Basketball_Tournament) before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The MTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\n\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2016-2019 and 2021). Note that there was no tournament held in 2020.\n\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2022 tournament begins.\n\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\n\nBelow we describe the format and fields of the contest data files. All of the files are complete through February 7th of the current season. At the start of Stage 2, we will provide updates to these files to incorporate data from the remaining weeks of the current season.\n\n### Data Section 1 - The Basics\n\nThis section provides everything you need to build a simple prediction model and submit predictions.\n\n- Team ID's and Team Names\n- Tournament seeds since 1984-85 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1984-85 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:** the college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first men's Division I games were played on November 9th, 2021 and the men's national championship game will be played on April 4th, 2022. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2022 season, not the 2021 season or the 2021-22 season or the 2021-2022 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: MTeams.csv**\n\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 358 teams currently in Division-I, and an overall total of 372 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs).\n\n- TeamID - a 4 digit id number, from 1000-1999, uniquely identifying each NCAA\u00ae men's team. A school's TeamID does not change from one year to the next, so for instance the Duke men's TeamID is 1181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName - a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n- FirstD1Season - the first season in our dataset that the school was a Division-I school. For instance, FL Gulf Coast (famously) was not a Division-I school until the 2008 season, despite their two wins just five years later in the 2013 NCAA\u00ae tourney. Of course, many schools were Division-I far earlier than 1985, but since we don't have any data included prior to 1985, all such teams are listed with a FirstD1Season of 1985.\n- LastD1Season - the last season in our dataset that the school was a Division-I school. For any teams that are currently Division-I, they will be listed with LastD1Season=2022, and you can confirm there are 358 such teams.\n\n**Data Section 1 file: MSeasons.csv**\n\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season - indicates the year in which the tournament was played. Remember that the current season counts as 2022.\n- DayZero - tells you the date corresponding to DayNum=0 during that season. All game dates have been aligned upon a common scale so that (each year) the Monday championship game of the men's tournament is on DayNum=154. Working backward, the national semifinals are always on DayNum=152, the \"play-in\" games are on days 135, Selection Sunday is on day 132, the final day of the regular season is also day 132, and so on. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"DayNum\" with the season's \"DayZero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on DayNum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z - by our contests' convention, each of the four regions in the final tournament is assigned a letter of W, X, Y, or Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files, even if the region names change from year to year. For instance, during the 2012 tournament, the four regions were East, Midwest, South, and West. Being the first alphabetically, East becomes W. Since the East regional champion (Ohio State) played against the Midwest regional champion (Kansas) in the national semifinals, that makes Midwest be region X. For the other two (South and West), since South comes first alphabetically, that makes South Y and therefore West is Z. So for that season, the W/X/Y/Z are East,Midwest,South,West. And so for instance, Ohio State, the #2 seed in the East, is listed in the MNCAATourneySeeds file that year with a seed of W02, meaning they were the #2 seed in the W region (the East region). We will not know the final W/X/Y/Z designations until Selection Sunday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\n**Data Section 1 file: MNCAATourneySeeds.csv**\n\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are between 64-68 rows for each year, depending on whether there were any play-in games and how many there were. In recent years the structure has settled at 68 total teams, with four \"play-in\" games leading to the final field of 64 teams entering Round 1 on Thursday of the first week (by definition, that is DayNum=136 each season). We will not know the seeds of the respective tournament teams, or even exactly which 68 teams it will be, until Selection Sunday on March 13, 2022 (DayNum=132).\n\n- Season - the year that the tournament was played in\n- Seed - this is a 3/4-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For play-in teams, there is a fourth character (a or b) to further distinguish the seeds, since teams that face each other in the play-in games will have seeds with the same first three characters. The \"a\" and \"b\" are assigned based on which Team ID is lower numerically. As an example of the format of the seed, the first record in the file is seed W01 from 1985, which means we are looking at the #1 seed in the W region (which we can see from the \"MSeasons.csv\" file was the East region).\n- TeamID - this identifies the id number of the team, as specified in the MTeams.csv file\n\n**Data Section 1 file: MRegularSeasonCompactResults.csv**\n\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1985 season (the first year the NCAA\u00ae had a 64-team tournament). For each season, the file includes all games played from DayNum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=132 is Selection Sunday, and there are always a few conference tournament finals actually played early in the day on Selection Sunday itself). Thus a game played on or before Selection Sunday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum - this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"MSeasons.csv\" file. For example, the first game in the file was DayNum=20. Combined with the fact from the \"MSeasons.csv\" file that day zero was 10/29/1984 that year, this means the first game was played 20 days later, or 11/18/1984. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID). In order to accomplish this uniqueness, we had to adjust one game's date. In March 2008, the SEC postseason tournament had to reschedule one game (Georgia-Kentucky) to a subsequent day because of a tornado, so Georgia had to actually play two games on the same day. In order to enforce this uniqueness, we moved the game date for the Georgia-Kentucky game back to its original scheduled date.\n- WTeamID - this identifies the id number of the team that won the game, as listed in the \"MTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore - this identifies the number of points scored by the winning team.\n- LTeamID - this identifies the id number of the team that lost the game.\n- LScore - this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- WLoc - this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n- NumOT - this indicates the number of overtime periods in the game, an integer 0 or higher.\n\n**Data Section 1 file: MNCAATourneyCompactResults.csv**\n\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the MRegularSeasonCompactResults data. All games will show up as neutral site (so WLoc is always N). Note that this tournament game data also includes the play-in games (which always occurred on day 134/135) for those years that had play-in games. Thus each season you will see between 63 and 67 games listed, depending on how many play-in games there were.\n\nBecause of the consistent structure of the NCAA\u00ae tournament schedule, you can actually tell what round a game was, depending on the exact DayNum. Thus:\n- DayNum=134 or 135 (Tue/Wed) - play-in games to get the tournament field down to the final 64 teams\n- DayNum=136 or 137 (Thu/Fri) - Round 1, to bring the tournament field from 64 teams to 32 teams\n- DayNum=138 or 139 (Sat/Sun) - Round 2, to bring the tournament field from 32 teams to 16 teams\n- DayNum=143 or 144 (Thu/Fri) - Round 3, otherwise known as \"Sweet Sixteen\", to bring the tournament field from 16 teams to 8 teams\n- DayNum=145 or 146 (Sat/Sun) - Round 4, otherwise known as \"Elite Eight\" or \"regional finals\", to bring the tournament field from 8 teams to 4 teams\n- DayNum=152 (Sat) - Round 5, otherwise known as \"Final Four\" or \"national semifinals\", to bring the tournament field from 4 teams to 2 teams\n- DayNum=154 (Mon) - Round 6, otherwise known as \"national final\" or \"national championship\", to bring the tournament field from 2 teams to 1 champion team\n\nSpecial note: Each year, there are also going to be other games that happened after Selection Sunday, which are not part of the NCAA\u00ae Tournament. This includes tournaments like the postseason NIT, the CBI, the CIT, and the Vegas 16. Such games are not listed in the Regular Season or the NCAA\u00ae Tourney files; they can be found in the \"Secondary Tourney\" data files within Data Section 6. Although they would not be games you would ever be predicting directly for the NCAA\u00ae tournament, and they would not be games you would have data from at the time of predicting NCAA\u00ae tournament outcomes, you may nevertheless wish to make use of these games for model optimization, depending on your methodology. The more games that you can test your predictions against, the better your optimized model might eventually become, depending on how applicable all those games are. A similar argument might be advanced in favor of optimizing your predictions against conference tournament games, which might be viewed as reasonable proxies for NCAA\u00ae tournament games.\n\n**Data Section 1 file: MSampleSubmissionStage1.csv**\n\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2016, 2017, 2018, 2019, and 2021). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2022).\n\nWhen there are 68 teams in the tournament, there are 68*67/2=2,278 predictions to make for that year, so a Stage 1 submission file will have 2,278*5=11,390 data rows.\n\n- ID - this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred - this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=1181) against Arizona (TeamID=1112) in the 2017 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2017_1112_1181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=1181) against North Carolina (TeamID=1314) in the 2018 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2018_1181_1314,0.516\n\nAlso note that a single prediction row serves as a prediction for each of the two teams' winning chances. So for instance, in Example #1, the submission row of \"2017_1112_1181,0.47\" specifically gives a 47% chance for Arizona to win, and doesn't explicitly mention Duke's 53% chance to win. However, our evaluation utility will automatically infer the winning percentage in the other direction, so a 47% prediction for Arizona to win also means a 53% prediction for Duke to win. And similarly, because the submission row in Example #2 gives Duke a 51.6% chance to beat North Carolina, we will automatically figure out that this also means North Carolina has a 48.4% chance to beat Duke.\n\n### Data Section 2 - Team Box Scores\n\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2002-03 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n- WFGM - field goals made (by the winning team)\n- WFGA - field goals attempted (by the winning team)\n- WFGM3 - three pointers made (by the winning team)\n- WFGA3 - three pointers attempted (by the winning team)\n- WFTM - free throws made (by the winning team)\n- WFTA - free throws attempted (by the winning team)\n- WOR - offensive rebounds (pulled by the winning team)\n- WDR - defensive rebounds (pulled by the winning team)\n- WAst - assists (by the winning team)\n- WTO - turnovers committed (by the winning team)\n- WStl - steals (accomplished by the winning team)\n- WBlk - blocks (accomplished by the winning team)\n- WPF - personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: MRegularSeasonDetailedResults.csv**\n\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2003 season. All games listed in the MRegularSeasonCompactResults file since the 2003 season should exactly be present in the MRegularSeasonDetailedResults file.\n\n**Data Section 2 file: MNCAATourneyDetailedResults.csv**\n\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2003 season. All games listed in the MNCAATourneyCompactResults file since the 2003 season should exactly be present in the MNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\n\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: Cities.csv**\n\nThis file provides a master list of cities that have been locations for games played. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. Also note that if you created any supplemental data last year on cities (latitude/longitude, altitude, etc.), the CityID's match between last year and this year, so you should be able to re-use that information.\n\n- CityID - a four-digit ID number uniquely identifying a city.\n- City - the text name of the city.\n- State - the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used. For instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: MGameCities.csv**\n\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season, the NCAA\u00ae tourney, and other post-season tournaments, are all listed together. There should be no games since the 2010 season where the CityID is not known. Games from the 2009 season and before are not listed in this file.\n\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results and/or Detailed Results file.\n- CRType - this can be either Regular or NCAA or Secondary. If it is Regular, you can find more about the game in the MRegularSeasonCompactResults.csv and MRegularSeasonDetailedResults.csv files. If it is NCAA, you can find more about the game in the MNCAATourneyCompactResults.csv and MNCAATourneyDetailedResults.csv files. If it is Secondary, you can find more about the game in the MSecondaryTourneyCompactResults file.\n- CityID - the ID of the city where the game was played, as specified by the CityID column in the Cities.csv file.\n\n### Data Section 4 - Public Rankings\n\nThis section provides weekly team rankings for dozens of top rating systems - Pomeroy, Sagarin, RPI, ESPN, etc., since the 2002-2003 season\n\n**Data Section 4 file: MMasseyOrdinals.csv**\n\nThis file lists out rankings (e.g. #1, #2, #3, ..., #N) of teams going back to the 2002-2003 season, under a large number of different ranking system methodologies. The information was gathered by Kenneth Massey and provided on his [College Basketball Ranking Composite page](http://www.masseyratings.com/cb/compare.htm).\n\nNote that a rating system is more precise than a ranking system, because a rating system can provide insight about the strength gap between two adjacently-ranked teams. A ranking system will just tell you who is #1 or who is #2, but a rating system might tell you whether the gap between #1 and #2 is large or small. Nevertheless, it can be hard to compare two different rating systems that are expressed in different scales, so it can be very useful to express all the systems in terms of their ordinal ranking (1, 2, 3, ..., N) of teams.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- RankingDayNum - this integer always ranges from 0 to 133, and is expressed in the same terms as a game's DayNum (where DayZero is found in the MSeasons.csv file). The RankingDayNum is intended to tell you the first day that it is appropriate to use the rankings for predicting games. For example, if RankingDayNum is 110, then the rankings ought to be based upon game outcomes up through DayNum=109, and so you can use the rankings to make predictions of games on DayNum=110 or later. The final pre-tournament rankings each year have a RankingDayNum of 133, and can thus be used to make predictions of the games from the NCAA\u00ae tournament, which start on DayNum=134 (the Tuesday after Selection Sunday).\n- SystemName - this is the (usually) 3-letter abbreviation for each distinct ranking system. These systems may evolve from year to year, but as a general rule they retain their meaning across the years. Near the top of the Massey composite page, you can find slightly longer labels describing each system, along with links to the underlying pages where the latest rankings are provided (and sometimes the calculation is described).\n- TeamID - this is the ID of the team being ranked, as described in MTeams.csv.\n- OrdinalRank - this is the overall ranking of the team in the underlying system. Most systems from recent seasons provide a complete ranking from #1 through #351, but more recently they go higher because additional teams were added to Division I in recent years.\n\nDisclaimer: you ought to be careful about your methodology when using or evaluating these ranking systems. They are presented on a weekly basis, and given a consistent date on the Massey Composite page that typically is a Sunday; that is how the ranking systems can be compared against each other on this page. However, these systems each follow their own timeline and some systems may be released on a Sunday and others on a Saturday or Monday or even Tuesday. You should remember that if a ranking is released on a Tuesday, and was calculated based on games played through Monday, it will make the system look unusually good at predicting if you use that system to forecast the very games played on Monday that already inform the rankings. To avoid this methodological trap, we have typically used a conservative RankingDayNum of Wednesday to represent the rankings that were released at approximately the end of the weekend, a few days before, even though those rankings are represented on the composite page as being on a Sunday. For some of the older years, a more precise timestamp was known for each ranking system that allowed a more precise assignment of a RankingDayNum. By convention, the final pre-tournament rankings are always expressed as RankingDayNum=133, even though sometimes the rankings for individual systems are not released until Tuesday (DayNum=134) or even Wednesday or Thursday. If you decide to use some rankings from these Massey Ordinals to inform your predictions, be forewarned that we have no control over when they are released, and not all systems may turn out to be available in time to make pre-tournament predictions by our submission deadline. In such a situation, you may wish to use the rankings from DayNum=128 or you may need to dig into the details of the actual source of the rankings, by following the respective links on the Massey Composite Page. We may also be able to provide partial releases of the final pre-tournament Massey Ordinals on the forums, so that as systems come in on Monday or Tuesday you can use them right away.\n\n### Data Section 5 - Supplements\n\nThis section contains additional supporting information, including coaches, conference affiliations, alternative team name spellings, bracket structure, and game results for NIT and other postseason tournaments.\n\n**Data Section 5 file: MTeamCoaches.csv**\n\nThis file indicates the head coach for each team in each season, including a start/finish range of DayNum's to indicate a mid-season coaching change. For scenarios where a team had the same head coach the entire season, they will be listed with a DayNum range of 0 to 154 for that season. For head coaches whose term lasted many seasons, there will be many rows listed, most of which have a DayNum range of 0 to 154 for the corresponding season.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the calendar year in which the final tournament occurs)\n- TeamID - this is the TeamID of the team that was coached, as described in MTeams.csv.\n- FirstDayNum, LastDayNum - this defines a continuous range of days within the season, during which the indicated coach was the head coach of the team. In most cases, a data row will either have FirstDayNum=0 (meaning they started the year as head coach) and/or LastDayNum=154 (meaning they ended the year as head coach), but in some cases there were multiple new coaches during a team's season, or a head coach who went on leave and then returned (in which case there would be multiple records in that season for that coach, indicating the continuous ranges of days when they were the head coach).\n- CoachName - this is a text representation of the coach's full name, in the format first_last, with underscores substituted in for spaces.\n\n**Data Section 5 file: Conferences.csv**\n\nThis file indicates the Division I conferences that have existed over the years since 1985. Each conference is listed with an abbreviation and a longer name. There has been no attempt to link up conferences who merged with other conferences, or whose names changed over time. Thus you will see, for instance, a \"Pacific-10\" conference up through the 2011 season, and then a \"Pacific-12\" conference starting in the 2012 season, and these look like different conferences in the data, even though it was still mostly the same teams. Please notice that the Cities and Conferences files are the only two that don't start with an M; this is because the data files are identical between men's and women's data, so you don't need to maintain separate listings of cities or conferences across the two contests. However, the Team Conferences data differs slightly between men's and women's, so those files do have the prefixes.\n\n- ConfAbbrev - this is a short abbreviation for each conference; the abbreviation is used in some other files to indicate the parent conference of a team or of a conference tournament.\n- Description - this is a longer text name for the conference.\n\n**Data Section 5 file: MTeamConferences.csv**\n\nThis file indicates the conference affiliations for each team during each season. Some conferences have changed their names from year to year, and/or changed which teams are part of the conference. This file tracks this information historically.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- TeamID - this identifies the TeamID (as described in MTeams.csv).\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv).\n\n**Data Section 5 file: MConferenceTourneyGames.csv**\n\nThis file indicates which games were part of each year's post-season conference tournaments (all of which finished on Selection Sunday or earlier), starting from the 2001 season. Many of these conference tournament games are held on neutral sites, and many of the games are played by tournament-caliber teams just a few days before the NCAA\u00ae tournament. Thus these games could be considered as very similar to NCAA\u00ae tournament games, and (depending on your methodology) may be of use in optimizing your predictions. However, this is NOT a new listing of games; these games are already present within the MRegularSeasonCompactResults and MRegularSeasonDetailedResults files. So this file simply helps you to identify which of the \"regular season\" games since the 2001 season were actually conference tournament games, in case that is useful information.\n\n- ConfAbbrev - this identifies the conference (as described in Conferences.csv) that the tournament was for.\n- Season, DayNum, WTeamID, LTeamID - these four columns are sufficient to uniquely identify each game. Further details about the game, such as the final score and other stats, can be found in the associated data row of the MRegularSeasonCompactResults and/or MRegularSeasonDetailedResults files.\n\n**Data Section 5 file: MSecondaryTourneyTeams.csv**\n\nThis file identifies the teams that participated in post-season tournaments other than the NCAA\u00ae Tournament (such events would run in parallel with the NCAA\u00ae Tournament). These are teams that were not invited to the NCAA\u00ae Tournament and instead were invited to some other tournament, of which the NIT is the most prominent tournament, but there have also been the CBI, CIT, and Vegas 16 (V16) at various points in recent years. Depending on your methodology, you might find it useful to have these additional game results, above and beyond what is available from the NCAA\u00ae Tournament results. Many of these teams, especially in the NIT, were \"bubble\" teams of comparable strength to several NCAA\u00ae Tournament invitees, and so these games may be of use in model optimization for predicting NCAA\u00ae Tournament results. Also note that this information could be determined just from inspecting the MSecondaryTourneyCompactResults file, but is presented in this file as well, for your convenience.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the post-season tournament was played)\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n- TeamID - this identifies the TeamID that participated in the tournament (as described in MTeams.csv).\n\n**Data Section 5 file: MSecondaryTourneyCompactResults.csv**\n\nThis file indicates the final scores for the tournament games of \"secondary\" post-season tournaments: the NIT, CBI, CIT, and Vegas 16. The detailed results (team box scores) have not been assembled for these games. For the most part, this file is exactly like other Compact Results listings, although it also has a column for Secondary Tourney. Also note that because these games are played after DayNum=132, they are NOT listed in the MRegularSeasonCompactResults file.\n\n- SecondaryTourney - this is the abbreviation of the tournament, either NIT, CBI, CIT, or V16 (which stands for Vegas 16).\n\n**Data Section 5 file: MTeamSpellings.csv**\n\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling - this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID - this identifies the TeamID for the team that has the alternative spelling (as described in MTeams.csv).\n\n**Data Section 5 file: MNCAATourneySlots**\n\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). Because of the existence of play-in games for particular seed numbers, the pairings have small differences from year to year. You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket.\n\n- Season - this is the year of the associated entry in MSeasons.csv (the year in which the final tournament occurs)\n- Slot - this uniquely identifies one of the tournament games. For play-in games, it is a three-character string identifying the seed fulfilled by the winning team, such as W16 or Z13. For regular tournament games, it is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. Even if that R2W1 slot were actually a game between the W09 and W16 teams, it is still considered to be the R2W1 slot. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed - this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\", which during the 1985 tournament would have been Georgetown. Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed - this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the MNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.\n\n**Data Section 5 file: MNCAATourneySeedRoundSlots.csv**\n\nThis file helps to represent the bracket structure in any given year. No matter where the play-in seeds are located, we can always know, for a given tournament seed, exactly what bracket slot they would be playing in, on each possible game round, and what the possible DayNum values would be for that round. Thus, if we know when a historical game was played, and what the team's seed was, we can identify the slot for that game. This can be useful in representing or simulating the tournament bracket structure.\n\n- Seed - this is the tournament seed of the team.\n- GameRound - this is the round during the tournament that the game would occur in, where Round 0 (zero) is for the play-in games, Rounds 1/2 are for the first weekend, Rounds 3/4 are for the second weekend, and Rounds 5/6 are the national semifinals and finals.\n- GameSlot - this is the game slot that the team would be playing in, during the given GameRound. The naming convention for slots is described above, in the definition of the MNCAATourneySlots file.\n- EarlyDayNum, LateDayNum - these fields describe the earliest possible, and latest possible, DayNums that the game might be played on.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "classification",
          "tabular",
          "pairwise modeling",
          "sports/basketball",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "microsoft-malware-prediction",
      "description": "Challenge description:\n# Microsoft Malware Prediction\n\nCan you predict if a machine will soon be hit with malware?\n\nThe malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.\n\nWith more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.\n\nAs one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.\n\nCan you help protect more than one billion machines from damage BEFORE it happens?\n\n## Acknowledgements\n\nThis competition is hosted by Microsoft, Windows Defender ATP Research, Northeastern University College of Computer and Information Science, and Georgia Tech Institute for Information Security & Privacy.\n\nMicrosoft contacts\nRob McCann (Robert.McCann@microsoft.com)\nChristian Seifert (chriseif@microsoft.com)\nSusan Higgs (Susan.Higgs@microsoft.com)\nMatt Duncan (Matthew.Duncan@microsoft.com)\n\nNortheastern University contact\nMansour Ahmadi (m.ahmadi@northeastern.edu)\n\nGeorgia Tech contacts\nBrendan Saltaformaggio (brendan@ece.gatech.edu)\nTaesoo Kim (taesoo@gatech.edu)\n\n## Evaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed label.\n\n### Submission File\n\nFor each MachineIdentifier in the test set, you must predict a probability for the HasDetections column. The file should contain a header and have the following format:\n\nMachineIdentifier,HasDetections\n1,0.5\n6,0.5\n14,0.5\netc.\n\n## Prizes\n\n1st Place - $12,000\n2nd Place - $7,000\n3rd Place - $3,000\n4th Place - $2,000\n5th Place - $1,000\n\nReminder: Winners of monetary prizes are required to provide a detailed report of their solution. Reports need to be written in English. Reports should be approximately four (4) pages long, and include technical details acceptable for an academic workshop. Teams must provide their report to Competition Sponsor no later than two (2) weeks after the Submission deadline. No Microsoft employee, intern, vendor, contractor, or other affiliated personnel, throughout the world, may win a cash prize in the competition.\n\n## Timeline\n\nMarch 6, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMarch 6, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nMarch 13, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Citation\n\nAddison Howard, Ben Hope, Brendan Saltaformaggio, Eric Avena, Mansour Ahmadi, Matthew Duncan, n_30, Rob McCann, and Will Cukierski. Microsoft Malware Prediction. https://kaggle.com/competitions/microsoft-malware-prediction, 2018. Kaggle.\n\n## Competition Details\n\n**Competition Host**: Microsoft\n\n**Prizes & Awards**: $25,000\n\n**Participation**:\n- 15,283 Entrants\n- 2,859 Participants\n- 2,410 Teams\n- 43,406 Submissions\n\n**Tags**: Area Under Receiver Operating Characteristic Curve\n\n**Competition Dates**:\n- Start: Dec 13, 2018\n- Close: Mar 13, 2019\n\nData description:\n# Microsoft Malware Prediction\n\nCan you predict if a machine will soon be hit with malware?\n\nThe goal of this competition is to predict a Windows machine's probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.\n\nEach row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.\n\nThe sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers' machines in the wild; it has been sampled to include a much larger proportion of malware machines.\n\n## Dataset Description\n\n### Columns\nUnavailable or self-documenting column names are marked with an \"NA\".\n\n- **MachineIdentifier** - Individual machine ID\n- **ProductName** - Defender state information e.g. win8defender\n- **EngineVersion** - Defender state information e.g. 1.1.12603.0\n- **AppVersion** - Defender state information e.g. 4.9.10586.0\n- **AvSigVersion** - Defender state information e.g. 1.217.1014.0\n- **IsBeta** - Defender state information e.g. false\n- **RtpStateBitfield** - NA\n- **IsSxsPassiveMode** - NA\n- **DefaultBrowsersIdentifier** - ID for the machine's default browser\n- **AVProductStatesIdentifier** - ID for the specific configuration of a user's antivirus software\n- **AVProductsInstalled** - NA\n- **AVProductsEnabled** - NA\n- **HasTpm** - True if machine has tpm\n- **CountryIdentifier** - ID for the country the machine is located in\n- **CityIdentifier** - ID for the city the machine is located in\n- **OrganizationIdentifier** - ID for the organization the machine belongs in, organization ID is mapped to both specific companies and broad industries\n- **GeoNameIdentifier** - ID for the geographic region a machine is located in\n- **LocaleEnglishNameIdentifier** - English name of Locale ID of the current user\n- **Platform** - Calculates platform name (of OS related properties and processor property)\n- **Processor** - This is the process architecture of the installed operating system\n- **OsVer** - Version of the current operating system\n- **OsBuild** - Build of the current operating system\n- **OsSuite** - Product suite mask for the current operating system\n- **OsPlatformSubRelease** - Returns the OS Platform sub-release (Windows Vista, Windows 7, Windows 8, TH1, TH2)\n- **OsBuildLab** - Build lab that generated the current OS. Example: 9600.17630.amd64fre.winblue_r7.150109-2022\n- **SkuEdition** - The goal of this feature is to use the Product Type defined in the MSDN to map to a 'SKU-Edition' name that is useful in population reporting. The valid Product Type are defined in %sdxroot%\\data\\windowseditions.xml. This API has been used since Vista and Server 2008, so there are many Product Types that do not apply to Windows 10. The 'SKU-Edition' is a string value that is in one of three classes of results. The design must hand each class\n- **IsProtected** - This is a calculated field derived from the Spynet Report's AV Products field. Returns: a. TRUE if there is at least one active and up-to-date antivirus product running on this machine. b. FALSE if there is no active AV product on this machine, or if the AV is active, but is not receiving the latest updates. c. null if there are no Anti Virus Products in the report. Returns: Whether a machine is protected\n- **AutoSampleOptIn** - This is the SubmitSamplesConsent value passed in from the service, available on CAMP 9+\n- **PuaMode** - Pua Enabled mode from the service\n- **SMode** - This field is set to true when the device is known to be in 'S Mode', as in, Windows 10 S mode, where only Microsoft Store apps can be installed\n- **IeVerIdentifier** - NA\n- **SmartScreen** - This is the SmartScreen enabled string value from registry. This is obtained by checking in order, HKLM\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\SmartScreenEnabled and HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\SmartScreenEnabled. If the value exists but is blank, the value \"ExistsNotSet\" is sent in telemetry\n- **Firewall** - This attribute is true (1) for Windows 8.1 and above if windows firewall is enabled, as reported by the service\n- **UacLuaenable** - This attribute reports whether or not the \"administrator in Admin Approval Mode\" user type is disabled or enabled in UAC. The value reported is obtained by reading the regkey HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\EnableLUA\n- **Census_MDC2FormFactor** - A grouping based on a combination of Device Census level hardware characteristics. The logic used to define Form Factor is rooted in business and industry standards and aligns with how people think about their device. (Examples: Smartphone, Small Tablet, All in One, Convertible\u2026)\n- **Census_DeviceFamily** - AKA DeviceClass. Indicates the type of device that an edition of the OS is intended for. Example values: Windows.Desktop, Windows.Mobile, and iOS.Phone\n- **Census_OEMNameIdentifier** - NA\n- **Census_OEMModelIdentifier** - NA\n- **Census_ProcessorCoreCount** - Number of logical cores in the processor\n- **Census_ProcessorManufacturerIdentifier** - NA\n- **Census_ProcessorModelIdentifier** - NA\n- **Census_ProcessorClass** - A classification of processors into high/medium/low. Initially used for Pricing Level SKU. No longer maintained and updated\n- **Census_PrimaryDiskTotalCapacity** - Amount of disk space on primary disk of the machine in MB\n- **Census_PrimaryDiskTypeName** - Friendly name of Primary Disk Type - HDD or SSD\n- **Census_SystemVolumeTotalCapacity** - The size of the partition that the System volume is installed on in MB\n- **Census_HasOpticalDiskDrive** - True indicates that the machine has an optical disk drive (CD/DVD)\n- **Census_TotalPhysicalRAM** - Retrieves the physical RAM in MB\n- **Census_ChassisTypeName** - Retrieves a numeric representation of what type of chassis the machine has. A value of 0 means xx\n- **Census_InternalPrimaryDiagonalDisplaySizeInInches** - Retrieves the physical diagonal length in inches of the primary display\n- **Census_InternalPrimaryDisplayResolutionHorizontal** - Retrieves the number of pixels in the horizontal direction of the internal display\n- **Census_InternalPrimaryDisplayResolutionVertical** - Retrieves the number of pixels in the vertical direction of the internal display\n- **Census_PowerPlatformRoleName** - Indicates the OEM preferred power management profile. This value helps identify the basic form factor of the device\n- **Census_InternalBatteryType** - NA\n- **Census_InternalBatteryNumberOfCharges** - NA\n- **Census_OSVersion** - Numeric OS version Example - 10.0.10130.0\n- **Census_OSArchitecture** - Architecture on which the OS is based. Derived from OSVersionFull. Example - amd64\n- **Census_OSBranch** - Branch of the OS extracted from the OsVersionFull. Example - OsBranch = fbl_partner_eeap where OsVersion = 6.4.9813.0.amd64fre.fbl_partner_eeap.140810-0005\n- **Census_OSBuildNumber** - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or 10240\n- **Census_OSBuildRevision** - OS Build revision extracted from the OsVersionFull. Example - OsBuildRevision = 1000 or 16458\n- **Census_OSEdition** - Edition of the current OS. Sourced from HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion@EditionID in registry. Example: Enterprise\n- **Census_OSSkuName** - OS edition friendly name (currently Windows only)\n- **Census_OSInstallTypeName** - Friendly description of what install was used on the machine i.e. clean\n- **Census_OSInstallLanguageIdentifier** - NA\n- **Census_OSUILocaleIdentifier** - NA\n- **Census_OSWUAutoUpdateOptionsName** - Friendly name of the WindowsUpdate auto-update settings on the machine\n- **Census_IsPortableOperatingSystem** - Indicates whether OS is booted up and running via Windows-To-Go on a USB stick\n- **Census_GenuineStateName** - Friendly name of OSGenuineStateID. 0 = Genuine\n- **Census_ActivationChannel** - Retail license key or Volume license key for a machine\n- **Census_IsFlightingInternal** - NA\n- **Census_IsFlightsDisabled** - Indicates if the machine is participating in flighting\n- **Census_FlightRing** - The ring that the device user would like to receive flights for. This might be different from the ring of the OS which is currently installed if the user changes the ring after getting a flight from a different ring\n- **Census_ThresholdOptIn** - NA\n- **Census_FirmwareManufacturerIdentifier** - NA\n- **Census_FirmwareVersionIdentifier** - NA\n- **Census_IsSecureBootEnabled** - Indicates if Secure Boot mode is enabled\n- **Census_IsWIMBootEnabled** - NA\n- **Census_IsVirtualDevice** - Identifies a Virtual Machine (machine learning model)\n- **Census_IsTouchEnabled** - Is this a touch device?\n- **Census_IsPenCapable** - Is the device capable of pen input?\n- **Census_IsAlwaysOnAlwaysConnectedCapable** - Retrieves information about whether the battery enables the device to be AlwaysOnAlwaysConnected\n- **Wdft_IsGamer** - Indicates whether the device is a gamer device or not based on its hardware combination\n- **Wdft_RegionIdentifier** - NA\n\n### Files\n- 3 files\n- Size: 8.47 GB\n- Type: csv\n- License: Subject to Competition Rules",
      "docker_challenge_path": "/data/microsoft-malware-prediction",
      "competition_description": "Challenge description:\n# Microsoft Malware Prediction\n\nCan you predict if a machine will soon be hit with malware?\n\nThe malware industry continues to be a well-organized, well-funded market dedicated to evading traditional security measures. Once a computer is infected by malware, criminals can hurt consumers and enterprises in many ways.\n\nWith more than one billion enterprise and consumer customers, Microsoft takes this problem very seriously and is deeply invested in improving security.\n\nAs one part of their overall strategy for doing so, Microsoft is challenging the data science community to develop techniques to predict if a machine will soon be hit with malware. As with their previous, Malware Challenge (2015), Microsoft is providing Kagglers with an unprecedented malware dataset to encourage open-source progress on effective techniques for predicting malware occurrences.\n\nCan you help protect more than one billion machines from damage BEFORE it happens?",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed label.",
      "dataset_description": "Data description:\n# Microsoft Malware Prediction\n\nCan you predict if a machine will soon be hit with malware?\n\nThe goal of this competition is to predict a Windows machine's probability of getting infected by various families of malware, based on different properties of that machine. The telemetry data containing these properties and the machine infections was generated by combining heartbeat and threat reports collected by Microsoft's endpoint protection solution, Windows Defender.\n\nEach row in this dataset corresponds to a machine, uniquely identified by a MachineIdentifier. HasDetections is the ground truth and indicates that Malware was detected on the machine. Using the information and labels in train.csv, you must predict the value for HasDetections for each machine in test.csv.\n\nThe sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running. Malware detection is inherently a time-series problem, but it is made complicated by the introduction of new machines, machines that come online and offline, machines that receive patches, machines that receive new operating systems, etc. While the dataset provided here has been roughly split by time, the complications and sampling requirements mentioned above may mean you may see imperfect agreement between your cross validation, public, and private scores! Additionally, this dataset is not representative of Microsoft customers' machines in the wild; it has been sampled to include a much larger proportion of malware machines.\n\n## Dataset Description\n\n### Columns\nUnavailable or self-documenting column names are marked with an \"NA\".\n\n- **MachineIdentifier** - Individual machine ID\n- **ProductName** - Defender state information e.g. win8defender\n- **EngineVersion** - Defender state information e.g. 1.1.12603.0\n- **AppVersion** - Defender state information e.g. 4.9.10586.0\n- **AvSigVersion** - Defender state information e.g. 1.217.1014.0\n- **IsBeta** - Defender state information e.g. false\n- **RtpStateBitfield** - NA\n- **IsSxsPassiveMode** - NA\n- **DefaultBrowsersIdentifier** - ID for the machine's default browser\n- **AVProductStatesIdentifier** - ID for the specific configuration of a user's antivirus software\n- **AVProductsInstalled** - NA\n- **AVProductsEnabled** - NA\n- **HasTpm** - True if machine has tpm\n- **CountryIdentifier** - ID for the country the machine is located in\n- **CityIdentifier** - ID for the city the machine is located in\n- **OrganizationIdentifier** - ID for the organization the machine belongs in, organization ID is mapped to both specific companies and broad industries\n- **GeoNameIdentifier** - ID for the geographic region a machine is located in\n- **LocaleEnglishNameIdentifier** - English name of Locale ID of the current user\n- **Platform** - Calculates platform name (of OS related properties and processor property)\n- **Processor** - This is the process architecture of the installed operating system\n- **OsVer** - Version of the current operating system\n- **OsBuild** - Build of the current operating system\n- **OsSuite** - Product suite mask for the current operating system\n- **OsPlatformSubRelease** - Returns the OS Platform sub-release (Windows Vista, Windows 7, Windows 8, TH1, TH2)\n- **OsBuildLab** - Build lab that generated the current OS. Example: 9600.17630.amd64fre.winblue_r7.150109-2022\n- **SkuEdition** - The goal of this feature is to use the Product Type defined in the MSDN to map to a 'SKU-Edition' name that is useful in population reporting. The valid Product Type are defined in %sdxroot%\\data\\windowseditions.xml. This API has been used since Vista and Server 2008, so there are many Product Types that do not apply to Windows 10. The 'SKU-Edition' is a string value that is in one of three classes of results. The design must hand each class\n- **IsProtected** - This is a calculated field derived from the Spynet Report's AV Products field. Returns: a. TRUE if there is at least one active and up-to-date antivirus product running on this machine. b. FALSE if there is no active AV product on this machine, or if the AV is active, but is not receiving the latest updates. c. null if there are no Anti Virus Products in the report. Returns: Whether a machine is protected\n- **AutoSampleOptIn** - This is the SubmitSamplesConsent value passed in from the service, available on CAMP 9+\n- **PuaMode** - Pua Enabled mode from the service\n- **SMode** - This field is set to true when the device is known to be in 'S Mode', as in, Windows 10 S mode, where only Microsoft Store apps can be installed\n- **IeVerIdentifier** - NA\n- **SmartScreen** - This is the SmartScreen enabled string value from registry. This is obtained by checking in order, HKLM\\SOFTWARE\\Policies\\Microsoft\\Windows\\System\\SmartScreenEnabled and HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\SmartScreenEnabled. If the value exists but is blank, the value \"ExistsNotSet\" is sent in telemetry\n- **Firewall** - This attribute is true (1) for Windows 8.1 and above if windows firewall is enabled, as reported by the service\n- **UacLuaenable** - This attribute reports whether or not the \"administrator in Admin Approval Mode\" user type is disabled or enabled in UAC. The value reported is obtained by reading the regkey HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\\EnableLUA\n- **Census_MDC2FormFactor** - A grouping based on a combination of Device Census level hardware characteristics. The logic used to define Form Factor is rooted in business and industry standards and aligns with how people think about their device. (Examples: Smartphone, Small Tablet, All in One, Convertible\u2026)\n- **Census_DeviceFamily** - AKA DeviceClass. Indicates the type of device that an edition of the OS is intended for. Example values: Windows.Desktop, Windows.Mobile, and iOS.Phone\n- **Census_OEMNameIdentifier** - NA\n- **Census_OEMModelIdentifier** - NA\n- **Census_ProcessorCoreCount** - Number of logical cores in the processor\n- **Census_ProcessorManufacturerIdentifier** - NA\n- **Census_ProcessorModelIdentifier** - NA\n- **Census_ProcessorClass** - A classification of processors into high/medium/low. Initially used for Pricing Level SKU. No longer maintained and updated\n- **Census_PrimaryDiskTotalCapacity** - Amount of disk space on primary disk of the machine in MB\n- **Census_PrimaryDiskTypeName** - Friendly name of Primary Disk Type - HDD or SSD\n- **Census_SystemVolumeTotalCapacity** - The size of the partition that the System volume is installed on in MB\n- **Census_HasOpticalDiskDrive** - True indicates that the machine has an optical disk drive (CD/DVD)\n- **Census_TotalPhysicalRAM** - Retrieves the physical RAM in MB\n- **Census_ChassisTypeName** - Retrieves a numeric representation of what type of chassis the machine has. A value of 0 means xx\n- **Census_InternalPrimaryDiagonalDisplaySizeInInches** - Retrieves the physical diagonal length in inches of the primary display\n- **Census_InternalPrimaryDisplayResolutionHorizontal** - Retrieves the number of pixels in the horizontal direction of the internal display\n- **Census_InternalPrimaryDisplayResolutionVertical** - Retrieves the number of pixels in the vertical direction of the internal display\n- **Census_PowerPlatformRoleName** - Indicates the OEM preferred power management profile. This value helps identify the basic form factor of the device\n- **Census_InternalBatteryType** - NA\n- **Census_InternalBatteryNumberOfCharges** - NA\n- **Census_OSVersion** - Numeric OS version Example - 10.0.10130.0\n- **Census_OSArchitecture** - Architecture on which the OS is based. Derived from OSVersionFull. Example - amd64\n- **Census_OSBranch** - Branch of the OS extracted from the OsVersionFull. Example - OsBranch = fbl_partner_eeap where OsVersion = 6.4.9813.0.amd64fre.fbl_partner_eeap.140810-0005\n- **Census_OSBuildNumber** - OS Build number extracted from the OsVersionFull. Example - OsBuildNumber = 10512 or 10240\n- **Census_OSBuildRevision** - OS Build revision extracted from the OsVersionFull. Example - OsBuildRevision = 1000 or 16458\n- **Census_OSEdition** - Edition of the current OS. Sourced from HKLM\\Software\\Microsoft\\Windows NT\\CurrentVersion@EditionID in registry. Example: Enterprise\n- **Census_OSSkuName** - OS edition friendly name (currently Windows only)\n- **Census_OSInstallTypeName** - Friendly description of what install was used on the machine i.e. clean\n- **Census_OSInstallLanguageIdentifier** - NA\n- **Census_OSUILocaleIdentifier** - NA\n- **Census_OSWUAutoUpdateOptionsName** - Friendly name of the WindowsUpdate auto-update settings on the machine\n- **Census_IsPortableOperatingSystem** - Indicates whether OS is booted up and running via Windows-To-Go on a USB stick\n- **Census_GenuineStateName** - Friendly name of OSGenuineStateID. 0 = Genuine\n- **Census_ActivationChannel** - Retail license key or Volume license key for a machine\n- **Census_IsFlightingInternal** - NA\n- **Census_IsFlightsDisabled** - Indicates if the machine is participating in flighting\n- **Census_FlightRing** - The ring that the device user would like to receive flights for. This might be different from the ring of the OS which is currently installed if the user changes the ring after getting a flight from a different ring\n- **Census_ThresholdOptIn** - NA\n- **Census_FirmwareManufacturerIdentifier** - NA\n- **Census_FirmwareVersionIdentifier** - NA\n- **Census_IsSecureBootEnabled** - Indicates if Secure Boot mode is enabled\n- **Census_IsWIMBootEnabled** - NA\n- **Census_IsVirtualDevice** - Identifies a Virtual Machine (machine learning model)\n- **Census_IsTouchEnabled** - Is this a touch device?\n- **Census_IsPenCapable** - Is the device capable of pen input?\n- **Census_IsAlwaysOnAlwaysConnectedCapable** - Retrieves information about whether the battery enables the device to be AlwaysOnAlwaysConnected\n- **Wdft_IsGamer** - Indicates whether the device is a gamer device or not based on its hardware combination\n- **Wdft_RegionIdentifier** - NA\n\n### Files\n- 3 files\n- Size: 8.47 GB\n- Type: csv\n- License: Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "binary classification",
          "tabular",
          "feature engineering",
          "cybersecurity",
          "auc"
        ]
      }
    },
    {
      "challenge_name": "nlp-getting-started",
      "description": "Challenge description:\n# Natural Language Processing with Disaster Tweets\n\n## Competition Description\n\nWelcome to one of our \"Getting Started\" competitions \ud83d\udc4b\n\nThis particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don't have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they're observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it's not always clear whether a person's words are actually announcing a disaster. Take this example:\n\nThe author explicitly uses the word \"ABLAZE\" but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it's less clear to a machine.\n\nIn this competition, you're challenged to build a machine learning model that predicts which Tweets are about real disasters and which one's aren't. You'll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n## Getting Started\n\nTo get started quickly, feel free to take advantage of this starter notebook.\n\n## Acknowledgments\n\nThis dataset was created by the company figure-eight and originally shared on their 'Data For Everyone' website here.\n\nTweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480\n\n## Evaluation\n\nSubmissions are evaluated using F1 between the predicted and expected answers.\n\nF1 is calculated as follows:\n$$F_1 = 2 * \\frac{precision * recall}{precision + recall}$$\n\nwhere:\n$$precision = \\frac{TP}{TP + FP}$$\n$$recall = \\frac{TP}{TP + FN}$$\n\nand:\n- True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n- False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n- False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.\n\n## Submission File\n\nFor each ID in the test set, you must predict 1 if the tweet is describing a real disaster, and 0 otherwise. The file should contain a header and have the following format:\n\nid,target\n0,0\n2,0\n3,1\n9,0\n11,0\n\n## FAQ\n\n### What is a Getting Started competition?\nGetting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.\n\nGetting Started competitions are a non-competitive way to get familiar with Kaggle's platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.\n\n### What's the difference between a private and public leaderboard?\nIn this competition, because it is a Getting Started competition, there is no difference. We're scoring the entire test set on the Public Leaderboard. And we will refresh the competition every two months, so the Private Leaderboard is irrelevant.\n\nFor non-Getting Started Kaggle competitions, there is the concept of a public and private leaderboard to prevent participants from \"overfitting\" to the leaderboard. If your model is \"overfit\" to a dataset then it is not generalizable outside of the dataset you trained it on. This means that your model would have low accuracy on another sample of data taken from a similar dataset.\n\n### Why are there perfect scores on the leaderboard?\nThis competition's test set labels are completely public. So it's likely that some participants will submit perfect submissions. Since there are no prizes, medals, or points associated with your leaderboard ranking in this competition, these scores have little consequence. The rankings are purely for the benefit of users to learn and see how their approach is improving.\n\n\"It's a heavily time-intensive process to manage removals from a continuously live leaderboard. The threshold for what should be removed also becomes arbitrary -- if you remove 1.00 scores, then .99 scores will quickly appear. Since there is no prize consequence, we have chosen to leave the scores as an open sandbox.\"\n\n### How do I create and manage a team?\nWhen you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the Team page.\n\nWe've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.\n\n### What are Notebooks?\nKaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Kernels supports scripts in R and Python, Jupyter Notebooks, and RMarkdown reports. Go to the Notebooks tab to view all of the publicly shared code on this competition. For more on how to use Notebooks to learn data science, visit Kaggle's Learn Courses.\n\n### Why did my team disappear from the leaderboard?\nTo keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.\n\nIf your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.\n\n### How do I contact Support?\nKaggle does not have a dedicated support team so you'll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you'll want to use this competition's discussion forum.)\n\nSupport is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can't find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often.\n\nIf your problem persists or it seems to be affecting all participants then please contact us.\n\n## Citation\n\nAddison Howard, devrishi, Phil Culliton, and Yufeng Guo. Natural Language Processing with Disaster Tweets. https://kaggle.com/competitions/nlp-getting-started, 2019. Kaggle.\n\n## Competition Details\n\n- **Competition Host**: Kaggle\n- **Prizes & Awards**: Does not award Points or Medals\n- **Participation**: 151,849 Entrants, 500 Participants, 488 Teams, 2,085 Submissions\n- **Tags**: Text, Binary Classification, NLP, Custom Metric\n- **Timeline**: This competition runs indefinitely with a rolling leaderboard\n\nData description:\nNatural Language Processing with Disaster TweetsPredict which Tweets are about real disasters and which ones are notDataset DescriptionWhat files do I need?You'll needtrain.csv,test.csvandsample_submission.csv.What should I expect the data format to be?Each sample in the train and test set has the following information:Thetextof a tweetAkeywordfrom that tweet (although this may be blank!)Thelocationthe tweet was sent from (may also be blank)What am I predicting?You are predicting whether a given tweet is about a real disaster or not. If so, predict a1. If not, predict a0.Filestrain.csv- the training settest.csv- the test setsample_submission.csv- a sample submission file in the correct formatColumnsid- a unique identifier for each tweettext- the text of the tweetlocation- the location the tweet was sent from (may be blank)keyword- a particular keyword from the tweet (may be blank)target- intrain.csvonly, this denotes whether a tweet is about a real disaster (1) or not (0)Files3 filesSize1.43 MBTypecsvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/nlp-getting-started",
      "competition_description": "## Competition Description\n\nWelcome to one of our \"Getting Started\" competitions \ud83d\udc4b\n\nThis particular challenge is perfect for data scientists looking to get started with Natural Language Processing. The competition dataset is not too big, and even if you don't have much personal computing power, you can do all of the work in our free, no-setup, Jupyter Notebooks environment called Kaggle Notebooks.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they're observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it's not always clear whether a person's words are actually announcing a disaster. Take this example:\n\nThe author explicitly uses the word \"ABLAZE\" but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it's less clear to a machine.\n\nIn this competition, you're challenged to build a machine learning model that predicts which Tweets are about real disasters and which one's aren't. You'll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated using F1 between the predicted and expected answers.\n\nF1 is calculated as follows:\n$$F_1 = 2 * \\frac{precision * recall}{precision + recall}$$\n\nwhere:\n$$precision = \\frac{TP}{TP + FP}$$\n$$recall = \\frac{TP}{TP + FN}$$\n\nand:\n- True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n- False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n- False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false.",
      "dataset_description": "Natural Language Processing with Disaster TweetsPredict which Tweets are about real disasters and which ones are notDataset DescriptionWhat files do I need?You'll needtrain.csv,test.csvandsample_submission.csv.What should I expect the data format to be?Each sample in the train and test set has the following information:Thetextof a tweetAkeywordfrom that tweet (although this may be blank!)Thelocationthe tweet was sent from (may also be blank)What am I predicting?You are predicting whether a given tweet is about a real disaster or not. If so, predict a1. If not, predict a0.Filestrain.csv- the training settest.csv- the test setsample_submission.csv- a sample submission file in the correct formatColumnsid- a unique identifier for each tweettext- the text of the tweetlocation- the location the tweet was sent from (may be blank)keyword- a particular keyword from the tweet (may also be blank)target- intrain.csvonly, this denotes whether a tweet is about a real disaster (1) or not (0)Files3 filesSize1.43 MBTypecsvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "nlp",
        "keywords": [
          "classification",
          "text",
          "embedding",
          "social_media",
          "f1"
        ]
      }
    },
    {
      "challenge_name": "novozymes-enzyme-stability-prediction",
      "description": "Challenge description:\n# Novozymes Enzyme Stability Prediction\n\n## Goal of the Competition\n\nEnzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences.\n\nUnderstanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world's challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts.\n\n## Context\n\nNovozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment.\n\nHowever, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest.\n\nComputational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge.\n\nIn this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes's high throughput screening lab. You'll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost.\n\nNovozymes is the world's leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less.\n\nTogether, we find biological answers for better lives in a growing world. Let's Rethink Tomorrow. This is Novozymes' purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport.\n\n## Evaluation\n\nSubmissions are evaluated on the Spearman's correlation coefficient between the ground truth and the predictions.\n\n## Submission File\n\nEach seq_id represents a single-mutation variant of an enzyme. Your task is to rank the stability of these variants, assigning greater ranks to more stable variants. For each seq_id in the test set, you must predict the value for the target tm. The file should contain a header and have the following format:\n\nseq_id,tm\n31394,9.7\n31395,56.3\n31396,112.4\netc.\n\n## Timeline\n\nSeptember 21, 2022 - Start Date.\nDecember 27, 2022 - Entry Deadline. You must accept the competition rules before this date in order to compete.\nDecember 27, 2022* - Team Merger Deadline. This is the last day participants may join or merge teams.\nJanuary 3, 2023 - Final Submission Deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $12,000\n2nd Place - $8,000\n3rd Place - $5,000\n\n## Citation\n\nDennis Pultz, Esben Friis, Jesper Salomon, Peter Fischer Hallin, Sarah Baag\u00f8e J\u00f8rgensen, Walter Reade, and Maggie Demkin. Novozymes Enzyme Stability Prediction. https://kaggle.com/competitions/novozymes-enzyme-stability-prediction, 2022. Kaggle.\n\n## Competition Host\n\nNovozymes\n\n## Total Prize Money\n\n$25,000\n\nData description:\nIn this competition, you are asked to develop models that can predict the ranking of protein thermostability (as measured by melting point, tm) after single-point amino acid mutation and deletion.\n\nFor the training set, the protein thermostability (experimental melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences. The data are mainly from different sources of published studies such as Meltome atlas\u2014thermal proteome stability across the tree of life. Many other public datasets exist for protein stability; please see the competition Rule 7C for external data usage requirements. There are also other publicly available methods which can predict protein stabilities such as ESM, EVE and Rosetta etc., without using the provided training set. These methods may also be used as part of the competition.\n\nThe test set contains experimental melting temperature of over 2,413 single-mutation variant of an enzyme (GenBank: KOC15878.1), obtained by Novozymes A/S. The amino acid sequence of the wild type is:\nVPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\n\nFiles:\ntrain.csv- the training data, with columns as follows:\nseq_id: unique identifier of each protein variants\nprotein_sequence: amino acid sequence of each protein variant. The stability (as measured by tm) of protein is determined by its protein sequence. (Please note that most of the sequences in the test data have the same length of 221 amino acids, but some of them have 220 because of amino acid deletion.)\npH: the scale used to specify the acidity of an aqueous solution under which the stability of protein was measured. Stability of the same protein can change at different pH levels.\ndata_source: source where the data was published\ntm: target column. Since only the spearman correlation will be used for the evaluation, the correct prediction of the relative order is more important than the absolute tm values. (Higher tm means the protein variant is more stable.)\n\ntrain_updates_20220929.csv- corrected rows in train, please see this forum post for details\ntest.csv- the test data; your task is to predict the target tm for each protein_sequence (indicated by a unique seq_id)\nsample_submission.csv- a sample submission file in the correct format, with seq_id values corresponding to test.csv\nwildtype_structure_prediction_af2.pdb- the 3 dimensional structure of the enzyme listed above, as predicted by AlphaFold\n\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/novozymes-enzyme-stability-prediction",
      "competition_description": "# Novozymes Enzyme Stability Prediction\n\n## Goal of the Competition\n\nEnzymes are proteins that act as catalysts in the chemical reactions of living organisms. The goal of this competition is to predict the thermostability of enzyme variants. The experimentally measured thermostability (melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences.\n\nUnderstanding and accurately predict protein stability is a fundamental problem in biotechnology. Its applications include enzyme engineering for addressing the world's challenges in sustainability, carbon neutrality and more. Improvements to enzyme stability could lower costs and increase the speed scientists can iterate on concepts.\n\n## Context\n\nNovozymes finds enzymes in nature and optimizes them for use in industry. In industry, enzymes replace chemicals and accelerate production processes. They help our customers make more from less, while saving energy and generating less waste. Enzymes are widely used in laundry and dishwashing detergents where they remove stains and enable low-temperature washing and concentrated detergents. Other enzymes improve the quality of bread, beer and wine, or increase the nutritional value of animal feed. Enzymes are also used in the production of biofuels where they turn starch or cellulose from biomass into sugars which can be fermented to ethanol. These are just a few examples as we sell enzymes to more than 40 different industries. Like enzymes, microorganisms have natural properties that can be put to use in a variety of processes. Novozymes supplies a range of microorganisms for use in agriculture, animal health and nutrition, industrial cleaning and wastewater treatment.\n\nHowever, many enzymes are only marginally stable, which limits their performance under harsh application conditions. Instability also decreases the amount of protein that can be produced by the cell. Therefore, the development of efficient computational approaches to predict protein stability carries enormous technical and scientific interest.\n\nComputational protein stability prediction based on physics principles have made remarkable progress thanks to advanced physics-based methods such as FoldX, Rosetta, and others. Recently, many machine learning methods were proposed to predict the stability impact of mutations on protein based on the pattern of variation in natural sequences and their three dimensional structures. More and more protein structures are being solved thanks to the recent breakthrough of AlphaFold2. However, accurate prediction of protein thermal stability remains a great challenge.\n\nIn this competition, Novozymes invites you to develop a model to predict/rank the thermostability of enzyme variants based on experimental melting temperature data, which is obtained from Novozymes's high throughput screening lab. You'll have access to data from previous scientific publications. The available thermostability data spans from natural sequences to engineered sequences with single or multiple mutations upon the natural sequences. If successful, you'll help tackle the fundamental problem of improving protein stability, making the approach to design novel and useful proteins, like enzymes and therapeutics, more rapidly and at lower cost.\n\nNovozymes is the world's leading biotech powerhouse. Our growing world is faced with pressing needs, emphasizing the necessity for solutions that can ensure the health of the planet and its population. At Novozymes, we believe biotech is at the core of connecting those societal needs with the challenges and opportunities our customers face. Novozymes is the global market leader in biological solutions, producing a wide range of enzymes, microorganisms, technical and digital solutions which help our customers, amongst other things, add new features to their products and produce more from less.\n\nTogether, we find biological answers for better lives in a growing world. Let's Rethink Tomorrow. This is Novozymes' purpose statement. Novozymes strives to have great impact by balancing good business for our customers and our company, while spearheading environmental and social change. In 2021, Novozymes enabled savings of 60 million tons of CO2 in global transport.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on the Spearman's correlation coefficient between the ground truth and the predictions.",
      "dataset_description": "Data description:\nIn this competition, you are asked to develop models that can predict the ranking of protein thermostability (as measured by melting point, tm) after single-point amino acid mutation and deletion.\n\nFor the training set, the protein thermostability (experimental melting temperature) data includes natural sequences, as well as engineered sequences with single or multiple mutations upon the natural sequences. The data are mainly from different sources of published studies such as Meltome atlas\u2014thermal proteome stability across the tree of life. Many other public datasets exist for protein stability; please see the competition Rule 7C for external data usage requirements. There are also other publicly available methods which can predict protein stabilities such as ESM, EVE and Rosetta etc., without using the provided training set. These methods may also be used as part of the competition.\n\nThe test set contains experimental melting temperature of over 2,413 single-mutation variant of an enzyme (GenBank: KOC15878.1), obtained by Novozymes A/S. The amino acid sequence of the wild type is:\nVPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\n\nFiles:\ntrain.csv- the training data, with columns as follows:\nseq_id: unique identifier of each protein variants\nprotein_sequence: amino acid sequence of each protein variant. The stability (as measured by tm) of protein is determined by its protein sequence. (Please note that most of the sequences in the test data have the same length of 221 amino acids, but some of them have 220 because of amino acid deletion.)\npH: the scale used to specify the acidity of an aqueous solution under which the stability of protein was measured. Stability of the same protein can change at different pH levels.\ndata_source: source where the data was published\ntm: target column. Since only the spearman correlation will be used for the evaluation, the correct prediction of the relative order is more important than the absolute tm values. (Higher tm means the protein variant is more stable.)\n\ntrain_updates_20220929.csv- corrected rows in train, please see this forum post for details\ntest.csv- the test data; your task is to predict the target tm for each protein_sequence (indicated by a unique seq_id)\nsample_submission.csv- a sample submission file in the correct format, with seq_id values corresponding to test.csv\nwildtype_structure_prediction_af2.pdb- the 3 dimensional structure of the enzyme listed above, as predicted by AlphaFold\n\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "bioinformatics",
        "keywords": [
          "ranking",
          "chemistry",
          "sequence_embedding",
          "biology",
          "spearman_correlation"
        ]
      }
    },
    {
      "challenge_name": "open-problems-single-cell-perturbations",
      "description": "Challenge description:\n# Open Problems \u2013 Single-Cell Perturbations\n\n## Competition Objective\nThe goal of this competition is to predict how small molecules change gene expression in different cell types.\nYour work will help develop methods to predict how cells respond to small molecule drug perturbations, which could have important applications in drug discovery and basic biology.\n\n## Description\nHuman biology can be complex, in part due to the function and interplay of the body's approximately 37 trillion cells, which are organized into tissues, organs, and systems. However, recent advances in single-cell technologies have provided unparalleled insight into the function of cells and tissues at the level of DNA, RNA, and proteins. Yet leveraging single-cell methods to develop medicines requires mapping causal links between chemical perturbations and the downstream impact on cell state. These experiments are costly and labor intensive, and not all cells and tissues are amenable to high-throughput transcriptomic screening. If data science could help accurately predict chemical perturbations in new cell types, it could accelerate and expand the development of new medicines.\n\nSeveral methods have been developed for drug perturbation prediction, most of which are variations on the autoencoder architecture (Dr.VAE, scGEN, and ChemCPA). However, these methods lack proper benchmarking datasets with diverse cell types to determine how well they generalize. The largest available training dataset is the NIH-funded Connectivity Map (CMap), which comprises over 1.3M small molecule perturbation measurements. However, the CMap includes observations of only 978 genes, less than 5% of all genes. Furthermore, the CMap data is comprised almost entirely of measurements in cancer cell lines, which may not accurately represent human biology.\n\nCompetition host Open Problems in Single-Cell Analysis is a non-profit scientific collaboration aiming to drive innovation in single-cell data science. They are partnering to host this competition with Cellarity, a first-of-its-kind therapeutics company that develops medicines by studying and altering the cellular signatures of disease.\n\nAlthough it is impossible to measure all perturbations in all cells, we hypothesize that it is possible to measure a subset of combinations and infer the rest. Today, we are far from this goal, but we hope that this competition will serve as an important proof of concept.\n\nYour work in helping to accurately predict chemical perturbations in new cell types could accelerate the discovery and enable the creation of new medicines to treat or cure disease.\n\n## Evaluation\nWe use the Mean Rowwise Root Mean Squared Error to score submissions, computed as follows:\n$$\\textrm{MRRMSE} = \\frac{1}{R}\\sum_{i=1}^R\\left(\\frac{1}{n} \\sum_{j=1}^{n} (y_{ij} - \\widehat{y}_{ij})^2\\right)^{1/2}$$\nwhere $R$ is the number of scored rows, and $y_{ij}$ and $\\widehat{y}_{ij}$ are the actual and predicted values, respectively, for row $i$ and column $j$, and $n$ is the number of columns.\n\n### Submission File\nFor each id in the evaluation set, you should predict a value for each of the 18,211 genes named in the remaining columns. Each id corresponds to a cell_type/sm_name pair, which you may identify from the id_map.csv file.\nYour submission should contain a header and have the following format:\nid,A1BG,A1BG-AS1,...,ZZEF1\n0,0.0,0.0,...,0.0\n1,0.0,0.0,...,0.0\n2,0.0,0.0,...,0.0\n3,0.0,0.0,...,0.0...\n\n## Timeline\nSeptember 12, 2023 - Start Date.\nNovember 23, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete.\nNovember 23, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams.\nNovember 30, 2023 - Final Submission Deadline.\nDecember 12, 2023 - Judges Award Submission Deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## \"Judges\" Prizes Scoring Rubrics\nThe Judges Awards will be decided by a team of experts in single-cell biology grading write-ups according to the following criteria. Each category will be graded on a scale of 1-5, and winners will be selected from the top scoring submissions.\n\nWe're excited about this part of the competition because it encourages competitors to focus on scientific impact in addition to performance. You do not need to be top on the leaderboard to be considered for a Judges Award! You can submit regardless of your final leaderboard score. We're more interested in scientific impact than performance for these awards.\n\nWe're not sure how many submissions we'll get to this track of the competition, and we only have the bandwidth to grade ~100 submissions. We'll accept submissions from anyone, and every submission will be looked at by a judge, but not all submissions will be graded (i.e. assigned scores). For those that are graded, each submission will graded by 2 Judges on the following criteria. The final selection of the Prize-Winning Submissions will be made by the Judges, who will meet to discuss the submissions in December after the competition closes.\n\n1. **Integration of Biological Knowledge**\n   How does your model integrate biological knowledge into predictions? We want to know what you tried, and how it worked! This may include, but is not limited to:\n   - How did you integrate the ATAC data? Which representation did you use?\n   - How did you integrate LINCS data? How did this improve your model?\n   - Did you use the chemical structures in your model?\n   - Did you use other data sources? Which ones, why?\n   - What representation of the single-cell data did you use? Did you reduce genes into modules? Did you learn a gene regulatory network? We want to know!\n   - If adding a particular biological prior didn't work, how did you judge this and why do you think this failed?\n\n2. **Exploration of the problem**\n   We're interested in understanding the problem of generalizing perturbation responses across cell lines. We hope this competition not only produces record-breaking models, but also helps us better understand the problem. Here are the kinds of questions we want you to help us answer:\n   - Are there some cell types it's easier to predict across? What about sets of genes?\n   - Do you have any evidence to suggest how you might develop an ideal training set for cell type translation beyond random sampling of compounds in cell types?\n   - What is the relationship between the number of compounds measured in the held-out cell types and model performance? Is there a sweet spot?\n\n3. **Model design**\n   It's no secret that the top models on the leaderboard are often complex ensembles super-tuned to the test data, and this is amazing for our application. That said, we're also interested in knowing if there are specific classes of models that perform especially well.\n   - Is there certain technical innovation in your model that you believe represents a step-change in the field?\n   - Can you show that top performing methods can be well approximated by a simpler model?\n   - Is your model explainable? How well can you identify what is causing your model to respond to certain inputs?\n\n4. **Robustness**\n   How robust is your model to variability in the data? Here are some ideas for how you might explore this, but we're interested in unique ideas too.\n   - Take subsets of the training data (e.g. 95%, 90%, \u2026, 10%). How well does your model performs as a function of percentage of the training data?\n   - Add small amounts of noise to the input data. What kinds of noise is your model invariant to? Bonus points if the noise is biologically motivated.\n\n5. **Documentation & code style**\n   Here we want to make sure your model and analysis notebooks are well documented and follow a consistent code style. At a minimum, we want to see:\n   - Documentation describes the general methodology of the solution\n   - Documentation describes the required hardware and software dependencies, as well as how to install and run the software\n   - Functions and their arguments are documented\n   - Code follows basic good practices for the chosen programming language. For example: PEP8 for Python, tidyverse style guide for R\n   - Code does not contain duplicated code\n\n6. **Reproducibility**\n   Here we want to make sure your model and notebooks are reproducible by other scientists.\n   - Code is available on GitHub\n   - A list of required dependencies is available (e.g. dependencies.txt for Python)\n   - Repository contains a Dockerfile or Viash component which can be used to train and run the model\n   - Documentation contains an example of how to run the method using the Docker container or Viash component\n\n## Prizes\nThe competition is part of the NeurIPS 2023 Competition Track. In addition to cash prizes, winners will be invited to present at the NeurIPS 2023 Competition Workshop, held virtually in December.\n\n### Leaderboard Prizes\n$50,000 will be awarded to the Top 5 teams with the highest scores on the Kaggle Leaderboard at the conclusion of the competition:\n- 1st Place - $12,000\n- 2nd Place - $10,000\n- 3rd Place - $10,000\n- 4th Place - $10,000\n- 5th Place - $8,000\n\nAs a condition to being awarded a Leaderboard Prize, a Prize winner must provide a detailed write-up on their solution in the competition forums within 14 days of the conclusion of the competition, i.e. December 12, 2023 11:59PM UTC.\n\nNote, if you are a Leaderboard Prize winner and want to compete in the Judges Award, we will accept a Judges Prize style write-up as sufficient to be awarded a Leaderboard Prize.\n\n### \"Judges\" Prizes\n$50,000 will be awarded to 5 teams ($10,000 each) for the \"Judges\" prizes at the conclusion of the competition. Please see \"Judges\" Prizes Scoring Rubrics for the details of the evaluation criteria.\n\nTo be considered for a Judges Prize, teams will need to post a write-up in the Discussion page linked as an official writeup (see instructions), with headers corresponding to the sections in the \"Judges\" Prizes Scoring Rubrics AND fill out a short submission form to confirm you are making a submission to the Judges Prize. Details about that form will go out on November 1.\n\nAll submissions will be reviewed, and the top ~100 will be assigned grades on the rubric. Judges' reviews and scores will be single-blind (you won't know which Judges reviewed your submission). Final decisions are up to the sole discretion of the Competition Host and may not be appealed.\n\n## Citation\nDaniel Burkhardt, Andrew Benz, Richard Lieberman, Scott Gigante, Ashley Chow, Ryan Holbrook, Robrecht Cannoodt, and Malte Luecken. Open Problems \u2013 Single-Cell Perturbations. https://kaggle.com/competitions/open-problems-single-cell-perturbations, 2023. Kaggle.\n\n## Competition Host\nOpen Problems in Single-Cell Analysis\n\nData description:\n# Open Problems \u2013 Single-Cell Perturbations\n\n## Competition Objective\nPredict how small molecules change gene expression in different cell types.\n\n## Dataset Description\nFor this competition, we designed and generated a novel single-cell perturbational dataset in human peripheral blood mononuclear cells (PBMCs). We selected 144 compounds from the Library of Integrated Network-Based Cellular Signatures (LINCS) Connectivity Map dataset (PMID: 29195078) and measured single-cell gene expression profiles after 24 hours of treatment. The experiment was repeated in three healthy human donors, and the compounds were selected based on diverse transcriptional signatures observed in CD34+ hematopoietic stem cells (data not released). We performed this experiment in human PBMCs because the cells are commercially available with pre-obtained consent for public release and PBMCs are a primary, disease-relevant tissue that contains multiple mature cell types (including T-cells, B-cells, myeloid cells, and NK cells) with established markers for annotation of cell types. To supplement this dataset, we also measured cells from each donor at baseline with joint scRNA and single-cell chromatin accessibility measurements using the 10x Multiome assay. We hope that the addition of rich multi-omic data for each donor and cell type at baseline will help establish biological priors that explain the susceptibility of particular genes to exhibit perturbation responses in difference biological contexts.\n\n## Technical details about the experiment\nTo understand the dataset, it is important to know the design of the plates used to measure the treatment effect. PBMCs from donors were thawed and plated on 96-well plates. Two columns of the plates were dedicated to positive controls (dabrafenib and belinostat) and one column was dedicated to a negative control (DMSO). The positive controls were selected because they tend to have a large impact on transcription, and the negative control is used as a solvent for the compounds used in this study. The remaining wells on the plate are allocated to each of 72 compounds. The full dataset comprises 2 different compound plates per donor for a total of 6 plates.\n\nNote, each well contains PBMCs, which are a collection of different cell types. These include T cells, B cells, NK cells, and Myeloid cells like Macrophages and Monocytes. Based on the gene expression data measured in scRNA, we can computationally assign each cell to a cell type. Note, because we only measure ~350 cells per well and because compounds may have a toxic effect on some cells types, we don't always observe every cell type in every well.\n\nAnother technical variable that will impact the raw data in this experiment is the chemical tagging of each well in each row of the plate, and then pooling all samples in each row into a single pool for sequencing. This is called Cell Multiplexing, and you can read more about it on the 10x Genomics website What is Cell Multiplexing? What you need to know is that this creates some technical bias linking all the wells in each row of a plate. One purpose of including two positive controls and one negative control in each row of the plate is to allow us to account for this source of noise when we calculate differential expression.\n\n## How we calculate differential expression\nIn this competition setup, participants are tasked with modelling differential expression (DE), which enables us to estimate the impact of an experimental perturbation on the expression level of every gene in the transcription (18211 genes in this dataset). We estimate the impact of each compound by first averaging the raw gene expression counts in each cell of a specific type in each sample, which is called pseudobulking in the single-cell literature. We then fit a linear model to the pseudobulked counts data using Limma and include the library (row), plate, and donor as technical covariates and compound as the experimental covariate. Here, pseudobulked means we summed the raw counts for all cells of a given type for each well in the experiment.\n\nWhat is differential expression:\nFrom The Encyclopedia of Bioinformatics and Computational Biology: Differential gene expression, commonly abbreviated as DG or DGE analysis refers to the analysis and interpretation of differences in abundance of gene transcripts within a transcriptome (Conesa et al., 2016). Lists of genes that differ between 2 sample sets are often provided by RNA-seq data analysis tools, or can be generated manually by statistical testing of data sets. Due to the large number of genes to be tested, (e.g., >20,000 in the human genome), multiple testing correction such as Bonferroni correction is usually applied.\n\nTo learn more, we suggest starting here: Single-cell best practices - Differential gene expression analysis.\n\nThe output of this model is an estimated fold-change in gene expression and a multiple-testing corrected p-value that a given gene's expression is dependent on the compound experimental variable. There is a long rabbit hole to go down in the world of differential expression testing. We don't have a complete mechanistic model of the data generative process of collecting scRNA data, and many groups disagree on the best way to account for nuisance variables or technical noise. We picked limma because it performs well in our testing.\n\nNote, there is an opportunity for privacy leakage from the test set if we release the raw counts data and the differential expression analysis computed on all samples. To protect against this, we fit the differential expression model twice. To generate the training data, we fit the DE model on only the samples from the training set. To generate the private and public test data, we fit the DE model to all samples in the experiment. This keeps the test data private and ensures the test data is the most accurate.\n\n## Data Splits\nYour task is to predict differential expression values for Myeloid and B cells for a majority of compounds. You will train your model on data from all 144 compounds in T cells (CD4+, CD8+, regulatory) and NK cells and a 10% of compounds in the Myeloid and B cells. This mirrors a scientific context where you might want to make predictions into new cell types while taking only 1/10th of the measurements in that cell type.\n\nTrain:\n- All compounds in T, NK cells\n- 15 compounds + positive and negative controls in B and myeloid cells\n\nPublic Test:\n- 50 randomly selected compounds in B and myeloid cells\n\nPrivate Test:\n- 79 randomly selected compounds in B and myeloid cells\n\nNote that there is no additional test data beyond the indicated cell_type/sm_name pairs.\n\nThe input to your model will be a tuple of cell_type and sm_name and the output of your model will be predicted signed -log10(p-values) for all 18211 genes.\n\nWe also provide the raw data for the training split, along with 10x Multiome data for the donors at baseline. This raw data is not necessary to compete for the leaderboard prize.\n\n## Files and Field Descriptions\n\n**de_train.parquet** - Aggregated differential expression data in dense array format.\n- genes A1BG, A1BG-AS1, \u2026, ZZEF1 (numbering 18,211 in all) - Differential expression value (-log10(p-value) * sign(LFC)) for each gene. Here, LFC is the estimated log-fold change in expression between the treatment and control condition after shrinkage as calculated by Limma. Positive LFC means the gene goes up in the treatment condition relative to the control.\n- cell_type - The annotated cell type of each cell based on RNA expression.\n- sm_name - The primary name for the (parent) compound (in a standardized representation) as chosen by LINCS. This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- sm_lincs_id - The global LINCS ID (parent) compound (in a standardized representation). This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- SMILES - Simplified molecular-input line-entry system (SMILES) representations of the compounds used in the experiment. This is a 1D representation of molecular structure. These SMILES are provided by Cellarity based on the specific compounds ordered for this experiment.\n- control - Boolean indicating whether this instance was used as a control.\n\nThe de_train.parquet file comprises the main competition data. It contains values for a number of cell_type/sm_name pairs. Your goal is to predict corresponding values for the cell_type/sm_name pairs given in id_map.csv.\n\nNote: there is no DE data for the DMSO sample, because it is the negative control. All DE output is calculated in reference to the DMSO, i.e. the DE analysis asks \"how confident am I that each gene increased or decreased relative to DMSO due to the compound treatment\".\n\n**adata_train.parquet** - Unaggregated count and normalized data in COO sparse-array format. A supplement to de_train. In addition to the fields in de_train, this data also has:\n- obs_id - This is a unique identifier assigned to each cell in the raw dataset.\n- gene - Corresponds to the columns of de_train.\n- count - The raw molecular counts for the gene expression data measured in the experiment as output by 10x CellRanger.\n- normalized_count - These counts have been library size normalized and log(X+1) transformed.\n\n**adata_obs_meta.csv** - Observation metadata for adata_train.\n- library_id - A unique identifier for each library, which is a measurement made on pooled samples from each row of the plate. All cells from wells on the same row of the same plate will share a library_id.\n- plate_name - A unique ID for all samples from the same plate.\n- well - The well location of the sample on each plate (this is standard across 96 well plate experiments). It is a concatenation of row and col.\n- row - Which row on the plate the sample came from.\n- col - Which column on the plate the sample came from.\n- donor_id - Identifies the donor source of the sample, one of three.\n- cell_type - The annotated cell type of each cell based on RNA expression. This matches the cell_type in the de_train.parquet.\n- cell_id - This is included for consistency with LINCS Connectivity Map metadata, which denotes a cell_id for each cell line.\n- sm_name - The primary name for the (parent) compound (in a standardized representation) as chosen by LINCS. This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- sm_lincs_id - The global LINCS ID (parent) compound (in a standardized representation). This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- SMILES - Simplified molecular-input line-entry system (SMILES) representations of the compounds used in the experiment. This is a 1D representation of molecular structure. These SMILES are provided by Cellarity based on the specific compounds ordered for this experiment.\n- dose_uM - Dose of the compound in on a micro-molar scale. This maps to the pert_idose field in LINCS.\n- timepoint_hr - Duration of treatment in hours. This maps to the pert_itime field in LINCS.\n- control - Whether this observation was used as a control, True or False.\n\n**multiome_train.parquet** - This is optional additional 10x Multiome data for each sample at baseline.\n- obs_id - Unique identifier for each observation. (Distinct from identifiers used in adata.)\n- location - This is a feature ID. If the feature_type in multiome_var_meta.csv is Gene Expression then this is a gene symbol. If feature_type is Peaks, then this is the genomic interval of the peak.\n- count - This is the raw molecular counts of the transcript for accessible DNA measurement as output by Cellranger-Arc.\n- normalized_count - If the feature_type in multiome_var_meta.csv is Gene Expression then this is library size normalized and log(X+1) transformed counts. If feature_type is Peaks, then this is ATAC-seq peak counts transformed with TF-IDF using the default log(TF) * log(IDF).\n\n**multiome_obs_meta.csv**\n- obs_id - Identifier corresponding to that in multiome_train.parquet.\n- cell_type - The annotated cell type of each cell based on RNA expression.\n- donor_id - Identifies the donor source of the sample, one of three.\n\n**multiome_var_meta.csv**\n- location - This is a feature ID. If the feature_type is Gene Expression then this is a gene symbol. If feature_type is Peaks, then this is the genomic interval of the peak.\n- gene_id - This is an alternative unique feature ID. If the feature_type is Gene Expression then this is an Ensembl Stable Gene ID. If feature_type is Peaks, then this is the genomic interval of the peak.\n- feature_type - Denotes whether the feature is an RNA expression measurement or a Chromatin Accessibility measurement.\n- genome - The genome version used when running CellRanger-Arc\n- interval - The genomic coordinates of each feature on reference genome GRCh38. Genomic coordinates are directly related to the reference genome and include the chromosome name, start position, and end position in the following format: chr1:1234570-1234870.\n\n**id_map.csv** - Identifies the cell_type/sm_name pair to be predicted for the given id.\n\n**sample_submission.csv** - A sample submission file in the correct format.",
      "docker_challenge_path": "/data/open-problems-single-cell-perturbations",
      "competition_description": "# Open Problems \u2013 Single-Cell Perturbations\n\n## Competition Objective\nThe goal of this competition is to predict how small molecules change gene expression in different cell types.\nYour work will help develop methods to predict how cells respond to small molecule drug perturbations, which could have important applications in drug discovery and basic biology.\n\n## Description\nHuman biology can be complex, in part due to the function and interplay of the body's approximately 37 trillion cells, which are organized into tissues, organs, and systems. However, recent advances in single-cell technologies have provided unparalleled insight into the function of cells and tissues at the level of DNA, RNA, and proteins. Yet leveraging single-cell methods to develop medicines requires mapping causal links between chemical perturbations and the downstream impact on cell state. These experiments are costly and labor intensive, and not all cells and tissues are amenable to high-throughput transcriptomic screening. If data science could help accurately predict chemical perturbations in new cell types, it could accelerate and expand the development of new medicines.\n\nSeveral methods have been developed for drug perturbation prediction, most of which are variations on the autoencoder architecture (Dr.VAE, scGEN, and ChemCPA). However, these methods lack proper benchmarking datasets with diverse cell types to determine how well they generalize. The largest available training dataset is the NIH-funded Connectivity Map (CMap), which comprises over 1.3M small molecule perturbation measurements. However, the CMap includes observations of only 978 genes, less than 5% of all genes. Furthermore, the CMap data is comprised almost entirely of measurements in cancer cell lines, which may not accurately represent human biology.\n\nCompetition host Open Problems in Single-Cell Analysis is a non-profit scientific collaboration aiming to drive innovation in single-cell data science. They are partnering to host this competition with Cellarity, a first-of-its-kind therapeutics company that develops medicines by studying and altering the cellular signatures of disease.\n\nAlthough it is impossible to measure all perturbations in all cells, we hypothesize that it is possible to measure a subset of combinations and infer the rest. Today, we are far from this goal, but we hope that this competition will serve as an important proof of concept.\n\nYour work in helping to accurately predict chemical perturbations in new cell types could accelerate the discovery and enable the creation of new medicines to treat or cure disease.",
      "evaluation_metric": "## Evaluation\nWe use the Mean Rowwise Root Mean Squared Error to score submissions, computed as follows:\n$$\\textrm{MRRMSE} = \\frac{1}{R}\\sum_{i=1}^R\\left(\\frac{1}{n} \\sum_{j=1}^{n} (y_{ij} - \\widehat{y}_{ij})^2\\right)^{1/2}$$\nwhere $R$ is the number of scored rows, and $y_{ij}$ and $\\widehat{y}_{ij}$ are the actual and predicted values, respectively, for row $i$ and column $j$, and $n$ is the number of columns.",
      "dataset_description": "# Open Problems \u2013 Single-Cell Perturbations\n\n## Competition Objective\nPredict how small molecules change gene expression in different cell types.\n\n## Dataset Description\nFor this competition, we designed and generated a novel single-cell perturbational dataset in human peripheral blood mononuclear cells (PBMCs). We selected 144 compounds from the Library of Integrated Network-Based Cellular Signatures (LINCS) Connectivity Map dataset (PMID: 29195078) and measured single-cell gene expression profiles after 24 hours of treatment. The experiment was repeated in three healthy human donors, and the compounds were selected based on diverse transcriptional signatures observed in CD34+ hematopoietic stem cells (data not released). We performed this experiment in human PBMCs because the cells are commercially available with pre-obtained consent for public release and PBMCs are a primary, disease-relevant tissue that contains multiple mature cell types (including T-cells, B-cells, myeloid cells, and NK cells) with established markers for annotation of cell types. To supplement this dataset, we also measured cells from each donor at baseline with joint scRNA and single-cell chromatin accessibility measurements using the 10x Multiome assay. We hope that the addition of rich multi-omic data for each donor and cell type at baseline will help establish biological priors that explain the susceptibility of particular genes to exhibit perturbation responses in difference biological contexts.\n\n## Technical details about the experiment\nTo understand the dataset, it is important to know the design of the plates used to measure the treatment effect. PBMCs from donors were thawed and plated on 96-well plates. Two columns of the plates were dedicated to positive controls (dabrafenib and belinostat) and one column was dedicated to a negative control (DMSO). The positive controls were selected because they tend to have a large impact on transcription, and the negative control is used as a solvent for the compounds used in this study. The remaining wells on the plate are allocated to each of 72 compounds. The full dataset comprises 2 different compound plates per donor for a total of 6 plates.\n\nNote, each well contains PBMCs, which are a collection of different cell types. These include T cells, B cells, NK cells, and Myeloid cells like Macrophages and Monocytes. Based on the gene expression data measured in scRNA, we can computationally assign each cell to a cell type. Note, because we only measure ~350 cells per well and because compounds may have a toxic effect on some cells types, we don't always observe every cell type in every well.\n\nAnother technical variable that will impact the raw data in this experiment is the chemical tagging of each well in each row of the plate, and then pooling all samples in each row into a single pool for sequencing. This is called Cell Multiplexing, and you can read more about it on the 10x Genomics website What is Cell Multiplexing? What you need to know is that this creates some technical bias linking all the wells in each row of a plate. One purpose of including two positive controls and one negative control in each row of the plate is to allow us to account for this source of noise when we calculate differential expression.\n\n## How we calculate differential expression\nIn this competition setup, participants are tasked with modelling differential expression (DE), which enables us to estimate the impact of an experimental perturbation on the expression level of every gene in the transcription (18211 genes in this dataset). We estimate the impact of each compound by first averaging the raw gene expression counts in each cell of a specific type in each sample, which is called pseudobulking in the single-cell literature. We then fit a linear model to the pseudobulked counts data using Limma and include the library (row), plate, and donor as technical covariates and compound as the experimental covariate. Here, pseudobulked means we summed the raw counts for all cells of a given type for each well in the experiment.\n\nWhat is differential expression:\nFrom The Encyclopedia of Bioinformatics and Computational Biology: Differential gene expression, commonly abbreviated as DG or DGE analysis refers to the analysis and interpretation of differences in abundance of gene transcripts within a transcriptome (Conesa et al., 2016). Lists of genes that differ between 2 sample sets are often provided by RNA-seq data analysis tools, or can be generated manually by statistical testing of data sets. Due to the large number of genes to be tested, (e.g., >20,000 in the human genome), multiple testing correction such as Bonferroni correction is usually applied.\n\nTo learn more, we suggest starting here: Single-cell best practices - Differential gene expression analysis.\n\nThe output of this model is an estimated fold-change in gene expression and a multiple-testing corrected p-value that a given gene's expression is dependent on the compound experimental variable. There is a long rabbit hole to go down in the world of differential expression testing. We don't have a complete mechanistic model of the data generative process of collecting scRNA data, and many groups disagree on the best way to account for nuisance variables or technical noise. We picked limma because it performs well in our testing.\n\nNote, there is an opportunity for privacy leakage from the test set if we release the raw counts data and the differential expression analysis computed on all samples. To protect against this, we fit the differential expression model twice. To generate the training data, we fit the DE model on only the samples from the training set. To generate the private and public test data, we fit the DE model to all samples in the experiment. This keeps the test data private and ensures the test data is the most accurate.\n\n## Data Splits\nYour task is to predict differential expression values for Myeloid and B cells for a majority of compounds. You will train your model on data from all 144 compounds in T cells (CD4+, CD8+, regulatory) and NK cells and a 10% of compounds in the Myeloid and B cells. This mirrors a scientific context where you might want to make predictions into new cell types while taking only 1/10th of the measurements in that cell type.\n\nTrain:\n- All compounds in T, NK cells\n- 15 compounds + positive and negative controls in B and myeloid cells\n\nPublic Test:\n- 50 randomly selected compounds in B and myeloid cells\n\nPrivate Test:\n- 79 randomly selected compounds in B and myeloid cells\n\nNote that there is no additional test data beyond the indicated cell_type/sm_name pairs.\n\nThe input to your model will be a tuple of cell_type and sm_name and the output of your model will be predicted signed -log10(p-values) for all 18211 genes.\n\nWe also provide the raw data for the training split, along with 10x Multiome data for the donors at baseline. This raw data is not necessary to compete for the leaderboard prize.\n\n## Files and Field Descriptions\n\n**de_train.parquet** - Aggregated differential expression data in dense array format.\n- genes A1BG, A1BG-AS1, \u2026, ZZEF1 (numbering 18,211 in all) - Differential expression value (-log10(p-value) * sign(LFC)) for each gene. Here, LFC is the estimated log-fold change in expression between the treatment and control condition after shrinkage as calculated by Limma. Positive LFC means the gene goes up in the treatment condition relative to the control.\n- cell_type - The annotated cell type of each cell based on RNA expression.\n- sm_name - The primary name for the (parent) compound (in a standardized representation) as chosen by LINCS. This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- sm_lincs_id - The global LINCS ID (parent) compound (in a standardized representation). This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- SMILES - Simplified molecular-input line-entry system (SMILES) representations of the compounds used in the experiment. This is a 1D representation of molecular structure. These SMILES are provided by Cellarity based on the specific compounds ordered for this experiment.\n- control - Boolean indicating whether this instance was used as a control.\n\nThe de_train.parquet file comprises the main competition data. It contains values for a number of cell_type/sm_name pairs. Your goal is to predict corresponding values for the cell_type/sm_name pairs given in id_map.csv.\n\nNote: there is no DE data for the DMSO sample, because it is the negative control. All DE output is calculated in reference to the DMSO, i.e. the DE analysis asks \"how confident am I that each gene increased or decreased relative to DMSO due to the compound treatment\".\n\n**adata_train.parquet** - Unaggregated count and normalized data in COO sparse-array format. A supplement to de_train. In addition to the fields in de_train, this data also has:\n- obs_id - This is a unique identifier assigned to each cell in the raw dataset.\n- gene - Corresponds to the columns of de_train.\n- count - The raw molecular counts for the gene expression data measured in the experiment as output by 10x CellRanger.\n- normalized_count - These counts have been library size normalized and log(X+1) transformed.\n\n**adata_obs_meta.csv** - Observation metadata for adata_train.\n- library_id - A unique identifier for each library, which is a measurement made on pooled samples from each row of the plate. All cells from wells on the same row of the same plate will share a library_id.\n- plate_name - A unique ID for all samples from the same plate.\n- well - The well location of the sample on each plate (this is standard across 96 well plate experiments). It is a concatenation of row and col.\n- row - Which row on the plate the sample came from.\n- col - Which column on the plate the sample came from.\n- donor_id - Identifies the donor source of the sample, one of three.\n- cell_type - The annotated cell type of each cell based on RNA expression. This matches the cell_type in the de_train.parquet.\n- cell_id - This is included for consistency with LINCS Connectivity Map metadata, which denotes a cell_id for each cell line.\n- sm_name - The primary name for the (parent) compound (in a standardized representation) as chosen by LINCS. This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- sm_lincs_id - The global LINCS ID (parent) compound (in a standardized representation). This is provided to map the data in this experiment to the LINCS Connectivity Map data.\n- SMILES - Simplified molecular-input line-entry system (SMILES) representations of the compounds used in the experiment. This is a 1D representation of molecular structure. These SMILES are provided by Cellarity based on the specific compounds ordered for this experiment.\n- dose_uM - Dose of the compound in on a micro-molar scale. This maps to the pert_idose field in LINCS.\n- timepoint_hr - Duration of treatment in hours. This maps to the pert_itime field in LINCS.\n- control - Whether this observation was used as a control, True or False.\n\n**multiome_train.parquet** - This is optional additional 10x Multiome data for each sample at baseline.\n- obs_id - Unique identifier for each observation. (Distinct from identifiers used in adata.)\n- location - This is a feature ID. If the feature_type in multiome_var_meta.csv is Gene Expression then this is a gene symbol. If feature_type is Peaks, then this is the genomic interval of the peak.\n- count - This is the raw molecular counts of the transcript for accessible DNA measurement as output by Cellranger-Arc.\n- normalized_count - If the feature_type in multiome_var_meta.csv is Gene Expression then this is library size normalized and log(X+1) transformed counts. If feature_type is Peaks, then this is ATAC-seq peak counts transformed with TF-IDF using the default log(TF) * log(IDF).\n\n**multiome_obs_meta.csv**\n- obs_id - Identifier corresponding to that in multiome_train.parquet.\n- cell_type - The annotated cell type of each cell based on RNA expression.\n- donor_id - Identifies the donor source of the sample, one of three.\n\n**multiome_var_meta.csv**\n- location - This is a feature ID. If the feature_type is Gene Expression then this is a gene symbol. If feature_type is Peaks, then this is the genomic interval of the peak.\n- gene_id - This is an alternative unique feature ID. If the feature_type is Gene Expression then this is an Ensembl Stable Gene ID. If feature_type is Peaks, then this is the genomic interval of the peak.\n- feature_type - Denotes whether the feature is an RNA expression measurement or a Chromatin Accessibility measurement.\n- genome - The genome version used when running CellRanger-Arc\n- interval - The genomic coordinates of each feature on reference genome GRCh38. Genomic coordinates are directly related to the reference genome and include the chromosome name, start position, and end position in the following format: chr1:1234570-1234870.\n\n**id_map.csv** - Identifies the cell_type/sm_name pair to be predicted for the given id.\n\n**sample_submission.csv** - A sample submission file in the correct format.",
      "metadata": {
        "domain": "bioinformatics",
        "keywords": [
          "regression",
          "tabular",
          "differential_expression",
          "biology",
          "rmse"
        ]
      }
    },
    {
      "challenge_name": "otto-recommender-system",
      "description": "Challenge description:\n# OTTO \u2013 Multi-Objective Recommender System\n\n## Goal of the Competition\n\nThe goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.\n\nYour work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales.\n\n## Context\n\nOnline shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer.\n\nCurrent recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you'll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events.\n\nWith more than 10 million products from over 19,000 brands, OTTO is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France).\n\nYour work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.\n\n## Evaluation\n\nSubmissions are evaluated on Recall@20 for each action type, and the three recall values are weight-averaged:\n\n$$score = 0.10 \\cdot R_{clicks} + 0.30 \\cdot R_{carts} + 0.60 \\cdot R_{orders}$$\n\nwhere \\( R \\) is defined as\n\n$$R_{type} = \\frac{\\sum_{i}^{N} | \\{ \\text{predicted aids} \\}_{i, type} \\cap \\{ \\text{ground truth aids} \\}_{i, type} | }{\\sum_{i}^{N} \\min{( 20, | \\{ \\text{ground truth aids} \\}_{i, type} | )}}$$\n\nand \\( N \\) is the total number of sessions in the test set, and \\( \\text{predicted aids} \\) are the predictions for each session-type (e.g., each row in the submission file) truncated after the first 20 predictions.\n\nFor each session in the test data, your task is to predict the aid values for each type that occur after the last timestamp the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.\n\nFor clicks there is only a single ground truth value for each session, which is the next aid clicked during the session (although you can still predict up to 20 aid values). The ground truth for carts and orders contains all aid values that were added to a cart and ordered respectively during the session.\n\nEach session and type combination should appear on its own session_type row in the submission, and predictions should be space delimited.\n\n## Submission File\n\nFor each session id and type combination in the test set, you must predict the aid values in the label column, which is space delimited. You can predict up to 20 aid values per row. The file should contain a header and have the following format:\n\nsession_type,labels\n12906577_clicks,135193129431119318...\n12906577_carts,135193129431119318...\n12906577_orders,135193129431119318...\n12906578_clicks,135193129431119318...\netc.\n\n## Timeline\n\nNovember 1, 2022 - Start Date.\nJanuary 24, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete.\nJanuary 24, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams.\nJanuary 31, 2023 - Final Submission Deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $15,000\n2nd Place - $10,000\n3rd Place - $5,000\n\n## Citation\n\nAndreas Wand, Philipp Normann, Sophie Baumeister, Timo Wilm, Walter Reade, and Maggie Demkin. OTTO \u2013 Multi-Objective Recommender System. https://kaggle.com/competitions/otto-recommender-system, 2022. Kaggle.\n\n## Competition Host\n\nOtto (GmbH & Co KG)\n\n## Participation Statistics\n\n15,226 Entrants\n3,221 Participants\n2,574 Teams\n34,393 Submissions\n\nData description:\nOTTO \u2013 Multi-Objective Recommender System\nBuild a recommender system based on real-world e-commerce sessions\n\nThe goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.\nThe training data contains full e-commerce session information. For each session in the test data, your task is to predict the aid values for each session type that occur after the last timestamp ts in the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.\nFor additional background, please see the published OTTO Recommender Systems Dataset GitHub.\n\nDataset Description\n\nFiles:\ntrain.jsonl - the training data, which contains full session data\n  session - the unique session id\n  events - the time ordered sequence of events in the session\n  aid - the article id (product code) of the associated event\n  ts - the Unix timestamp of the event\n  type - the event type, i.e., whether a product was clicked, added to the user's cart, or ordered during the session\n\ntest.jsonl - the test data, which contains truncated session data\n  your task is to predict the next aid clicked after the session truncation, as well as the remaining aids that are added to carts and orders; you may predict up to 20 values for each session type\n\nsample_submission.csv - a sample submission file in the correct format\n\nSize: 11.89 GB\nType: jsonl, csv\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/otto-recommender-system",
      "competition_description": "Challenge description:\n# OTTO \u2013 Multi-Objective Recommender System\n\n## Goal of the Competition\n\nThe goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.\n\nYour work will help improve the shopping experience for everyone involved. Customers will receive more tailored recommendations while online retailers may increase their sales.\n\n## Context\n\nOnline shoppers have their pick of millions of products from large retailers. While such variety may be impressive, having so many options to explore can be overwhelming, resulting in shoppers leaving with empty carts. This neither benefits shoppers seeking to make a purchase nor retailers that missed out on sales. This is one reason online retailers rely on recommender systems to guide shoppers to products that best match their interests and motivations. Using data science to enhance retailers' ability to predict which products each customer actually wants to see, add to their cart, and order at any given moment of their visit in real-time could improve your customer experience the next time you shop online with your favorite retailer.\n\nCurrent recommender systems consist of various models with different approaches, ranging from simple matrix factorization to a transformer-type deep neural network. However, no single model exists that can simultaneously optimize multiple objectives. In this competition, you'll build a single entry to predict click-through, add-to-cart, and conversion rates based on previous same-session events.\n\nWith more than 10 million products from over 19,000 brands, OTTO is the largest German online shop. OTTO is a member of the Hamburg-based, multi-national Otto Group, which also subsidizes Crate & Barrel (USA) and 3 Suisses (France).\n\nYour work will help online retailers select more relevant items from a vast range to recommend to their customers based on their real-time behavior. Improving recommendations will ensure navigating through seemingly endless options is more effortless and engaging for shoppers.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on Recall@20 for each action type, and the three recall values are weight-averaged:\n\n$$score = 0.10 \\cdot R_{clicks} + 0.30 \\cdot R_{carts} + 0.60 \\cdot R_{orders}$$\n\nwhere \\( R \\) is defined as\n\n$$R_{type} = \\frac{\\sum_{i}^{N} | \\{ \\text{predicted aids} \\}_{i, type} \\cap \\{ \\text{ground truth aids} \\}_{i, type} | }{\\sum_{i}^{N} \\min{( 20, | \\{ \\text{ground truth aids} \\}_{i, type} | )}}$$\n\nand \\( N \\) is the total number of sessions in the test set, and \\( \\text{predicted aids} \\) are the predictions for each session-type (e.g., each row in the submission file) truncated after the first 20 predictions.\n\nFor each session in the test data, your task is to predict the aid values for each type that occur after the last timestamp the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.\n\nFor clicks there is only a single ground truth value for each session, which is the next aid clicked during the session (although you can still predict up to 20 aid values). The ground truth for carts and orders contains all aid values that were added to a cart and ordered respectively during the session.\n\nEach session and type combination should appear on its own session_type row in the submission, and predictions should be space delimited.",
      "dataset_description": "Data description:\nOTTO \u2013 Multi-Objective Recommender System\nBuild a recommender system based on real-world e-commerce sessions\n\nThe goal of this competition is to predict e-commerce clicks, cart additions, and orders. You'll build a multi-objective recommender system based on previous events in a user session.\nThe training data contains full e-commerce session information. For each session in the test data, your task is to predict the aid values for each session type that occur after the last timestamp ts in the test session. In other words, the test data contains sessions truncated by timestamp, and you are to predict what occurs after the point of truncation.\nFor additional background, please see the published OTTO Recommender Systems Dataset GitHub.\n\nDataset Description\n\nFiles:\ntrain.jsonl - the training data, which contains full session data\n  session - the unique session id\n  events - the time ordered sequence of events in the session\n  aid - the article id (product code) of the associated event\n  ts - the Unix timestamp of the event\n  type - the event type, i.e., whether a product was clicked, added to the user's cart, or ordered during the session\n\ntest.jsonl - the test data, which contains truncated session data\n  your task is to predict the next aid clicked after the session truncation, as well as the remaining aids that are added to carts and orders; you may predict up to 20 values for each session type\n\nsample_submission.csv - a sample submission file in the correct format\n\nSize: 11.89 GB\nType: jsonl, csv\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "recommendation_and_ranking",
        "keywords": [
          "ranking",
          "tabular",
          "sequence modeling",
          "e-commerce",
          "recall@20"
        ]
      }
    },
    {
      "challenge_name": "pku-autonomous-driving",
      "description": "Challenge description:\n# Peking University/Baidu - Autonomous Driving\n\n## Can you predict vehicle angle in different settings?\n\nWho do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles \u2015 and it's at the heart of our newest challenge.\n\nSelf-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles' ability to accurately perceive objects in traffic.\n\nBaidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They're providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.\n\nYour challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.\n\nSucceed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.\n\nPlease cite the following paper when using the dataset:\nApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving\n@inproceedings{song2019apollocar3d,\ntitle={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},\nauthor={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\npages={5452--5462},\nyear={2019}\n}\n\n## Evaluation\n\nSubmissions are evaluated on mean average precision between the predicted pose information and the correct position and rotation.\n\nWe use the following C# code to determine the translation and rotation distances:\n\npublic static double RotationDistance(Object3D o1, Object3D o2) {\n    Quaternion q1 = Quaternion.CreateFromYawPitchRoll(o1.yaw, o1.pitch, o1.roll);\n    Quaternion q2 = Quaternion.CreateFromYawPitchRoll(o2.yaw, o2.pitch, o2.roll);\n    Quaternion diff = Quaternion.Normalize(q1) * Quaternion.Inverse(Quaternion.Normalize(q2));\n    diff.W = Math.Clamp(diff.W, -1.0f, 1.0f);\n    return Object3D.RadianToDegree( Math.Acos(diff.W) );\n}\n\npublic static double TranslationDistance(Object3D o1, Object3D o2) {\n    var dx = o1.x - o2.x;\n    var dy = o1.y - o2.y;\n    var dz = o1.z - o2.z;\n    return Math.Sqrt(dx * dx + dy * dy + dz * dz);\n}\n\nWe then take the resulting distances between all pairs of objects and determine which predicted objects are closest to solution objects, and apply thresholds for both translation and rotation. Confidence scores are used to sort submission objects. Units for rotation are radians; translation is meters.\n\nIf both of the distances between prediction and solution (as calculated above) are less than the threshold, then that prediction object is counted as a true positive for that threshold. If not the predicted object is counted as a false positive for that threshold.\n\nFinally, mAP is calculated using these TP/FP determinations across all thresholds.\n\nThe thresholds are as follows:\nRotation: 50, 45, 40, 35, 30, 25, 20, 15, 10, 5\nTranslation: 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01\n\n## Submission Requirements\n\nYou can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.\n\nFor each image ID in the test set, you must predict a pose (position and rotation) for all unmasked cars in the image. The file should contain a header and have the following format:\n\nImageId,PredictionString\nID_1d7bc9b31,0.5 0.25 0.5 0.0 0.5 0.0 1.0\nID_f9c21a4e3,0.5 0.5 0.5 0.0 0.0 0.0 0.9\nID_e83dd7c22,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nID_1a050c9a4,0.5 0.5 0.5 0.0 0.0 0.0 0.25\nID_d943d1083,0.5 0.5 0.5 0.0 0.0 0.0 1.0 0.5 0.5 0.5 0.0 0.0 0.0 1.0\nID_3155084f7,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nID_f74dcaa3d,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nID_b183b55dd,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nID_ff5ea7211,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nEach 7-value element in PredictionString corresponds to pitch, yaw, roll, x, y, z and confidence for each car in the scene.\n\n## Prizes\n\n1st Place: $12,000\n2nd Place: $8,000\n3rd Place: $5,000\n\n## Timeline\n\nJanuary 14, 2020 - Entry deadline. You must accept the competition rules before this date in order to compete.\nJanuary 14, 2020 - Pre-trained model and external data disclosure deadline. Participants must disclose any external data or pre-trained models used in the official forum thread in adherence with competition rules.\nJanuary 14, 2020 - Team merger deadline. This is the last day participants may join or merge teams.\nJanuary 21, 2020 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Citation\n\nAddison Howard, Phil Culliton, Ruigang Yang, and zdf56. Peking University/Baidu - Autonomous Driving. https://kaggle.com/competitions/pku-autonomous-driving, 2019. Kaggle.\n\n## Competition Host\n\nPeking University\n\nData description:\nPeking University/Baidu - Autonomous Driving\nCan you predict vehicle angle in different settings?\n\nDataset Description\nThis dataset contains photos of streets, taken from the roof of a car. We're attempting to predict the position and orientation of all un-masked cars in the test images. You should also provide aconfidencescore indicating how sure you are of your prediction.\n\nPose Information (train.csv)\nNote that rotation values are angles expressed in radians, relative to the camera.\nThe primary data is images of cars and relatedposeinformation. Theposeinformation is formatted as strings, as follows:\nmodel type, yaw, pitch, roll, x, y, z\nA concrete example with two cars in the photo:\n5 0.5 0.5 0.5 0.0 0.0 0.0 32 0.25 0.25 0.25 0.5 0.4 0.7\n\nSubmissions (sample_submission.csv) are very similar, with the addition of a confidence score, and theremoval of the model type. You are not required to predict the model type of the vehicle in question.\nID, PredictionString\nID_1d7bc9b31,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nindicating that this prediction has aconfidencescore of1.0.\n\nImage Masks (test_masks.zip / train_masks.zip)\nSome cars in the images are not of interest (too far away, etc.).   Binary masks are provided to allow competitors to remove them from consideration.\n\nCar Models\n3D models of all cars of interest are available for download as pickle files - they can be compared against cars in images, used as references for rotation, etc.\nThe pickles were created in Python 2.  For Python 3 users, the following code will load a given model:\nwithopen(model,\"rb\")asfile:\n    pickle.load(file, encoding=\"latin1\")\n\nFile descriptions\ntrain.csv-poseinformation for all of the images in the training set.\ntrain_images.zip- images in the training set.\ntrain_masks.zip- ignore masks for the training set. (Not all images have a mask.)\ntest_images.zip- images in the test set.\ntest_masks.zip- ignore masks for the test set. (Not all images have a mask.)\nsample_submission.csv- a sample submission file in the correct format\nImageId- a unique identifier for each image (and related mask, if one exists).\nPredictionString- a collection of poses and confidence scores.\ncar_models.zip- 3D models of the unmasked cars in the training / test images. They can be used for pose estimation, etc.\ncamera.zip- camera intrinsic parameters.\n\nLicense\nSubject to Competition Rules",
      "docker_challenge_path": "/data/pku-autonomous-driving",
      "competition_description": "Challenge description:\n# Peking University/Baidu - Autonomous Driving\n\n## Can you predict vehicle angle in different settings?\n\nWho do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles \u2015 and it's at the heart of our newest challenge.\n\nSelf-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles' ability to accurately perceive objects in traffic.\n\nBaidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They're providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.\n\nYour challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.\n\nSucceed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.\n\nPlease cite the following paper when using the dataset:\nApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving\n@inproceedings{song2019apollocar3d,\ntitle={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},\nauthor={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},\nbooktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\npages={5452--5462},\nyear={2019}\n}",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on mean average precision between the predicted pose information and the correct position and rotation.\n\nWe use the following C# code to determine the translation and rotation distances:\n\npublic static double RotationDistance(Object3D o1, Object3D o2) {\n    Quaternion q1 = Quaternion.CreateFromYawPitchRoll(o1.yaw, o1.pitch, o1.roll);\n    Quaternion q2 = Quaternion.CreateFromYawPitchRoll(o2.yaw, o2.pitch, o2.roll);\n    Quaternion diff = Quaternion.Normalize(q1) * Quaternion.Inverse(Quaternion.Normalize(q2));\n    diff.W = Math.Clamp(diff.W, -1.0f, 1.0f);\n    return Object3D.RadianToDegree( Math.Acos(diff.W) );\n}\n\npublic static double TranslationDistance(Object3D o1, Object3D o2) {\n    var dx = o1.x - o2.x;\n    var dy = o1.y - o2.y;\n    var dz = o1.z - o2.z;\n    return Math.Sqrt(dx * dx + dy * dy + dz * dz);\n}\n\nWe then take the resulting distances between all pairs of objects and determine which predicted objects are closest to solution objects, and apply thresholds for both translation and rotation. Confidence scores are used to sort submission objects. Units for rotation are radians; translation is meters.\n\nIf both of the distances between prediction and solution (as calculated above) are less than the threshold, then that prediction object is counted as a true positive for that threshold. If not the predicted object is counted as a false positive for that threshold.\n\nFinally, mAP is calculated using these TP/FP determinations across all thresholds.\n\nThe thresholds are as follows:\nRotation: 50, 45, 40, 35, 30, 25, 20, 15, 10, 5\nTranslation: 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01",
      "dataset_description": "Data description:\nPeking University/Baidu - Autonomous Driving\nCan you predict vehicle angle in different settings?\n\nDataset Description\nThis dataset contains photos of streets, taken from the roof of a car. We're attempting to predict the position and orientation of all un-masked cars in the test images. You should also provide aconfidencescore indicating how sure you are of your prediction.\n\nPose Information (train.csv)\nNote that rotation values are angles expressed in radians, relative to the camera.\nThe primary data is images of cars and relatedposeinformation. Theposeinformation is formatted as strings, as follows:\nmodel type, yaw, pitch, roll, x, y, z\nA concrete example with two cars in the photo:\n5 0.5 0.5 0.5 0.0 0.0 0.0 32 0.25 0.25 0.25 0.5 0.4 0.7\n\nSubmissions (sample_submission.csv) are very similar, with the addition of a confidence score, and theremoval of the model type. You are not required to predict the model type of the vehicle in question.\nID, PredictionString\nID_1d7bc9b31,0.5 0.5 0.5 0.0 0.0 0.0 1.0\nindicating that this prediction has aconfidencescore of1.0.\n\nImage Masks (test_masks.zip / train_masks.zip)\nSome cars in the images are not of interest (too far away, etc.).   Binary masks are provided to allow competitors to remove them from consideration.\n\nCar Models\n3D models of all cars of interest are available for download as pickle files - they can be compared against cars in images, used as references for rotation, etc.\nThe pickles were created in Python 2.  For Python 3 users, the following code will load a given model:\nwithopen(model,\"rb\")asfile:\n    pickle.load(file, encoding=\"latin1\")\n\nFile descriptions\ntrain.csv-poseinformation for all of the images in the training set.\ntrain_images.zip- images in the training set.\ntrain_masks.zip- ignore masks for the training set. (Not all images have a mask.)\ntest_images.zip- images in the test set.\ntest_masks.zip- ignore masks for the test set. (Not all images have a mask.)\nsample_submission.csv- a sample submission file in the correct format\nImageId- a unique identifier for each image (and related mask, if one exists).\nPredictionString- a collection of poses and confidence scores.\ncar_models.zip- 3D models of the unmasked cars in the training / test images. They can be used for pose estimation, etc.\ncamera.zip- camera intrinsic parameters.",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "regression",
          "images",
          "3d pose estimation",
          "autonomous driving",
          "map"
        ]
      }
    },
    {
      "challenge_name": "planttraits2024",
      "description": "Challenge description:\n# Uncovering the biosphere: Predicting 6 Vital Plant Traits from Plant Images for Ecosystem Health\n\nWe welcome you to PlantTraits2024 Challenge, an exciting part of the FGVC11 workshop at CVPR 2024, where you have the chance to personally contribute to advance our understanding of the global patterns of biodiversity. Our goal is to predict a broad set of 6 plant traits (e.g. leaf area, plant height) from crowd-sourced plant images and some ancillary data.\n\nThink of plants as the superheroes of our ecosystems. Their traits hold the key to understanding ecosystems, e.g., in terms of their diversity, productivity, or how these green heroes face the challenges brought on by climate change. By solving this competition effectively, you become a superhero, too \u2014 contributing to our understanding of how plants navigate the complexities of climate change.\n\nBy analyzing thousands of plant images acquired by citizens, participation in this competition is a chance to be a part of something bigger and help us uncover the properties of our ecosystems. Are you ready for the mission?\n\n## Description\n\nThis competition aims to predict plant properties - so called plant traits - from citizen science plant photographs. Why are plant traits currently so relevant?\n\nPlant traits are plant properties that are used to describe how plants function how they interact with the environment. For instance, the trait of plant canopy height indicates how good a plant is at overshadowing its neighbors in the competition for sun light. Robust leaves (indicated by the leaf mass per leaf area) indicate that plants optimize towards extreme conditions, such as heavy winds or droughts. Yet, environmental conditions are not static. Due to global change, the biosphere is being transformed at accelerating pace. Especially climate change is assumed to drastically impact the functioning of the ecosystems. This includes several processes, e.g. adaptions of plants and their traits to new conditions or even an altered plant species distribution with a resulting modification of the distribution of plant traits. However, we can hardly project on a global scale how plant traits and as such entire ecosystems will react to climate change because we do not have sufficient data on plant traits.\n\nA data treasure in this regard may be the growing availability of citizen science photographs. Thousands of citizens around the globe photograph plants with species identification apps (examples are iNaturalist or Pl@ntNet). The species are identified using AI algorithms, and the prediction, photograph, and geolocation are curated in open databases. There are already more than 20 million plant photographs available, covering all ecosystem types and continents.\n\nIn its original form, this data initially only provides information on the species name of a plant and not its traits. However, a pioneering study showed that artificial intelligence can predict plant traits from such photographs using Convolutional Neural Networks (Schiller et al., 2021). To achieve this, we paired sample images from the iNaturalist database with plant trait data that scientists have been curating for decades for various species. The challenge was that the images and plant trait observations were not acquired for the same plant individuals or at the same time. Nevertheless, using a weakly supervised learning approach, we trained models that demonstrated the potential of this approach for a few plant traits. However, this potential was evident only for a limited number of plant traits and a couple of thousand images.\n\nThis competition aims to further unlock the potential of predicting plant traits from plant photographs. To achieve this, we gathered more training data (over 30,000 images with labels).\n\nFind here the original article:\nSchiller, C., Schmidtlein, S., Boonman, C., Moreno-Mart\u00ednez, A., & Kattenborn, T. (2021). Deep learning and citizen science enable automated plant trait predictions from photographs. Scientific Reports, 11(1), 16395. https://www.nature.com/articles/s41598-021-95616-0\n\nThe interested reader may also see these references for some background and the general idea:\nWolf, S., Mahecha, M. D., Sabatini, F. M., Wirth, C., Bruelheide, H., Kattge, J., \u2026 & Kattenborn, T. (2022). Citizen science plant observations encode global trait patterns. Nature Ecology & Evolution, 1-10. https://www.nature.com/articles/s41559-022-01904-x\nMoles, A.T., Xirocostas, Z.A. Statistical power from the people. Nat Ecol Evol 6, 1802\u20131803 (2022). https://www.nature.com/articles/s41559-022-01902-z\n\n## Competition\n\nThe primary objective of this competition is to employ deep learning-based regression models, such as Convolutional Neural Networks (CNNs) like ConvNext or Transformers, to predict plant traits from photographs. These plant traits, although available for each image, may not yield exceptionally high accuracies due to the inherent heterogeneity of citizen science data. The various plant traits describe chemical tissue properties that are loosely related to the visible appearance of plants in images. Despite the anticipated moderate accuracies, the overarching goal is to explore the potential of this approach and gain insights into global changes affecting ecosystems. Your contribution to uncovering the wealth of data and the distribution of plant traits worldwide is invaluable.\n\nTo enhance model performance, consider implementing the following strategies:\n\n**Multi-Task Learning Scheme:**\nImplement a multi-task learning approach where the model predicts multiple plant traits simultaneously from photographs. This enables the model to capture relationships between different plant traits, potentially boosting overall performance. Predicting all traits at once is considered the most promising approach.\n\n**Integration of Ancillary Geodata:**\nIn a pioneering study (Schiller et al. 2021), climate information was integrated based on the photograph's location, recognizing the significant impact of temperature, precipitation, and seasonality on plant growth. For this competition, besides climate data (derived from worldclim), additional ancillary predictors from various geodatasets are provided, including multitemporal optical and radar satellite data (MODIS, VOD, respectively), and soil information. These predictors are intended to supplement plant photographs, offering valuable contextual information. Explore the potential of training a multi-modal model where predictors include a plant image along with a single vector combining all ancillary information or multiple vectors for each information set (climate data, soil data, satellite data).\n\n**Ensembles**\nIt may also be promising to test model ensembles (different CNN backbones in combination), given that four eyes see more than one.\n\nYour contribution in exploring and utilizing this diverse set of data will not only advance the field but also contribute significantly to understanding the broader implications of global changes on ecosystems. Your efforts in this competition will help uncover valuable insights from the wealth of citizen science data.\n\nQuestions can be asked in the discussion or directly to Ayushi Sharma or Teja Kattenborn (https://www.geosense.uni-freiburg.de).\n\n## Evaluation\n\nThe models will be evaluated against the independent test data. The evaluation metric for this competition is the mean R2 over all 6 traits. The R2 is commonly used for evaluating regression models and is the ratio of the sum of squares the residuals (SSres) to the total sum of squares (SStot).\n\nThe R2 can result in large negative values. To prevent that we will only consider R2 values > 0.\n\nThe submission should include a .csv file with a prediction for each trait and the following columns: id (see labels) and a prediction for each trait (X1080, X50, \u2026). An example is given with sample_submission.csv\n\n## Timeline\n\nFebruary 01, 2024 - Competition start date.\nMay 26, 2024 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMay 26, 2024 - Team Merger deadline. This is the last day participants may join or merge teams.\nJune 02, 2024 - Final submission deadline.\nJune 16, 2024 - Active discussion. We strongly encourage teams to collaboratively engage in solution discussions. This competition is driven by a greater purpose, and active participation in solution discourse is integral to our collective success. Of course, in this timeframe we expect very active discussion, however discussion forum would be active even later.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Organizer\n\nAyushi Sharma - Sensor-based Geoinformatics, University of Freiburg\nTeja Kattenborn - Sensor-based Geoinformatics, University of Freiburg\n\n## Acknowledgements\n\nThe TRY initiative: https://www.try-db.org/de/de.php: Kattge, J., B\u00f6nisch, G., D\u00edaz, S., Lavorel, S., Prentice, I. C., Leadley, P., Tautenhahn, S., Werner, G., et al. (2020). TRY plant trait database - enhanced coverage and open access. Global Change Biology, 26(1), 119-188. doi:10.1111/gcb.14904.\n\nThe iNaturalist project: https://www.inaturalist.org/\n\nSoil grids: Hengl, T., Mendes de Jesus, J., Heuvelink, G. B., Ruiperez Gonzalez, M., Kilibarda, M., Blagoti\u0107, A., \u2026 & Kempen, B. (2017). SoilGrids250m: Global gridded soil information based on machine learning. PLoS one, 12(2), e0169748.\n\nWorldclim: Fick, S. E., & Hijmans, R. J. (2017). WorldClim 2: new 1\u2010km spatial resolution climate surfaces for global land areas. International journal of climatology, 37(12), 4302-4315.\n\n## CVPR 2024\n\nFGVC11 at CVPR 2024\nThis competition is part of the Fine-Grained Visual Categorization FGVC11 workshop on the 18th of June at the Computer Vision and Pattern Recognition Conference CVPR 2024. The task results will be presented at the workshop, and the contribution of the winning team(s) will be highlighted. Attending the workshop is not required to participate in the competition.\n\nCVPR 2024 will take place in Seattle, USA, on June 17-21, 2024.\n\nPLEASE NOTE: CVPR frequently sells out early; we cannot guarantee CVPR registration after the competition's end. If you are interested in attending, please plan ahead.\n\nYou can see a list of the FGVC11 competitions here.\n\n## Citation\n\nAwsaf, Ayushi Sharma, HCL-Jevster, inversion, Martin G\u00f6rner, and Teja Kattenborn. PlantTraits2024 - FGVC11. https://kaggle.com/competitions/planttraits2024, 2024. Kaggle.\n\nData description:\nPlantTraits2024 - FGVC11  \nUncovering the biosphere: Predicting 6 Vital Plant Traits from Plant Images for Ecosystem Health  \n\nBackground on Dataset  \nTo create this database, we utilized the TRY database (trait information) and the iNaturalist database (citizen science plant photographs). Based on the species names found in both databases, we linked the trait observations obtained from the TRY database (species-specific mean and standard deviation) with the plant photographs (iNaturalist). Based on the geocoordinates that comes with each plant photographs, we linked the ancillary predictors, which are derived from globally available raster data (WORLDCLIM, SOIL, VOD, MODIS). To state briefly, WORLDCLIM includes temperature and precipitation data, SOIL is the global soil grids dataset (interpolated products on various soil properties, such as sand content or pH value), MODIS is satellite data that measures optical reflectance of sun light (like a camera but with many wavelengths), while VOD represents data from a radar constellation that is sensitive to water content and biomass of plants. All these geodatasets are meant to serve as supporting information in addition to the plant photographs.  \n\nDataset Description  \nFiles  \ntrain_images - The folder with the training images (.jpeg)  \ntrain.csv - The labels and the ancillary data (satellite data, soil data, climate data etc.) for each training image  \ntest_images - The folder with the test images (.jpeg) that shall be used to create the predictions for the submission.  \ntest.csv - The ancillary data (satellite data, soil data, climate data etc.) for each test image  \ntarget_name_meta.csv - full names of the traits obtained from the TRY database. This is important to understand what all traits are we predicting  \nsample_submission.csv - a sample submission file in the correct format  \n\nColumns  \nid - unique id and also prefix of image name  \nWORLDCLIM_BIO[*] - These are ancillary climate variables that can be used to facilitate the trait prediction. The selection is based on Schiller et al. 2021.  \nSOIL_[*] - These are ancillary soil variables that can be used to facilitate the trait prediction.  \nMODIS_[*]/VOD_[*] - These are ancillary multitemporal satellite variables that can be used to facilitate the trait prediction (details see below).  \nX[*]_mean - These are the targets to be predicted. There are multiple traits (X3112, X1080, \u2026).  \nX[*]_sd - This is the standard deviation of the traits found for each species. You may incorporate this during the training process with a response-based data augmentation. Thereby, you can inform the model during training that the traits can vary for a species (depending on environmental conditions, see Schiller et al. 2021).  \n\nDataset Statistics  \nTotal files: 62,626  \nTotal size: 3.45 GB  \nFile types: jpeg, csv, tsv  \nColumns: 349  \nLicense: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "docker_challenge_path": "/data/planttraits2024",
      "competition_description": "## Description\n\nThis competition aims to predict plant properties - so called plant traits - from citizen science plant photographs. Why are plant traits currently so relevant?\n\nPlant traits are plant properties that are used to describe how plants function how they interact with the environment. For instance, the trait of plant canopy height indicates how good a plant is at overshadowing its neighbors in the competition for sun light. Robust leaves (indicated by the leaf mass per leaf area) indicate that plants optimize towards extreme conditions, such as heavy winds or droughts. Yet, environmental conditions are not static. Due to global change, the biosphere is being transformed at accelerating pace. Especially climate change is assumed to drastically impact the functioning of the ecosystems. This includes several processes, e.g. adaptions of plants and their traits to new conditions or even an altered plant species distribution with a resulting modification of the distribution of plant traits. However, we can hardly project on a global scale how plant traits and as such entire ecosystems will react to climate change because we do not have sufficient data on plant traits.\n\nA data treasure in this regard may be the growing availability of citizen science photographs. Thousands of citizens around the globe photograph plants with species identification apps (examples are iNaturalist or Pl@ntNet). The species are identified using AI algorithms, and the prediction, photograph, and geolocation are curated in open databases. There are already more than 20 million plant photographs available, covering all ecosystem types and continents.\n\nIn its original form, this data initially only provides information on the species name of a plant and not its traits. However, a pioneering study showed that artificial intelligence can predict plant traits from such photographs using Convolutional Neural Networks (Schiller et al., 2021). To achieve this, we paired sample images from the iNaturalist database with plant trait data that scientists have been curating for decades for various species. The challenge was that the images and plant trait observations were not acquired for the same plant individuals or at the same time. Nevertheless, using a weakly supervised learning approach, we trained models that demonstrated the potential of this approach for a few plant traits. However, this potential was evident only for a limited number of plant traits and a couple of thousand images.\n\nThis competition aims to further unlock the potential of predicting plant traits from plant photographs. To achieve this, we gathered more training data (over 30,000 images with labels).\n\nFind here the original article:\nSchiller, C., Schmidtlein, S., Boonman, C., Moreno-Mart\u00ednez, A., & Kattenborn, T. (2021). Deep learning and citizen science enable automated plant trait predictions from photographs. Scientific Reports, 11(1), 16395. https://www.nature.com/articles/s41598-021-95616-0\n\nThe interested reader may also see these references for some background and the general idea:\nWolf, S., Mahecha, M. D., Sabatini, F. M., Wirth, C., Bruelheide, H., Kattge, J., \u2026 & Kattenborn, T. (2022). Citizen science plant observations encode global trait patterns. Nature Ecology & Evolution, 1-10. https://www.nature.com/articles/s41559-022-01904-x\nMoles, A.T., Xirocostas, Z.A. Statistical power from the people. Nat Ecol Evol 6, 1802\u20131803 (2022). https://www.nature.com/articles/s41559-022-01902-z",
      "evaluation_metric": "## Evaluation\n\nThe models will be evaluated against the independent test data. The evaluation metric for this competition is the mean R2 over all 6 traits. The R2 is commonly used for evaluating regression models and is the ratio of the sum of squares the residuals (SSres) to the total sum of squares (SStot).\n\nThe R2 can result in large negative values. To prevent that we will only consider R2 values > 0.",
      "dataset_description": "Data description:\nPlantTraits2024 - FGVC11  \nUncovering the biosphere: Predicting 6 Vital Plant Traits from Plant Images for Ecosystem Health  \n\nBackground on Dataset  \nTo create this database, we utilized the TRY database (trait information) and the iNaturalist database (citizen science plant photographs). Based on the species names found in both databases, we linked the trait observations obtained from the TRY database (species-specific mean and standard deviation) with the plant photographs (iNaturalist). Based on the geocoordinates that comes with each plant photographs, we linked the ancillary predictors, which are derived from globally available raster data (WORLDCLIM, SOIL, VOD, MODIS). To state briefly, WORLDCLIM includes temperature and precipitation data, SOIL is the global soil grids dataset (interpolated products on various soil properties, such as sand content or pH value), MODIS is satellite data that measures optical reflectance of sun light (like a camera but with many wavelengths), while VOD represents data from a radar constellation that is sensitive to water content and biomass of plants. All these geodatasets are meant to serve as supporting information in addition to the plant photographs.  \n\nDataset Description  \nFiles  \ntrain_images - The folder with the training images (.jpeg)  \ntrain.csv - The labels and the ancillary data (satellite data, soil data, climate data etc.) for each training image  \ntest_images - The folder with the test images (.jpeg) that shall be used to create the predictions for the submission.  \ntest.csv - The ancillary data (satellite data, soil data, climate data etc.) for each test image  \ntarget_name_meta.csv - full names of the traits obtained from the TRY database. This is important to understand what all traits are we predicting  \nsample_submission.csv - a sample submission file in the correct format  \n\nColumns  \nid - unique id and also prefix of image name  \nWORLDCLIM_BIO[*] - These are ancillary climate variables that can be used to facilitate the trait prediction. The selection is based on Schiller et al. 2021.  \nSOIL_[*] - These are ancillary soil variables that can be used to facilitate the trait prediction.  \nMODIS_[*]/VOD_[*] - These are ancillary multitemporal satellite variables that can be used to facilitate the trait prediction (details see below).  \nX[*]_mean - These are the targets to be predicted. There are multiple traits (X3112, X1080, \u2026).  \nX[*]_sd - This is the standard deviation of the traits found for each species. You may incorporate this during the training process with a response-based data augmentation. Thereby, you can inform the model during training that the traits can vary for a species (depending on environmental conditions, see Schiller et al. 2021).  \n\nDataset Statistics  \nTotal files: 62,626  \nTotal size: 3.45 GB  \nFile types: jpeg, csv, tsv  \nColumns: 349  \nLicense: Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "regression",
          "images+tabular+multitemporal",
          "cnn+weakly_supervised+augmentation",
          "ecology",
          "r2"
        ]
      }
    },
    {
      "challenge_name": "predict-ai-model-runtime",
      "description": "Challenge description:\n# Google - Fast or Slow? Predict AI Model Runtime\n\n## Overview\nAlice is an AI model developer, but some of the models her team developed run very slow. She recently discovered compiler's configurations that change the way the compiler compiles and optimizes the models, and hence make the models run faster (or slower)! Your task is to help Alice find the best configuration for each model.\n\nYour goal: Train a machine learning model based on the runtime data provided to you in the training dataset and further predict the runtime of graphs and configurations in the test dataset.\n\n## Technical Background\nA bit of technical background on an AI compiler will help you get started! An AI model can be represented as a graph, where a node is a tensor operation (e.g. matrix multiplication, convolution, etc), and an edge represents a tensor. A compilation configuration controls how the compiler transforms the graph for a specific optimization pass. In particular, Alice can control two types of configurations/optimizations:\n\n- A layout configuration controls how tensors in the graph are laid out in the physical memory, by specifying the dimension order of each input and output of an operation node.\n- A tile configuration controls the tile size of each fused subgraph.\n\nBeing able to predict an optimal configuration for a given graph will not only help Alice's team but also improve the compiler's heuristic to select the best configuration without human's intervention. This will make AI models run more efficiently, consuming less time and resources overall!\n\n## Dataset\nOur dataset, called TpuGraphs, is the performance prediction dataset on XLA HLO graphs running on Tensor Processing Units (TPUs) v3. There are 5 data collections in total: layout:xla:random, layout:xla:default, layout:nlp:random, layout:nlp:default, and tile:xla. The final score will be the average across all collections.\n\nWe provide baseline models along with a training set up readily for you to get started at https://github.com/google-research-datasets/tpu_graphs. Please refer to our dataset paper on the details of the baseline models.\n\n## Evaluation Metric\nAs driven by realistic requirements, we use two evaluation metrics, and average them.\n\nSpecifically, for the collection tile:xla, we use the (1-slowdown) incurred of the top-K predictions to reflect how much slower the top-K configurations predicted by the model is from the actual fastest configuration as follows:\n\n$$1 - \\left(\\frac{\\text{The best runtime of the top-k predictions}}{\\text{The best runtime of all configurations}} - 1\\right) = 2 - \\frac{\\min_{i \\in K} y_i}{\\min_{i \\in A} y_i}$$\n\nwhere K is the top-K predictions, A is all configurations of the given graph from the dataset collection, and y is the measured execution time.\n\nFor the collections layout:*, we use the Kendall Tau Correlation (a ranking metric: how well does your model-predicted ranking, correspond to the real ranking of runtimes).\n\nThe choice of metrics is justified as follows. For the tile size search space, since the number of possibilities is relatively small, one can enumerate all possibilities and invoke a model on each, then choose the best few (=5, here) configurations as suggested by the model, compile with each of them, then measure the runtime of each and commit to the best. On the other hand, for the layout:* collections, the search space is quite large. Therefore, common search strategies, such as Genetic Algorithm, Simulated Annealing, and Langevin Dynamics, need access to a fitness/utility function (which can be your model). Therefore, it is important that the model can well-preserve the order of the configurations (from fastest to slowest).\n\n## Submission File\nYour submission must be a csv file with header ID,TopConfigs. Each npz/**/test/*.npz file (see Data) must have one row in the csv file.\n\nThe ID is {collection}:{test_filename_without_extension}, where collection is one of tile:xla, layout:xla:random, layout:xla:default, layout:nlp:random, and layout:nlp:default.\n\nThe TopConfigs should list the indices of configurations from fastest (smallest runtime) to slowest (largest runtime) according to your model prediction separated by \";\".\n\n- For the tile:xla collection, only the first 5 entries will be considered and the rest will be ignored.\n- For the layout:* collections, all entries will be considered (you should output a permutation of the number of configurations!).\n\nFor a sample submission file, please download sample_submission.csv from the Data tab.\n\nYou can also generate the csv using our starter code: github.com/google-research-datasets/tpu_graphs -- search for combine_csvs.py\n\n## Timeline\n- August 29, 2023 - Start Date.\n- November 10, 2023 - Entry Deadline. You must accept the competition rules before this date in order to compete.\n- November 10, 2023 - Team Merger Deadline. This is the last day participants may join or merge teams.\n- November 17, 2023 - Final Submission Deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n- 1st Place - $15,000\n- 2nd Place - $10,000\n- 3rd Place - $8,000\n- 4th Place - $7,000\n- 5th Place - $5,000\n- 6th Place - $5,000\n\nAs a condition to being awarded a Prize, a Prize winner must provide a detailed write-up on their solution in the competition forums within 14 days of the conclusion of the competition.\n\n## Additional Resources\n- To get started quickly, feel free to take advantage of this starter notebook.\n- Here is a handy link to Kaggle's competition documentation, which includes, among other things, instructions on submitting predictions.\n\nData description:\n# Google - Fast or Slow? Predict AI Model Runtime\n\n## Competition Objective\nPredict how fast an AI model runs on a TPU by forecasting the optimal configuration for graph-based computations.\n\n## Dataset Download Instructions\nYou have two options to download the dataset:\n\n1. **(Recommended)** From this page: Download the npz_all zip file (scroll towards bottom of this page, on the right, look for \"Data Explorer\", click the npz_all directory, then click the download icon left of the \"Data Explorer\" pane). Then, unzip this file to the path `~/data/tpugraphs`.\n\n2. From tensorflow.org: Follow instructions on our GitHub repo by running:\n   ```\n   curl https://raw.githubusercontent.com/google-research-datasets/tpu_graphs/main/echo_download_commands.py | python | bash\n   ```\n   This command automatically downloads to `~/data/tpugraphs`.\n\nAfter either option, your final data path should have `~/data/tpugraphs/npz/layout` and `~/data/tpugraphs/npz/tile`. By default, the trainer scripts (see GitHub repo) read from `~/data/tpugraphs`, but you may override flag `--data_root` in the trainer of tile or layout collections.\n\nAdditionally, you may download `sample_submission.csv` which contains the example submission file in the expected format.\n\n## Dataset Description\nOnce downloaded (e.g., to `~/data/tpugraphs`, which is the default path for training code) and unzipped, you'll get the following directory structure:\n\n- `npz/tile/xla/{train, valid, test}/*.npz`: contains the `.npz` files for the tile collection\n- `npz/layout/{nlp, xla}/{random, default}/{train, valid, test}/*.npz`: contains the `.npz` files for the layout collection\n\n### .npz Structures\n\n#### Tile .npz files\nSuppose a `.npz` file stores a graph (representing a kernel) with `n` nodes and `m` edges. In addition, suppose we compile the graph with `c` different configurations, and run each on a TPU. Crucially, the configuration is at the graph-level. Then, the `.npz` file stores the following dictionary (can be loaded with `d = dict(np.load(\"npz/tile/xla/train/<pick_1>.npz\"))`):\n\n- Key `\"node_feat\"`: contains `float32` matrix with shape `(n, 140)`. The `u`th row contains the feature vector for node `u < n` (please see Subsection \"Node Features\"). Nodes are ordered topologically.\n- Key `\"node_opcode\"`: contains `int32` vector with shape `(n,)`. The `u`th entry stores the op-code for node `u` (please see the mapping of opcode to instruction's name [here](https://github.com/google-research-datasets/tpu_graphs)).\n- Key `\"edge_index\"`: contains `int32` matrix with shape `(m, 2)`. If entry `i` is `[u, v]` (where `0 <= u, v < n`), then there is a directed edge from node `u <- v`, where `u` is a tensor operation consuming the output tensor of `v` (a reverse of a typical definition of an edge direction).\n- Key `\"config_feat\"`: contains `float32` matrix with shape `(c, 24)` with row `j` containing the (graph-level) configuration feature vector (please see Subsection \"Tile Config Features\").\n- Keys `\"config_runtime\"` and `\"config_runtime_normalizers\"`: both are `int64` vectors of length `c`. Entry `j` stores the runtime (in nanoseconds) of the given graph compiled with configuration `j` and a default configuration, respectively. Samples from the same graph may have slightly different `\"config_runtime_normalizers\"` because they are measured from different runs on multiple machines.\n\nFor the tile collection, your job is to predict the indices of the best configurations (i.e., ones leading to the smallest `d[\"config_runtime\"] / d[\"config_runtime_normalizers\"]`).\n\n#### Layout .npz files\nSuppose a `.npz` file stores a graph (representing the entire program) with `n` nodes and `m` edges. In addition, suppose we compile the graph with `c` different configurations, and run each on a TPU. Crucially, the configuration is at the node-level. Suppose that `n_c` of the `n` nodes are configurable. Then, the `.npz` file stores the following dictionary (can be loaded with, e.g., `d = dict(np.load(\"npz/layout/xla/random/train/<pick_1>.npz\"))`):\n\n- Keys `\"node_feat\"`, `\"node_opcode\"`, `\"edge_index\"`: are like above.\n- Key `\"node_config_ids\"`: contains `int32` vector with shape `(n_c,)` and every entry is in `{0, 1, ..., n - 1}` i.e. indicating the indices of the configurable nodes. For these nodes, they can have an additional feature vector that instructs the compiler (described next).\n- Key `\"node_config_feat\"`: contains `float32` tensor with shape `(c, n_c, 18)`. Entry `[j, k]` gives an 18-dimensional vector describing the configuration features for node `d[\"node_config_ids\"][k]` for the `j`th run (please see Subsection \"Layout Config Features\").\n- Key `\"config_runtime\"`: contains `int32` vector with shape `(c,)` where the `j`th entry contains the runtime of the `j`th run (i.e., when nodes are configured with `d[\"node_config_feat\"][j]`).\n\nFor the layout collections, your job is to predict the order of the indices from best-to-worse configurations (i.e., ones leading to the smallest `d[\"config_runtime\"]`). We do not have to use runtime normalizers for this task because the runtime variation at the entire program level is very small.\n\nOptionally, you may access key `\"node_splits\"`, which is a variable-length list of node IDs that are the starting of HLO computations in the graph (similar to functions in a program). Essentially, nodes `d[\"node_splits\"][i]` to `d[\"node_splits\"][i+1] - 1` belongs to the same computation. If you want to partition the graph into multiple segments, this information may be useful, e.g., putting nodes from the same computation in the same partition. However, you may compute your own partitioning (e.g., using METIS) as well.\n\n## Feature Descriptions\n\n### Node Features\nTo extract a node feature vector, we either copy values from various fields in an XLA's HLO instruction (a node in an HLO graph) as they are, or convert categorical values using one-hot encoding. To convert an unbounded list of numbers (e.g. tensor shape) to a fixed-size vector, we truncate the list to six elements and include the summation and/or product of all elements in the list (e.g., the product of dimension sizes represents the volume of the tensor). In our dataset, none of the tensors has more than six dimensions.\n\nThe following describe each element at a particular index in the node feature vector:\n\n0: `is_root` - whether this node is the output  \n1: `element_size_in_bits` - deprecated, always 0  \n\n// 2\u201320: One hot vector of shape_element_type  \n2: `shape_element_type_is_invalid_type`  \n3: `shape_element_type_is_pred`  \n4: `shape_element_type_is_s8`  \n5: `shape_element_type_is_s16`  \n6: `shape_element_type_is_s32`  \n7: `shape_element_type_is_s64`  \n8: `shape_element_type_is_u8`  \n9: `shape_element_type_is_u16`  \n10: `shape_element_type_is_u32`  \n11: `shape_element_type_is_u64`  \n12: `shape_element_type_is_f16`  \n13: `shape_element_type_is_f32`  \n14: `shape_element_type_is_f64`  \n15: `shape_element_type_is_bf16`  \n16: `shape_element_type_is_c64`  \n17: `shape_element_type_is_c128`  \n18: `shape_element_type_is_tuple`  \n19: `shape_element_type_is_opaque_type`  \n20: `shape_element_type_is_token`  \n\n// 21\u201328: Size (number of elements) for each dimension, or an upper bound on the size if the dimension is dynamic. In XLA, dimensions are numbered from 0 to N-1 for an N-dimensional array. The first element of 'shape_dimensions' is the size of dimension 0, the second element is the size of dimension 1, and so forth. Empty list indicates a scalar.  \n21: `shape_dimensions_0`  \n22: `shape_dimensions_1`  \n23: `shape_dimensions_2`  \n24: `shape_dimensions_3`  \n25: `shape_dimensions_4`  \n26: `shape_dimensions_5`  \n27: `shape_dimensions_sum`  \n28: `shape_dimensions_product`  \n\n29: `shape_tuple_shapes_size` - for tuples only, the shapes of constituent shapes in the tuple sequence  \n30: `parameter_number = K` - indicating that is is the Kth parameter to the computation, only for Parameter operation  \n\n// 31\u201336: Dimensions present for some operations that require reshaping or broadcasting, including Reshape, Reduce, ReduceWindow, and Reverse.  \n31: `dimensions_0`  \n32: `dimensions_1`  \n33: `dimensions_2`  \n34: `dimensions_3`  \n35: `dimensions_4`  \n36: `dimensions_5`  \n\n// 37\u201392: Windowing information in an operation such as convolution. The window is moved across a base area and for each position of the window a computation is performed.  \n37: `window_size_0`  \n38: `window_size_1`  \n39: `window_size_2`  \n40: `window_size_3`  \n41: `window_size_4`  \n42: `window_size_5`  \n43: `window_size_sum`  \n44: `window_size_product`  \n45: `window_stride_0`  \n46: `window_stride_1`  \n47: `window_stride_2`  \n48: `window_stride_3`  \n49: `window_stride_4`  \n50: `window_stride_5`  \n51: `window_stride_sum`  \n52: `window_stride_product`  \n53: `window_padding_low_0`  \n54: `window_padding_low_1`  \n55: `window_padding_low_2`  \n56: `window_padding_low_3`  \n57: `window_padding_low_4`  \n58: `window_padding_low_5`  \n59: `window_padding_low_sum`  \n60: `window_padding_low_product`  \n61: `window_padding_high_0`  \n62: `window_padding_high_1`  \n63: `window_padding_high_2`  \n64: `window_padding_high_3`  \n65: `window_padding_high_4`  \n66: `window_padding_high_5`  \n67: `window_padding_high_sum`  \n68: `window_padding_high_product`  \n\n// 69\u201376: Dilation factor of the sliding window. A dilation factor of 1 means no dilation. window_dilation - 1 no-op entries (\"holes\") are implicitly placed between each kernel element.  \n69: `window_window_dilation_0`  \n70: `window_window_dilation_1`  \n71: `window_window_dilation_2`  \n72: `window_window_dilation_3`  \n73: `window_window_dilation_4`  \n74: `window_window_dilation_5`  \n75: `window_window_dilation_sum`  \n76: `window_window_dilation_product`  \n\n// 77-84: Dilation factor of the base area. A dilation factor of 1 means no dilation. base_dilation - 1 no-op entries (\"holes\") are implicitly placed between each base area element.  \n77: `window_base_dilation_0`  \n78: `window_base_dilation_1`  \n79: `window_base_dilation_2`  \n80: `window_base_dilation_3`  \n81: `window_base_dilation_4`  \n82: `window_base_dilation_5`  \n83: `window_base_dilation_sum`  \n84: `window_base_dilation_product`  \n\n// 85-92: Window reversal means that this dimension was logically reversed before the operation.  \n85: `window_window_reversal_0`  \n86: `window_window_reversal_1`  \n87: `window_window_reversal_2`  \n88: `window_window_reversal_3`  \n89: `window_window_reversal_4`  \n90: `window_window_reversal_5`  \n91: `window_window_reversal_true_count`  \n92: `window_window_reversal_false_count`  \n\n// 93\u2013106: The dimension numbers used for a convolution.  \n93: `convolution_dim_numbers_input_batch_dim` - the dimension number that represents batch in the input  \n94: `convolution_dim_numbers_input_feature_dim` - the dimension number that represents features in the input  \n\n// 95\u201398: Dimension numbers for the spatial dimensions that the window moves through in the input.  \n95: `convolution_dim_numbers_input_spatial_dims_0`  \n96: `convolution_dim_numbers_input_spatial_dims_1`  \n97: `convolution_dim_numbers_input_spatial_dims_2`  \n98: `convolution_dim_numbers_input_spatial_dims_3`  \n\n99: `convolution_dim_numbers_kernel_input_feature_dim` - the dimension number that represents input features in the convolutional kernel (rhs)  \n100: `convolution_dim_numbers_kernel_output_feature_dim` - the dimension number that represents output features in the convolutional kernel (rhs)  \n\n// 101-104: Dimension numbers for the spatial dimensions that the window moves through in the kernel (rhs). window.strides(0) is the stride in the kernel_spatial_dimensions(0) dimension.  \n101: `convolution_dim_numbers_kernel_spatial_dims_0`  \n102: `convolution_dim_numbers_kernel_spatial_dims_1`  \n103: `convolution_dim_numbers_kernel_spatial_dims_2`  \n104: `convolution_dim_numbers_kernel_spatial_dims_3`  \n\n105: `convolution_dim_numbers_output_batch_dim` - the dimension number that represents batch in the output  \n106: `convolution_dim_numbers_output_feature_dim` - the dimension number that represents features in the output  \n\n107: `feature_group_count` - the number of feature groups, used for a convolution. Must be a divisor of the input feature dimension and output feature dimension. If not specified, it will use a default value of 1.  \n108: `batch_group_count` - the number of batch groups, used for a convolution.  \n\n// 109\u2013120: [begin/start, end/limit) index range and stride for a slice operation.  \n109: `slice_dims_start_0`  \n110: `slice_dims_start_1`  \n111: `slice_dims_start_sum`  \n112: `slice_dims_start_product`  \n113: `slice_dims_stride_0`  \n114: `slice_dims_stride_1`  \n115: `slice_dims_stride_sum`  \n116: `slice_dims_stride_product`  \n117: `slice_dims_limit_0`  \n118: `slice_dims_limit_1`  \n119: `slice_dims_limit_sum`  \n120: `slice_dims_limit_product`  \n\n// 121 - 124: [start, start + size) range size for a dynamic slice ('start' is specified dynamically in the second operand of the operation).  \n121: `dynamic_slice_sizes_0`  \n122: `dynamic_slice_sizes_1`  \n123: `dynamic_slice_sizes_sum`  \n124: `dynamic_slice_sizes_product`  \n\n// 125\u2013132: Padding configuration that describes the edge padding of a pad operation.  \n125: `padding_config_edge_padding_low_0`  \n126: `padding_config_edge_padding_low_1`  \n127: `padding_config_edge_padding_low_sum`  \n128: `padding_config_edge_padding_low_product`  \n129: `padding_config_edge_padding_high_0`  \n130: `padding_config_edge_padding_high_1`  \n131: `padding_config_edge_padding_high_sum`  \n132: `padding_config_edge_padding_high_product`  \n\n133: `is_stable` - whether this Sort operation should be stable  \n\n// 134\u2013139: Physical layout used to pack the tensor shape.  \n134: `layout_minor_to_major_0`  \n135: `layout_minor_to_major_1`  \n136: `layout_minor_to_major_2`  \n137: `layout_minor_to_major_3`  \n138: `layout_minor_to_major_4`  \n139: `layout_minor_to_major_5`  \n\nSuffix `_i`, where i is an integer, indicates the information for the tensor dimension i. If a tensor has N dimensions, feature values of `_i` are set to 0 if i >= N (0 padding). Suffix `_sum` is the summation of the feature values across all dimensions. Suffix `_product` is the product of the feature values across all dimensions.\n\nThe source code of the node features extractor can be found [here](https://github.com/google-research-datasets/tpu_graphs), which extracts features/attributes from HloProto defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo.proto).\n\n### Tile Config Features\nThe following describe each element at a particular index in the tile config feature vector.\n\n// 0\u20137: Tile sizes of the convolution kernel, only for a convolution operation.  \n0: `kernel_bounds_0`  \n1: `kernel_bounds_1`  \n2: `kernel_bounds_2`  \n3: `kernel_bounds_3`  \n4: `kernel_bounds_4`  \n5: `kernel_bounds_5`  \n6: `kernel_bounds_sum`  \n7: `kernel_bounds_product`  \n\n// 8\u201315: Output tile sizes.  \n8: `output_bounds_0`  \n9: `output_bounds_1`  \n10: `output_bounds_2`  \n11: `output_bounds_3`  \n12: `output_bounds_4`  \n13: `output_bounds_5`  \n14: `output_bounds_sum`  \n15: `output_bounds_product`  \n\n// 16-23: Input tile sizes.  \n16: `input_bounds_0`  \n17: `input_bounds_1`  \n18: `input_bounds_2`  \n19: `input_bounds_3`  \n20: `input_bounds_4`  \n21: `input_bounds_5`  \n22: `input_bounds_sum`  \n23: `input_bounds_product`  \n\nNote that input_bounds are usually set to 0 because they can be inferred by the compiler from output_bounds (and kernel_bounds). If a tensor has N dimensions, feature values of `_i` are set to 0 if i >= N (0 padding).\n\n### Layout Config Features\nThe following describe each element at a particular index in the per-node layout config feature vector.\n\n// 0\u20135: Physical layout of the output tensor  \n0: `output_layout_0`  \n1: `output_layout_1`  \n2: `output_layout_2`  \n3: `output_layout_3`  \n4: `output_layout_4`  \n5: `output_layout_5`  \n\n// 6-11: Physical layout of the input tensor  \n6: `input_layout_0`  \n7: `input_layout_1`  \n8: `input_layout_2`  \n9: `input_layout_3`  \n10: `input_layout_4`  \n11: `input_layout_5`  \n\n// 12-17: Physical layout of the kernel tensor, only for a convolution operation  \n12: `kernel_layout_0`  \n13: `kernel_layout_1`  \n14: `kernel_layout_2`  \n15: `kernel_layout_3`  \n16: `kernel_layout_4`  \n17: `kernel_layout_5`  \n\nIf a tensor has N dimensions, feature values of `_i` are set to -1 if i >= N (-1 padding). A layout determines the order of minor-to-major tensor dimensions. For example, the layout of {1, 0, 2, -1, -1, -1} of a 3D tensor indicates that dimension 1 is the most minor (elements of the most minor dimension are consecutive in the physical space), and dimension 2 is the most major. We also include a tensor layout of {-1, -1, -1, -1, -1, -1} to indicate the compiler to apply its default strategy of selecting the layout for that tensor.\n\n## Custom Feature Extraction\nIf you want to extract node features differently from the pre-extracted features provided in an `.npz` file, you can extract the features from a raw graph in an `.pb` file following this [instruction](https://github.com/google-research-datasets/tpu_graphs). The directory structure and filenames of `.pb` files match those of `.npz` files. In particular, the raw graph in `npz/.../xxx.npz` can be found in `pb/.../xxx.pb`. The raw graphs are only available for the layout collections.\n\n## Dataset Metadata\n- Files: 8508 files\n- Size: 7.3 GB\n- Type: npz, pb, csv\n- License: Attribution 4.0 International (CC BY 4.0)",
      "docker_challenge_path": "/data/predict-ai-model-runtime",
      "competition_description": "# Google - Fast or Slow? Predict AI Model Runtime\n\n## Overview\nAlice is an AI model developer, but some of the models her team developed run very slow. She recently discovered compiler's configurations that change the way the compiler compiles and optimizes the models, and hence make the models run faster (or slower)! Your task is to help Alice find the best configuration for each model.\n\nYour goal: Train a machine learning model based on the runtime data provided to you in the training dataset and further predict the runtime of graphs and configurations in the test dataset.\n\n## Technical Background\nA bit of technical background on an AI compiler will help you get started! An AI model can be represented as a graph, where a node is a tensor operation (e.g. matrix multiplication, convolution, etc), and an edge represents a tensor. A compilation configuration controls how the compiler transforms the graph for a specific optimization pass. In particular, Alice can control two types of configurations/optimizations:\n\n- A layout configuration controls how tensors in the graph are laid out in the physical memory, by specifying the dimension order of each input and output of an operation node.\n- A tile configuration controls the tile size of each fused subgraph.\n\nBeing able to predict an optimal configuration for a given graph will not only help Alice's team but also improve the compiler's heuristic to select the best configuration without human's intervention. This will make AI models run more efficiently, consuming less time and resources overall!",
      "evaluation_metric": "## Evaluation Metric\nAs driven by realistic requirements, we use two evaluation metrics, and average them.\n\nSpecifically, for the collection tile:xla, we use the (1-slowdown) incurred of the top-K predictions to reflect how much slower the top-K configurations predicted by the model is from the actual fastest configuration as follows:\n\n$$1 - \\left(\\frac{\\text{The best runtime of the top-k predictions}}{\\text{The best runtime of all configurations}} - 1\\right) = 2 - \\frac{\\min_{i \\in K} y_i}{\\min_{i \\in A} y_i}$$\n\nwhere K is the top-K predictions, A is all configurations of the given graph from the dataset collection, and y is the measured execution time.\n\nFor the collections layout:*, we use the Kendall Tau Correlation (a ranking metric: how well does your model-predicted ranking, correspond to the real ranking of runtimes).\n\nThe choice of metrics is justified as follows. For the tile size search space, since the number of possibilities is relatively small, one can enumerate all possibilities and invoke a model on each, then choose the best few (=5, here) configurations as suggested by the model, compile with each of them, then measure the runtime of each and commit to the best. On the other hand, for the layout:* collections, the search space is quite large. Therefore, common search strategies, such as Genetic Algorithm, Simulated Annealing, and Langevin Dynamics, need access to a fitness/utility function (which can be your model). Therefore, it is important that the model can well-preserve the order of the configurations (from fastest to slowest).",
      "dataset_description": "## Dataset Description\nOnce downloaded (e.g., to `~/data/tpugraphs`, which is the default path for training code) and unzipped, you'll get the following directory structure:\n\n- `npz/tile/xla/{train, valid, test}/*.npz`: contains the `.npz` files for the tile collection\n- `npz/layout/{nlp, xla}/{random, default}/{train, valid, test}/*.npz`: contains the `.npz` files for the layout collection\n\n### .npz Structures\n\n#### Tile .npz files\nSuppose a `.npz` file stores a graph (representing a kernel) with `n` nodes and `m` edges. In addition, suppose we compile the graph with `c` different configurations, and run each on a TPU. Crucially, the configuration is at the graph-level. Then, the `.npz` file stores the following dictionary (can be loaded with `d = dict(np.load(\"npz/tile/xla/train/<pick_1>.npz\"))`):\n\n- Key `\"node_feat\"`: contains `float32` matrix with shape `(n, 140)`. The `u`th row contains the feature vector for node `u < n` (please see Subsection \"Node Features\"). Nodes are ordered topologically.\n- Key `\"node_opcode\"`: contains `int32` vector with shape `(n,)`. The `u`th entry stores the op-code for node `u` (please see the mapping of opcode to instruction's name [here](https://github.com/google-research-datasets/tpu_graphs)).\n- Key `\"edge_index\"`: contains `int32` matrix with shape `(m, 2)`. If entry `i` is `[u, v]` (where `0 <= u, v < n`), then there is a directed edge from node `u <- v`, where `u` is a tensor operation consuming the output tensor of `v` (a reverse of a typical definition of an edge direction).\n- Key `\"config_feat\"`: contains `float32` matrix with shape `(c, 24)` with row `j` containing the (graph-level) configuration feature vector (please see Subsection \"Tile Config Features\").\n- Keys `\"config_runtime\"` and `\"config_runtime_normalizers\"`: both are `int64` vectors of length `c`. Entry `j` stores the runtime (in nanoseconds) of the given graph compiled with configuration `j` and a default configuration, respectively. Samples from the same graph may have slightly different `\"config_runtime_normalizers\"` because they are measured from different runs on multiple machines.\n\nFor the tile collection, your job is to predict the indices of the best configurations (i.e., ones leading to the smallest `d[\"config_runtime\"] / d[\"config_runtime_normalizers\"]`).\n\n#### Layout .npz files\nSuppose a `.npz` file stores a graph (representing the entire program) with `n` nodes and `m` edges. In addition, suppose we compile the graph with `c` different configurations, and run each on a TPU. Crucially, the configuration is at the node-level. Suppose that `n_c` of the `n` nodes are configurable. Then, the `.npz` file stores the following dictionary (can be loaded with, e.g., `d = dict(np.load(\"npz/layout/xla/random/train/<pick_1>.npz\"))`):\n\n- Keys `\"node_feat\"`, `\"node_opcode\"`, `\"edge_index\"`: are like above.\n- Key `\"node_config_ids\"`: contains `int32` vector with shape `(n_c,)` and every entry is in `{0, 1, ..., n - 1}` i.e. indicating the indices of the configurable nodes. For these nodes, they can have an additional feature vector that instructs the compiler (described next).\n- Key `\"node_config_feat\"`: contains `float32` tensor with shape `(c, n_c, 18)`. Entry `[j, k]` gives an 18-dimensional vector describing the configuration features for node `d[\"node_config_ids\"][k]` for the `j`th run (please see Subsection \"Layout Config Features\").\n- Key `\"config_runtime\"`: contains `int32` vector with shape `(c,)` where the `j`th entry contains the runtime of the `j`th run (i.e., when nodes are configured with `d[\"node_config_feat\"][j]`).\n\nFor the layout collections, your job is to predict the order of the indices from best-to-worse configurations (i.e., ones leading to the smallest `d[\"config_runtime\"]`). We do not have to use runtime normalizers for this task because the runtime variation at the entire program level is very small.\n\nOptionally, you may access key `\"node_splits\"`, which is a variable-length list of node IDs that are the starting of HLO computations in the graph (similar to functions in a program). Essentially, nodes `d[\"node_splits\"][i]` to `d[\"node_splits\"][i+1] - 1` belongs to the same computation. If you want to partition the graph into multiple segments, this information may be useful, e.g., putting nodes from the same computation in the same partition. However, you may compute your own partitioning (e.g., using METIS) as well.\n\n## Feature Descriptions\n\n### Node Features\nTo extract a node feature vector, we either copy values from various fields in an XLA's HLO instruction (a node in an HLO graph) as they are, or convert categorical values using one-hot encoding. To convert an unbounded list of numbers (e.g. tensor shape) to a fixed-size vector, we truncate the list to six elements and include the summation and/or product of all elements in the list (e.g., the product of dimension sizes represents the volume of the tensor). In our dataset, none of the tensors has more than six dimensions.\n\nThe following describe each element at a particular index in the node feature vector:\n\n0: `is_root` - whether this node is the output  \n1: `element_size_in_bits` - deprecated, always 0\n\n// 2\u201320: One hot vector of shape_element_type  \n2: `shape_element_type_is_invalid_type`  \n3: `shape_element_type_is_pred`  \n4: `shape_element_type_is_s8`  \n5: `shape_element_type_is_s16`  \n6: `shape_element_type_is_s32`  \n7: `shape_element_type_is_s64`  \n8: `shape_element_type_is_u8`  \n9: `shape_element_type_is_u16`  \n10: `shape_element_type_is_u32`  \n11: `shape_element_type_is_u64`  \n12: `shape_element_type_is_f16`  \n13: `shape_element_type_is_f32`  \n14: `shape_element_type_is_f64`  \n15: `shape_element_type_is_bf16`  \n16: `shape_element_type_is_c64`  \n17: `shape_element_type_is_c128`  \n18: `shape_element_type_is_tuple`  \n19: `shape_element_type_is_opaque_type`  \n20: `shape_element_type_is_token`  \n\n// 21\u201328: Size (number of elements) for each dimension, or an upper bound on the size if the dimension is dynamic. In XLA, dimensions are numbered from 0 to N-1 for an N-dimensional array. The first element of 'shape_dimensions' is the size of dimension 0, the second element is the size of dimension 1, and so forth. Empty list indicates a scalar.  \n21: `shape_dimensions_0`  \n22: `shape_dimensions_1`  \n23: `shape_dimensions_2`  \n24: `shape_dimensions_3`  \n25: `shape_dimensions_4`  \n26: `shape_dimensions_5`  \n27: `shape_dimensions_sum`  \n28: `shape_dimensions_product`  \n\n29: `shape_tuple_shapes_size` - for tuples only, the shapes of constituent shapes in the tuple sequence  \n30: `parameter_number = K` - indicating that is is the Kth parameter to the computation, only for Parameter operation\n\n// 31\u201336: Dimensions present for some operations that require reshaping or broadcasting, including Reshape, Reduce, ReduceWindow, and Reverse.  \n31: `dimensions_0`  \n32: `dimensions_1`  \n33: `dimensions_2`  \n34: `dimensions_3`  \n35: `dimensions_4`  \n36: `dimensions_5`  \n\n// 37\u201392: Windowing information in an operation such as convolution. The window is moved across a base area and for each position of the window a computation is performed.  \n37: `window_size_0`  \n38: `window_size_1`  \n39: `window_size_2`  \n40: `window_size_3`  \n41: `window_size_4`  \n42: `window_size_5`  \n43: `window_size_sum`  \n44: `window_size_product`  \n45: `window_stride_0`  \n46: `window_stride_1`  \n47: `window_stride_2`  \n48: `window_stride_3`  \n49: `window_stride_4`  \n50: `window_stride_5`  \n51: `window_stride_sum`  \n52: `window_stride_product`  \n53: `window_padding_low_0`  \n54: `window_padding_low_1`  \n55: `window_padding_low_2`  \n56: `window_padding_low_3`  \n57: `window_padding_low_4`  \n58: `window_padding_low_5`  \n59: `window_padding_low_sum`  \n60: `window_padding_low_product`  \n61: `window_padding_high_0`  \n62: `window_padding_high_1`  \n63: `window_padding_high_2`  \n64: `window_padding_high_3`  \n65: `window_padding_high_4`  \n66: `window_padding_high_5`  \n67: `window_padding_high_sum`  \n68: `window_padding_high_product`  \n\n// 69\u201376: Dilation factor of the sliding window. A dilation factor of 1 means no dilation. window_dilation - 1 no-op entries (\"holes\") are implicitly placed between each kernel element.  \n69: `window_window_dilation_0`  \n70: `window_window_dilation_1`  \n71: `window_window_dilation_2`  \n72: `window_window_dilation_3`  \n73: `window_window_dilation_4`  \n74: `window_window_dilation_5`  \n75: `window_window_dilation_sum`  \n76: `window_window_dilation_product`  \n\n// 77-84: Dilation factor of the base area. A dilation factor of 1 means no dilation. base_dilation - 1 no-op entries (\"holes\") are implicitly placed between each base area element.  \n77: `window_base_dilation_0`  \n78: `window_base_dilation_1`  \n79: `window_base_dilation_2`  \n80: `window_base_dilation_3`  \n81: `window_base_dilation_4`  \n82: `window_base_dilation_5`  \n83: `window_base_dilation_sum`  \n84: `window_base_dilation_product`  \n\n// 85-92: Window reversal means that this dimension was logically reversed before the operation.  \n85: `window_window_reversal_0`  \n86: `window_window_reversal_1`  \n87: `window_window_reversal_2`  \n88: `window_window_reversal_3`  \n89: `window_window_reversal_4`  \n90: `window_window_reversal_true_count`  \n91: `window_window_reversal_false_count`  \n\n// 93\u2013106: The dimension numbers used for a convolution.  \n93: `convolution_dim_numbers_input_batch_dim` - the dimension number that represents batch in the input  \n94: `convolution_dim_numbers_input_feature_dim` - the dimension number that represents features in the input  \n\n// 95\u201398: Dimension numbers for the spatial dimensions that the window moves through in the input.  \n95: `convolution_dim_numbers_input_spatial_dims_0`  \n96: `convolution_dim_numbers_input_spatial_dims_1`  \n97: `convolution_dim_numbers_input_spatial_dims_2`  \n98: `convolution_dim_numbers_input_spatial_dims_3`  \n\n99: `convolution_dim_numbers_kernel_input_feature_dim` - the dimension number that represents input features in the convolutional kernel (rhs)  \n100: `convolution_dim_numbers_kernel_output_feature_dim` - the dimension number that represents output features in the convolutional kernel (rhs)  \n\n// 101-104: Dimension numbers for the spatial dimensions that the window moves through in the kernel (rhs). window.strides(0) is the stride in the kernel_spatial_dimensions(0) dimension.  \n101: `convolution_dim_numbers_kernel_spatial_dims_0`  \n102: `convolution_dim_numbers_kernel_spatial_dims_1`  \n103: `convolution_dim_numbers_kernel_spatial_dims_2`  \n104: `convolution_dim_numbers_kernel_spatial_dims_3`  \n\n105: `convolution_dim_numbers_output_batch_dim` - the dimension number that represents batch in the output  \n106: `convolution_dim_numbers_output_feature_dim` - the dimension number that represents features in the output  \n\n107: `feature_group_count` - the number of feature groups, used for a convolution. Must be a divisor of the input feature dimension and output feature dimension. If not specified, it will use a default value of 1.  \n108: `batch_group_count` - the number of batch groups, used for a convolution.  \n\n// 109\u2013120: [begin/start, end/limit) index range and stride for a slice operation.  \n109: `slice_dims_start_0`  \n110: `slice_dims_start_1`  \n111: `slice_dims_start_sum`  \n112: `slice_dims_start_product`  \n113: `slice_dims_stride_0`  \n114: `slice_dims_stride_1`  \n115: `slice_dims_stride_sum`  \n116: `slice_dims_stride_product`  \n117: `slice_dims_limit_0`  \n118: `slice_dims_limit_1`  \n119: `slice_dims_limit_sum`  \n120: `slice_dims_limit_product`  \n\n// 121 - 124: [start, start + size) range size for a dynamic slice ('start' is specified dynamically in the second operand of the operation).  \n121: `dynamic_slice_sizes_0`  \n122: `dynamic_slice_sizes_1`  \n123: `dynamic_slice_sizes_sum`  \n124: `dynamic_slice_sizes_product`  \n\n// 125\u2013132: Padding configuration that describes the edge padding of a pad operation.  \n125: `padding_config_edge_padding_low_0`  \n126: `padding_config_edge_padding_low_1`  \n127: `padding_config_edge_padding_low_sum`  \n128: `padding_config_edge_padding_low_product`  \n129: `padding_config_edge_padding_high_0`  \n130: `padding_config_edge_padding_high_1`  \n131: `padding_config_edge_padding_high_sum`  \n132: `padding_config_edge_padding_high_product`  \n\n133: `is_stable` - whether this Sort operation should be stable  \n\n// 134\u2013139: Physical layout used to pack the tensor shape.  \n134: `layout_minor_to_major_0`  \n135: `layout_minor_to_major_1`  \n136: `layout_minor_to_major_2`  \n137: `layout_minor_to_major_3`  \n138: `layout_minor_to_major_4`  \n139: `layout_minor_to_major_5`  \n\nSuffix `_i`, where i is an integer, indicates the information for the tensor dimension i. If a tensor has N dimensions, feature values of `_i` are set to 0 if i >= N (0 padding). Suffix `_sum` is the summation of the feature values across all dimensions. Suffix `_product` is the product of the feature values across all dimensions.\n\nThe source code of the node features extractor can be found [here](https://github.com/google-research-datasets/tpu_graphs), which extracts features/attributes from HloProto defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/service/hlo.proto).\n\n### Tile Config Features\nThe following describe each element at a particular index in the tile config feature vector.\n\n// 0\u20137: Tile sizes of the convolution kernel, only for a convolution operation.  \n0: `kernel_bounds_0`  \n1: `kernel_bounds_1`  \n2: `kernel_bounds_2`  \n3: `kernel_bounds_3`  \n4: `kernel_bounds_4`  \n5: `kernel_bounds_5`  \n6: `kernel_bounds_sum`  \n7: `kernel_bounds_product`  \n\n// 8\u201315: Output tile sizes.  \n8: `output_bounds_0`  \n9: `output_bounds_1`  \n10: `output_bounds_2`  \n11: `output_bounds_3`  \n12: `output_bounds_4`  \n13: `output_bounds_5`  \n14: `output_bounds_sum`  \n15: `output_bounds_product`  \n\n// 16-23: Input tile sizes.  \n16: `input_bounds_0`  \n17: `input_bounds_1`  \n18: `input_bounds_2`  \n19: `input_bounds_3`  \n20: `input_bounds_4`  \n21: `input_bounds_5`  \n22: `input_bounds_sum`  \n23: `input_bounds_product`  \n\nNote that input_bounds are usually set to 0 because they can be inferred by the compiler from output_bounds (and kernel_bounds). If a tensor has N dimensions, feature values of `_i` are set to 0 if i >= N (0 padding).\n\n### Layout Config Features\nThe following describe each element at a particular index in the per-node layout config feature vector.\n\n// 0\u20135: Physical layout of the output tensor  \n0: `output_layout_0`  \n1: `output_layout_1`  \n2: `output_layout_2`  \n3: `output_layout_3`  \n4: `output_layout_4`  \n5: `output_layout_5`  \n\n// 6-11: Physical layout of the input tensor  \n6: `input_layout_0`  \n7: `input_layout_1`  \n8: `input_layout_2`  \n9: `input_layout_3`  \n10: `input_layout_4`  \n11: `input_layout_5`  \n\n// 12-17: Physical layout of the kernel tensor, only for a convolution operation  \n12: `kernel_layout_0`  \n13: `kernel_layout_1`  \n14: `kernel_layout_2`  \n15: `kernel_layout_3`  \n16: `kernel_layout_4`  \n17: `kernel_layout_5`  \n\nIf a tensor has N dimensions, feature values of `_i` are set to -1 if i >= N (-1 padding). A layout determines the order of minor-to-major tensor dimensions. For example, the layout of {1, 0, 2, -1, -1, -1} of a 3D tensor indicates that dimension 1 is the most minor (elements of the most minor dimension are consecutive in the physical space), and dimension 2 is the most major. We also include a tensor layout of {-1, -1, -1, -1, -1, -1} to indicate the compiler to apply its default strategy of selecting the layout for that tensor.\n\n## Custom Feature Extraction\nIf you want to extract node features differently from the pre-extracted features provided in an `.npz` file, you can extract the features from a raw graph in an `.pb` file following this [instruction](https://github.com/google-research-datasets/tpu_graphs). The directory structure and filenames of `.pb` files match those of `.npz` files. In particular, the raw graph in `npz/.../xxx.npz` can be found in `pb/.../xxx.pb`. The raw graphs are only available for the layout collections.\n\n## Dataset Metadata\n- Files: 8508 files\n- Size: 7.3 GB\n- Type: npz, pb, csv\n- License: Attribution 4.0 International (CC BY 4.0)",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "ranking",
          "graph",
          "gnn",
          "compiler-optimization",
          "kendall-tau"
        ]
      }
    },
    {
      "challenge_name": "recruit-restaurant-visitor-forecasting",
      "description": "Challenge description:\nRecruit Restaurant Visitor Forecasting\nPredict how many future visitors a restaurant will receive\n\nDescription\nRunning a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business.\nOne common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data.\nRecruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software).\nIn this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.\n\nEvaluation\nSubmissions are evaluated on the root mean squared logarithmic error.\nThe RMSLE is calculated as\n$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 },$$\nwhere:\n\\\\(n\\\\) is the total number of observations\n\\\\(p_i\\\\) is your prediction of visitors\n\\\\(a_i\\\\) is the actual number of visitors\n\\\\(\\log(x)\\\\) is the natural logarithm of \\\\(x\\\\)\n\nSubmission File\nFor every store and date combination in the test set, submission files should contain two columns: id and visitors. The id is formed by concatenating the air_store_id and visit_date with an underscore. The file should contain a header and have the following format:\nid,visitors\nair_00a91d42b08b08d9_2017-04-23,0\nair_00a91d42b08b08d9_2017-04-24,0\nair_00a91d42b08b08d9_2017-04-25,0\netc.\n\nPrizes\n1st place - $12,000\n2nd place - $8,000\n3rd place - $5,000\n\nTimeline\nJanuary 30, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nJanuary 30, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nFebruary 6, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nCitation\nAddison Howard, Haruka Yui, Mark McDonald, and Will Cukierski. Recruit Restaurant Visitor Forecasting. https://kaggle.com/competitions/recruit-restaurant-visitor-forecasting, 2017. Kaggle.\n\nCompetition Host\nRecruit Holdings\n\nPrizes & Awards\n$25,000\n\nAwards\nPoints & Medals\n\nParticipation\n9,125 Entrants\n2,426 Participants\n2,148 Teams\n39,869 Submissions\n\nTags\nRoot Mean Squared Logarithmic Error\n\nData description:\nRecruit Restaurant Visitor Forecasting\nPredict how many future visitors a restaurant will receive\n\nIn this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites:\nHot Pepper Gourmet (hpg): similar to Yelp, here users can search restaurants and also make a reservation online\nAirREGI / Restaurant Board (air): similar to Square, a reservation control and cash register system\n\nYou must use the reservations, visits, and other information from these sites to forecast future restaurant visitor totals on a given date. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the \"Golden Week.\"\n\nThere are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed.\n\nFile Descriptions\nThis is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants.\n\nair_reserve.csv\nThis file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.\n- air_store_id: the restaurant's id in the air system\n- visit_datetime: the time of the reservation\n- reserve_datetime: the time the reservation was made\n- reserve_visitors: the number of visitors for that reservation\n\nhpg_reserve.csv\nThis file contains reservations made in the hpg system.\n- hpg_store_id: the restaurant's id in the hpg system\n- visit_datetime: the time of the reservation\n- reserve_datetime: the time the reservation was made\n- reserve_visitors: the number of visitors for that reservation\n\nair_store_info.csv\nThis file contains information about select air restaurants.\n- air_store_id\n- air_genre_name\n- air_area_name\n- latitude\n- longitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nhpg_store_info.csv\nThis file contains information about select hpg restaurants.\n- hpg_store_id\n- hpg_genre_name\n- hpg_area_name\n- latitude\n- longitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nstore_id_relation.csv\nThis file allows you to join select restaurants that have both the air and hpg system.\n- hpg_store_id\n- air_store_id\n\nair_visit_data.csv\nThis file contains historical visit data for the air restaurants.\n- air_store_id\n- visit_date: the date\n- visitors: the number of visitors to the restaurant on the date\n\nsample_submission.csv\nThis file shows a submission in the correct format, including the days for which you must forecast.\n- id: the id is formed by concatenating the air_store_id and visit_date with an underscore\n- visitors: the number of visitors forecasted for the store and date combination\n\ndate_info.csv\nThis file gives basic information about the calendar dates in the dataset.\n- calendar_date\n- day_of_week\n- holiday_flg: is the day a holiday in Japan\n\nFiles\n8 files\nSize: 27.3 MB\nType: zip\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/recruit-restaurant-visitor-forecasting",
      "competition_description": "Challenge description:\nRecruit Restaurant Visitor Forecasting\nPredict how many future visitors a restaurant will receive\n\nDescription\nRunning a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business.\nOne common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members. This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data.\nRecruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software).\nIn this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.",
      "evaluation_metric": "Evaluation\nSubmissions are evaluated on the root mean squared logarithmic error.\nThe RMSLE is calculated as\n$$\\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 },$$\nwhere:\n\\\\(n\\\\) is the total number of observations\n\\\\(p_i\\\\) is your prediction of visitors\n\\\\(a_i\\\\) is the actual number of visitors\n\\\\(\\\\log(x)\\\\) is the natural logarithm of \\\\(x\\\\)",
      "dataset_description": "Data description:\nRecruit Restaurant Visitor Forecasting\nPredict how many future visitors a restaurant will receive\n\nIn this competition, you are provided a time-series forecasting problem centered around restaurant visitors. The data comes from two separate sites:\nHot Pepper Gourmet (hpg): similar to Yelp, here users can search restaurants and also make a reservation online\nAirREGI / Restaurant Board (air): similar to Square, a reservation control and cash register system\n\nYou must use the reservations, visits, and other information from these sites to forecast future restaurant visitor totals on a given date. The training data covers the dates from 2016 until April 2017. The test set covers the last week of April and May of 2017. The test set is split based on time (the public fold coming first, the private fold following the public) and covers a chosen subset of the air restaurants. Note that the test set intentionally spans a holiday week in Japan called the \"Golden Week.\"\n\nThere are days in the test set where the restaurant were closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants were closed.\n\nFile Descriptions\nThis is a relational dataset from two systems. Each file is prefaced with the source (either air_ or hpg_) to indicate its origin. Each restaurant has a unique air_store_id and hpg_store_id. Note that not all restaurants are covered by both systems, and that you have been provided data beyond the restaurants for which you must forecast. Latitudes and Longitudes are not exact to discourage de-identification of restaurants.\n\nair_reserve.csv\nThis file contains reservations made in the air system. Note that the reserve_datetime indicates the time when the reservation was created, whereas the visit_datetime is the time in the future where the visit will occur.\n- air_store_id: the restaurant's id in the air system\n- visit_datetime: the time of the reservation\n- reserve_datetime: the time the reservation was made\n- reserve_visitors: the number of visitors for that reservation\n\nhpg_reserve.csv\nThis file contains reservations made in the hpg system.\n- hpg_store_id: the restaurant's id in the hpg system\n- visit_datetime: the time of the reservation\n- reserve_datetime: the time the reservation was made\n- reserve_visitors: the number of visitors for that reservation\n\nair_store_info.csv\nThis file contains information about select air restaurants.\n- air_store_id\n- air_genre_name\n- air_area_name\n- latitude\n- longitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nhpg_store_info.csv\nThis file contains information about select hpg restaurants.\n- hpg_store_id\n- hpg_genre_name\n- hpg_area_name\n- latitude\n- longitude\nNote: latitude and longitude are the latitude and longitude of the area to which the store belongs\n\nstore_id_relation.csv\nThis file allows you to join select restaurants that have both the air and hpg system.\n- hpg_store_id\n- air_store_id\n\nair_visit_data.csv\nThis file contains historical visit data for the air restaurants.\n- air_store_id\n- visit_date: the date\n- visitors: the number of visitors to the restaurant on the date\n\nsample_submission.csv\nThis file shows a submission in the correct format, including the days for which you must forecast.\n- id: the id is formed by concatenating the air_store_id and visit_date with an underscore\n- visitors: the number of visitors forecasted for the store and date combination\n\ndate_info.csv\nThis file gives basic information about the calendar dates in the dataset.\n- calendar_date\n- day_of_week\n- holiday_flg: is the day a holiday in Japan\n\nFiles\n8 files\nSize: 27.3 MB\nType: zip\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "forecasting",
          "time_series",
          "feature_engineering",
          "hospitality",
          "rmsle"
        ]
      }
    },
    {
      "challenge_name": "rsna-pneumonia-detection-challenge",
      "description": "Challenge description:\n# RSNA Pneumonia Detection Challenge\n\nIn this competition, you're challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.\n\n## Background and Importance\n\nPneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.\n\nWhile common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.\n\nCXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.\n\nTo improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle's machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.\n\nThe RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.\n\nChallenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.\n\n## Acknowledgements\n\nThank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].\n\nNIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\n\nOriginal source files and documents\n\nAlso, a big thank you to the competition organizers!\n\n## References\n\nRui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables. Table 27. Available from: www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf\n\nDeaths: Final Data for 2015. Supplemental Tables. Tables I-21, I-22. Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf\n\nFranquet T. Imaging of community-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print). PMID 30036297\n\nKelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148\n\nWang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\n\n## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$\n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.4 to 0.75 with a step size of 0.05: (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value $t$, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object.\n\nImportant note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.\n\nThe average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nIn your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.\n\nNOTE: In nearly all cases confidence will have no impact on scoring. It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases. None of these edge cases are known to exist in the data set. If you do not wish to use or calculate confidence you can use a placeholder value - like 1.0 - to indicate that no particular order applies to the evaluation of your submission boxes.\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n\n### Intersection over Union (IoU)\n\nIntersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects). It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.\n\nIt can be visualized as the following:\n\nThe two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together. IoU would be low - and would likely not count as a \"hit\" at higher IoU thresholds.\n\n## Submission File\n\nThe submission format requires a space delimited set of bounding boxes. For example:\n\n```\n0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\n```\n\nindicates that image 0004cfab-14fd-4e49-80ba-63a80b6bddd6 has a bounding box with a confidence of 0.5, at x = 0 and y = 0, with a width and height of 100.\n\nThe file should contain a header and have the following format. Each row in your submission should contain ALL bounding boxes for a given image.\n\n```\npatientId,PredictionString\n0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\n00313ee0-9eaa-42f4-b0ab-c148ed3241cd,\n00322d4d-1c29-4943-afc9-b6754be640eb,0.8 10 10 50 50 0.75 100 100 5 5\netc...\n```\n\n## Timeline\n\n- October 17, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\n- October 17, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\n- October 24, 2018 - Stage 1 ends & Model upload deadline*.\n- October 25, 2018 - Stage 2 begins. New test set uploaded.\n- October 31, 2018 - Stage 2 ends & Final submission deadline.\n- November 9, 2018 - Solutions & Other Winners Obligations due from winners.\n- November 25-30, 2018 - RSNA 2018 Conference in Chicago, IL\n\n* In order to be eligible for Stage 2, each team's Stage 1 submission must include the model uploaded, via Team -> Your Model, per the Competition Rules. This model should match that which was used to generate the 1 final submission selected for scoring. Be aware that if you do not select a final submission (via 'My Submissions'), the platform will auto-select your best-scoring model on the Stage 1 public leaderboard. The deadline for model upload is firmly the end of Stage 1.\n\nThis requirement is in place for the host team to verify the performance of the uploaded models matches the Stage 2 submission file. Compliance with the above will be verified by the host team.\n\nSubmitters who fail to upload their model by the Stage 1 deadline, or are found not to be in compliance, may be disqualified from Stage 2 and removed from the final leaderboard.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n- 1st Place - $12,000\n- 2nd Place - $7,000\n- 3rd Place - $4,000\n- 4th Place - 10th Places - $1,000 each\n\nBecause this competition is being hosted in coordination with the Radiological Society of North America (RSNA\u00ae) Annual Meeting, winners will be invited and strongly encouraged to attend the conference with waived fees, contingent on review of solution and fulfillment of winners' obligations.\n\nNote that in addition to the standard Kaggle Winners' Obligations (open-source licensing requirements, solution packaging/delivery, presentation to host), the host team also asks that you create a short video (under 5 minutes) summarizing your solution.\n\n## Getting Started\n\nHere are some resources to help you get started:\n\n- Starter Kaggle Kernel\n- iPynb Starter Code created by host team. Also, same starter code on Google's Colab Tool.\n- We'll also be distributing Google Cloud Platform Credits for the first 200 participants to submit the form before September 14th. View the Data page for the URL to the request form, accept the competition's rules, and make a submission to qualify.\n\n## Acknowledgements\n\nKaggle and RSNA would like to recognize the radiologists from the Society of Thoracic Radiology and RSNA who contributed the considerable effort required to annotate the datasets in preparation for the challenge:\n\n**Society of Thoracic Radiology**\n- Judith Amorosa, MD - Rutgers Robert Wood Johnson Medical School\n- Veronica Arteaga, MD - University of Arizona *\n- Maya Galperin-Aizenberg, MD - University of Pennsylvania\n- Ritu Gill, MD - Beth Israel Deaconess Medical Center *\n- Myrna Godoy, MD, PhD - MD Anderson Cancer Center *\n- Stephen Hobbs, MD - University of Kentucky *\n- Jean Jeudy, MD - University of Maryland *\n- Archana Laroia, MD - University of Iowa *\n- Palmi Shah, MD - Rush University *\n- Dharshan Vummidi, MD - University of Michigan *\n- Carol Wu, MD - MD Anderson Cancer Center *\n- Kavitha Yaddanapudi, MD - Stony Brook School of Medicine *\n\n**Radiological Society of North America**\n- Tessa Cook, MD, PhD - University of Pennsylvania *\n- Safwan Halabi, MD - Stanford University *\n- Marc Kohli, MD - University of California - San Francisco *\n- Luciano M Prevedello, MD, MPH - The Ohio State University *\n- Arjun Sharma, MD - Amita Health *\n- George Shih, MD - Weill Cornell Medicine *\n\n* 1,000 cases or more\n\nSpecial thanks to Anouk Stein, MD of MD.ai, who contributed the annotation tools used in creating the challenge datasets, helped to organize the team of annotators and provided extensive consulting on the annotation process.\n\nSpecial thanks to Jayashree Kalpathy-Cramer, PhD, Massachusetts General Hospital; Peter Chang, MD and John Mongan, MD, PhD, University of California - San Francisco; and George Shih, MD, Weill Cornell Medicine for their work in validating the evaluation metric used in the challenge.\n\nFinally, thanks to the members of the RSNA Machine Learning Steering and Machine Learning Data Standards subcommittees, who were responsible for designing, organizing and conducting the challenge in collaboration with Kaggle:\n- Katherine P. Andriole, PhD - MGH & BWH Center for Clinical Data Science\n- Falgun H. Chokshi, MD - Emory University\n- Brad Erickson, MD - Mayo Clinic\n- Adam Flanders, MD - Thomas Jefferson University\n- Safwan Halabi, MD - Stanford University\n- Jayashree Kalpathy-Cramer, PhD - Massachusetts General Hospital\n- Marc Kohli, University of California, MD - San Francisco\n- Luciano Prevedello, MD, MPH - The Ohio State University\n- George Shih, MD - Weill Cornell Medicine\n- Carol Wu, MD - MD Anderson Cancer Center\n\n## Citation\n\nAnouk Stein, MD, Carol Wu, Chris Carr, George Shih, Jamie Dulkowski, kalpathy, Leon Chen, Luciano Prevedello, Marc Kohli, MD, Mark McDonald, Peter, Phil Culliton, Safwan Halabi MD, and Tian Xia. RSNA Pneumonia Detection Challenge. https://kaggle.com/competitions/rsna-pneumonia-detection-challenge, 2018. Kaggle.\n\nData description:\nRSNA Pneumonia Detection Challenge\nCan you build an algorithm that automatically detects potential pneumonia cases?\n\nSTAGE 2 UPDATE\nNote that new files are available to download! The training set now contains both the train and test set from stage 1. The test set is comprised of new, unseen images. The metric and file formats remain the same, but you'll now be making predictions using the updated train and test sets. We have an FAQ about two-stage competitions that provides some context for how this all works. Please give it a read.\nGood luck!\n\nWhat files do I need?\nThis is a two-stage challenge. You will need the images for the current stage - provided as stage_2_train_images.zip and stage_2_test_images.zip. You will also need the training data - stage_2_train_labels.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. The file stage_2_detailed_class_info.csv contains detailed information about the positive and negative classes in the training set, and may be used to build more nuanced models.\n\nWhat should I expect the data format to be?\nThe training data is provided as a set of patientIds and bounding boxes. Bounding boxes are defined as follows: x-min y-min width height\nThere is also a binary target column, Target, indicating pneumonia or non-pneumonia.\nThere may be multiple rows per patientId.\n\nDICOM Images\nAll provided images are in DICOM format.\n\nWhat am I predicting?\nIn this challenge competitors are predicting whether pneumonia exists in a given image. They do so by predicting bounding boxes around areas of the lung. Samples without bounding boxes are negative and contain no definitive evidence of pneumonia. Samples with bounding boxes indicate evidence of pneumonia.\nWhen making predictions, competitors should predict as many bounding boxes as they feel are necessary, in the format: confidence x-min y-min width height\nThere should be only ONE predicted row per image. This row may include multiple bounding boxes.\nA properly formatted row may look like any of the following.\nFor patientIds with no predicted pneumonia / bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,\nFor patientIds with a single predicted bounding box: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\nFor patientIds with multiple predicted bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 0.5 0 0 100 100, etc.\n\nFile descriptions\nstage_2_train.csv - the training set. Contains patientIds and bounding box / target information.\nstage_2_sample_submission.csv - a sample submission file in the correct format. Contains patientIds for the test set. Note that the sample submission contains one box per image, but there is no limit to the number of bounding boxes that can be assigned to a given image.\nstage_2_detailed_class_info.csv - provides detailed information about the type of positive or negative class for each image.\n\nData fields\npatientId - A patientId. Each patientId corresponds to a unique image.\nx - the upper-left x coordinate of the bounding box.\ny - the upper-left y coordinate of the bounding box.\nwidth - the width of the bounding box.\nheight - the height of the bounding box.\nTarget - the binary Target, indicating whether this sample has evidence of pneumonia.\n\nFiles\n29687 files\nSize 3.96 GB\nType dcm, csv, txt\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/rsna-pneumonia-detection-challenge",
      "competition_description": "Challenge description:\n# RSNA Pneumonia Detection Challenge\n\nIn this competition, you're challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.\n\n## Background and Importance\n\nPneumonia accounts for over 15% of all deaths of children under 5 years old internationally. In 2015, 920,000 children under the age of 5 died from the disease. In the United States, pneumonia accounts for over 500,000 visits to emergency departments [1] and over 50,000 deaths in 2015 [2], keeping the ailment on the list of top 10 causes of death in the country.\n\nWhile common, accurately diagnosing pneumonia is a tall order. It requires review of a chest radiograph (CXR) by highly trained specialists and confirmation through clinical history, vital signs and laboratory exams. Pneumonia usually manifests as an area or areas of increased opacity [3] on CXR. However, the diagnosis of pneumonia on CXR is complicated because of a number of other conditions in the lungs such as fluid overload (pulmonary edema), bleeding, volume loss (atelectasis or collapse), lung cancer, or post-radiation or surgical changes. Outside of the lungs, fluid in the pleural space (pleural effusion) also appears as increased opacity on CXR. When available, comparison of CXRs of the patient taken at different time points and correlation with clinical symptoms and history are helpful in making the diagnosis.\n\nCXRs are the most commonly performed diagnostic imaging study. A number of factors such as positioning of the patient and depth of inspiration can alter the appearance of the CXR [4], complicating interpretation further. In addition, clinicians are faced with reading high volumes of images every shift.\n\nTo improve the efficiency and reach of diagnostic services, the Radiological Society of North America (RSNA\u00ae) has reached out to Kaggle's machine learning community and collaborated with the US National Institutes of Health, The Society of Thoracic Radiology, and MD.ai to develop a rich dataset for this challenge.\n\nThe RSNA is an international society of radiologists, medical physicists and other medical professionals with more than 54,000 members from 146 countries across the globe. They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.\n\nChallenge participants may be invited to present their AI models and methodologies during an award ceremony at the RSNA Annual Meeting which will be held in Chicago, Illinois, USA, from November 25-30, 2018.\n\n## Acknowledgements\n\nThank you to the National Institutes of Health Clinical Center for publicly providing the Chest X-Ray dataset [5].\n\nNIH News release: NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community\n\nOriginal source files and documents\n\nAlso, a big thank you to the competition organizers!\n\n## References\n\nRui P, Kang K. National Ambulatory Medical Care Survey: 2015 Emergency Department Summary Tables. Table 27. Available from: www.cdc.gov/nchs/data/nhamcs/web_tables/2015_ed_web_tables.pdf\n\nDeaths: Final Data for 2015. Supplemental Tables. Tables I-21, I-22. Available from: www.cdc.gov/nchs/data/nvsr/nvsr66/nvsr66_06_tables.pdf\n\nFranquet T. Imaging of community-acquired pneumonia. J Thorac Imaging 2018 (epub ahead of print). PMID 30036297\n\nKelly B. The Chest Radiograph. Ulster Med J 2012;81(3):143-148\n\nWang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017, http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf\n",
      "evaluation_metric": "## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a set of predicted bounding boxes and ground truth bounding boxes is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$ \n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.4 to 0.75 with a step size of 0.05: (0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value $t$, a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$ \n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object.\n\nImportant note: if there are no ground truth objects at all for a given image, ANY number of predictions (false positives) will result in the image receiving a score of zero, and being included in the mean average precision.\n\nThe average precision of a single image is calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$ \n\nIn your submission, you are also asked to provide a confidence level for each bounding box. Bounding boxes will be evaluated in order of their confidence levels in the above process. This means that bounding boxes with higher confidence will be checked first for matches against solutions, which determines what boxes are considered true and false positives.\n\nNOTE: In nearly all cases confidence will have no impact on scoring. It exists primarily to allow for submission boxes to be evaluated in a particular order to resolve extreme edge cases. None of these edge cases are known to exist in the data set. If you do not wish to use or calculate confidence you can use a placeholder value - like 1.0 - to indicate that no particular order applies to the evaluation of your submission boxes.\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n\n### Intersection over Union (IoU)\n\nIntersection over Union is a measure of the magnitude of overlap between two bounding boxes (or, in the more general case, two objects). It calculates the size of the overlap between two objects, divided by the total area of the two objects combined.\n\nIt can be visualized as the following:\n\nThe two boxes in the visualization overlap, but the area of the overlap is insubstantial compared with the area taken up by both objects together. IoU would be low - and would likely not count as a \"hit\" at higher IoU thresholds.\n",
      "dataset_description": "Data description:\nRSNA Pneumonia Detection Challenge\nCan you build an algorithm that automatically detects potential pneumonia cases?\n\nSTAGE 2 UPDATE\nNote that new files are available to download! The training set now contains both the train and test set from stage 1. The test set is comprised of new, unseen images. The metric and file formats remain the same, but you'll now be making predictions using the updated train and test sets. We have an FAQ about two-stage competitions that provides some context for how this all works. Please give it a read.\nGood luck!\n\nWhat files do I need?\nThis is a two-stage challenge. You will need the images for the current stage - provided as stage_2_train_images.zip and stage_2_test_images.zip. You will also need the training data - stage_2_train_labels.csv - and the sample submission stage_2_sample_submission.csv, which provides the IDs for the test set, as well as a sample of what your submission should look like. The file stage_2_detailed_class_info.csv contains detailed information about the positive and negative classes in the training set, and may be used to build more nuanced models.\n\nWhat should I expect the data format to be?\nThe training data is provided as a set of patientIds and bounding boxes. Bounding boxes are defined as follows: x-min y-min width height\nThere is also a binary target column, Target, indicating pneumonia or non-pneumonia.\nThere may be multiple rows per patientId.\n\nDICOM Images\nAll provided images are in DICOM format.\n\nWhat am I predicting?\nIn this challenge competitors are predicting whether pneumonia exists in a given image. They do so by predicting bounding boxes around areas of the lung. Samples without bounding boxes are negative and contain no definitive evidence of pneumonia. Samples with bounding boxes indicate evidence of pneumonia.\nWhen making predictions, competitors should predict as many bounding boxes as they feel are necessary, in the format: confidence x-min y-min width height\nThere should be only ONE predicted row per image. This row may include multiple bounding boxes.\nA properly formatted row may look like any of the following.\nFor patientIds with no predicted pneumonia / bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,\nFor patientIds with a single predicted bounding box: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100\nFor patientIds with multiple predicted bounding boxes: 0004cfab-14fd-4e49-80ba-63a80b6bddd6,0.5 0 0 100 100 0.5 0 0 100 100, etc.\n\nFile descriptions\nstage_2_train.csv - the training set. Contains patientIds and bounding box / target information.\nstage_2_sample_submission.csv - a sample submission file in the correct format. Contains patientIds for the test set. Note that the sample submission contains one box per image, but there is no limit to the number of bounding boxes that can be assigned to a given image.\nstage_2_detailed_class_info.csv - provides detailed information about the type of positive or negative class for each image.\n\nData fields\npatientId - A patientId. Each patientId corresponds to a unique image.\nx - the upper-left x coordinate of the bounding box.\ny - the upper-left y coordinate of the bounding box.\nwidth - the width of the bounding box.\nheight - the height of the bounding box.\nTarget - the binary Target, indicating whether this sample has evidence of pneumonia.\n\nFiles\n29687 files\nSize 3.96 GB\nType dcm, csv, txt\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "classification",
          "images",
          "bbox_localization",
          "healthcare",
          "map"
        ]
      }
    },
    {
      "challenge_name": "santander-customer-transaction-prediction",
      "description": "Challenge description:\nSantander Customer Transaction Prediction\nCan you identify who will make a transaction?\n\nDescription\nAtSantanderour mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.\n\nEvaluation\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\nSubmission File\nFor each Id in the test set, you must make a binary prediction of the target variable. The file should contain a header and have the following format:\nID_code,target\ntest_0,0\ntest_1,1\ntest_2,0\netc.\n\nPrizes\n1st Place-   $ 25,000\n2nd Place- $ 17,000\n3rd Place-  $ 10,000\n4th Place-  $ 8,000\n5th Place-  $ 5,000\n\nTimeline\nApril 3, 2019- Entry deadline. You must accept the competition rules before this date in order to compete.\nApril 3, 2019- Team Merger deadline. This is the last day participants may join or merge teams.\nApril 10, 2019- Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nCitation\nMercedes Piedra, Sohier Dane, and Soraya_Jimenez. Santander Customer Transaction Prediction. https://kaggle.com/competitions/santander-customer-transaction-prediction, 2019. Kaggle.\n\nCompetition Host\nBanco Santander\n\nPrizes & Awards\n$65,000\nAwards Points & Medals\n\nParticipation\n25,493 Entrants\n9,787 Participants\n8,751 Teams\n104,129 Submissions\n\nTags\nBanking\nTabular\nBinary Classification\nArea Under Receiver Operating Characteristic Curve\n\nData description:\nSantander Customer Transaction Prediction\nBanco Santander \u00b7 Featured Prediction Competition\nCan you identify who will make a transaction?\n\nDataset Description\nYou are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\nThe task is to predict the value of target column in the test set.\n\nFile descriptions\ntrain.csv - the training set.\ntest.csv - the test set. The test set contains some rows which are not included in scoring.\nsample_submission.csv - a sample submission file in the correct format.\n\nFiles: 3 files\nSize: 606.35 MB\nType: csv\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/santander-customer-transaction-prediction",
      "competition_description": "Challenge description:\nSantander Customer Transaction Prediction\nCan you identify who will make a transaction?\n\nDescription\nAtSantanderour mission is to help people and businesses prosper.  We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure  we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.",
      "evaluation_metric": "Evaluation\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.",
      "dataset_description": "Data description:\nSantander Customer Transaction Prediction\nBanco Santander \u00b7 Featured Prediction Competition\nCan you identify who will make a transaction?\n\nDataset Description\nYou are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.\nThe task is to predict the value of target column in the test set.\n\nFile descriptions\ntrain.csv - the training set.\ntest.csv - the test set. The test set contains some rows which are not included in scoring.\nsample_submission.csv - a sample submission file in the correct format.\n\nFiles: 3 files\nSize: 606.35 MB\nType: csv\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "binary classification",
          "tabular",
          "feature engineering",
          "banking",
          "auc"
        ]
      }
    },
    {
      "challenge_name": "santander-value-prediction-challenge",
      "description": "Challenge description:\nSantander Value Prediction Challenge\nPredict the value of transactions for potential customers.\n\nAccording to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.\nThe digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner\u2026 and often before they\u00b4ve even realized they need the service. \nIn their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.\n\nEvaluation\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\nThe RMSLE is calculated as\n$$\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\nWhere:\n$\\epsilon$ is the RMSLE value (score)\n$n$ is the total number of observations in the (public/private) data set,\n$p_i$ is your prediction of target, and\n$a_i$ is the actual target for $i$.\n$\\log(x)$ is the natural logarithm of $x$\n\nSubmission File\nFor every row in the test.csv, submission files should contain two columns: ID and target. The ID corresponds to the column of that ID in the test.tsv. The file should contain a header and have the following format:\nID,target\n000137c73,5944923.322036332\n00021489f,5944923.322036332\n0004d7953,5944923.322036332\netc.\n\nPrizes\n1st Place - $25,000\n2nd Place - $15,000\n3rd Place - $10,000\n4th Place - $5,000\n5th Place - $5,000\n\nTimeline\nAugust 13, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nAugust 13, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nAugust 20, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nCitation\nMark McDonald, Mercedes Piedra, Sohier Dane, and Soraya_Jimenez. Santander Value Prediction Challenge. https://kaggle.com/competitions/santander-value-prediction-challenge, 2018. Kaggle.\n\nCompetition Host\nBanco Santander\n\nPrizes & Awards\n$60,000\nAwards Points & Medals\n\nParticipation\n19,032 Entrants\n4,865 Participants\n4,463 Teams\n54,750 Submissions\n\nTags\nBanking\nFinance\nRoot Mean Squared Logarithmic Error\n\nData description:\nSantander Value Prediction Challenge\nPredict the value of transactions for potential customers.\n\nDataset Description\nYou are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column.\nThe task is to predict the value of target column in the test set.\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\nsample_submission.csv - a sample submission file in the correct format\n\nFiles: 3 files\nSize: 1.08 GB\nType: csv\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/santander-value-prediction-challenge",
      "competition_description": "Challenge description:\nSantander Value Prediction Challenge\nPredict the value of transactions for potential customers.\n\nAccording to Epsilon research, 80% of customers are more likely to do business with you if you provide personalized service. Banking is no exception.\nThe digitalization of everyday lives means that customers expect services to be delivered in a personalized and timely manner\u2026 and often before they\u00b4ve even realized they need the service. \nIn their 3rd Kaggle competition, Santander Group aims to go a step beyond recognizing that there is a need to provide a customer a financial service and intends to determine the amount or value of the customer's transaction. This means anticipating customer needs in a more concrete, but also simple and personal way. With so many choices for financial services, this need is greater now than ever before.\nIn this competition, Santander Group is asking Kagglers to help them identify the value of transactions for each potential customer. This is a first step that Santander needs to nail in order to personalize their services at scale.",
      "evaluation_metric": "Evaluation\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\nThe RMSLE is calculated as\n$$\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$\nWhere:\n$\\epsilon$ is the RMSLE value (score)\n$n$ is the total number of observations in the (public/private) data set,\n$p_i$ is your prediction of target, and\n$a_i$ is the actual target for $i$.\n$\\log(x)$ is the natural logarithm of $x$",
      "dataset_description": "Data description:\nSantander Value Prediction Challenge\nPredict the value of transactions for potential customers.\n\nDataset Description\nYou are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column.\nThe task is to predict the value of target column in the test set.\n\nFile descriptions\ntrain.csv - the training set\ntest.csv - the test set\nsample_submission.csv - a sample submission file in the correct format\n\nFiles: 3 files\nSize: 1.08 GB\nType: csv\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "regression",
          "tabular",
          "feature engineering",
          "finance",
          "rmsle"
        ]
      }
    },
    {
      "challenge_name": "siim-acr-pneumothorax-segmentation",
      "description": "Challenge description:\n# SIIM-ACR Pneumothorax Segmentation\n\n## Description\n\nImagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer.\n\nPneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying\u2014it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.\n\nPneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more more confident diagnosis for non-radiologists.\n\nThe Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help.\n\nIn this competition, you'll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives.\n\nIf you're up for the challenge, take a deep breath, and get started now.\n\n**Note:** As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so.\n\n### Acknowledgments\n\nSIIM Machine Learning Committee Co-Chairs, Steven G. Langer, PhD, CIIP and George Shih, MD, MS for tirelessly leading this effort and making the challenge possible in such a short period of time.\n\nSIIM Machine Learning Committee Members for their dedication in annotating the dataset, helping to define the most useful metrics and running tests to prepare the challenge for launch.\n\nSIIM Hackathon Committee, especially Mohannad Hussain, for their crucial technical support with data conversion.\n\nAmerican College of Radiology (ACR), @RadiologyACR: For Co-hosting the challenge and Co-sponsoring the Prizes\n\nSociety of Thoracic Radiology (STR), @thoracicrad: For their unparalleled expertise in adjudicating the dataset\n\nMD.ai: For providing the annotation tool and helping with the first layer of annotations\n\n## Evaluation\n\nThis competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$ \\frac{2 * |X \\cap Y|}{|X| + |Y|},$$\n\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.\n\n### Submission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values.\n\nInstead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. Note that this competition uses relative pixel positions, meaning that after the first pixel position, the remaining pixel values are simply offsets. For example, '1 3 10 5' implies pixels 1,2,3 are to be included in the mask, as well as 14,15,16,17,18. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nThe file should contain a header and have the following format:\n\nImageId,EncodedPixels\n0004d4463b50_01,1 1 5 1\n0004d4463b50_02,1 1\n0004d4463b50_03,1 1\netc.\n\nSubmission files may take several minutes to process due to the size.\n\n## Timeline\n\nAugust 21, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\n\nAugust 21, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\n\nAugust 28, 2019 - Stage 1 ends & Model upload deadline.\n\nIn order to be eligible for Stage 2, each team's Stage 1 submission must include the model uploaded, via Team -> Your Model, per the Competition Rules. This model should match your final submission selected for scoring in Stage 2. The deadline for model upload is firmly the end of Stage 1.\n\nThis requirement is in place for the host team to verify the performance of the uploaded models matches the Stage 2 submission file. Compliance with the above will be verified by the host team. Submitters who fail to upload their model by the Stage 1 deadline, or are found not to be in compliance, may be disqualified from Stage 2 and removed from the final leaderboard.\n\nAugust 29, 2019 - Stage 2 begins. New test set uploaded.\n\nSeptember 4, 2019 - Stage 2 ends & Final submission deadline.\n\nSeptember 13, 2019 - Solutions & Other Winners Obligations due from winners. Review Prizes page for details on special winners obligations for this competition.\n\nSeptember 22-23, 2019 - SIIM CMIMI Conference 2019 in Austin, TX\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $12,000\n2nd Place - $7,000\n3rd Place - $4,000\n4th - 10th Place - $1,000 each\n\nBecause this competition is being hosted in coordination with the Annual Conference on Machine Intelligence in Medical Imaging (C-MIMI), one representative from each winning team will be invited and strongly encouraged to attend the conference with waived registration fees, contingent on review of solution and fulfillment of winners' obligations.\n\nNote that in addition to the standard Kaggle Winners' Obligations (open-source licensing requirements, solution packaging/delivery, presentation to host), the host team also asks that you:\n(i) create a short video (not to exceed 5 minutes),\n(ii) publish a link to your open sourced code on the competition forum, and\n(iii) (strongly suggested) make some version of your model publicly available for more hands-on testing purposes only. As an example of a hosted algorithm, please see http://demos.md.ai/#/bone-age.\n\n## SIIM CMIMI Conference\n\nThe Annual Conference on Machine Intelligence in Medical Imaging (C-MIMI) has become a can't-miss event that helps you get up to speed on what's happening in imaging AI, and provides access to researchers, practitioners, and industry.\n\nJoin Us Where Innovation Happens and\n- Deep dive into one of the hottest areas in medical imaging\n- Play a role in advancing machine learning-based applications into the real world\n- Get access to the latest cutting-edge research & tools, regulatory updates and best practice real life implementations\n\nMeet us in Austin to find out who are the Top 10 Winners of the SIIM-ACR Pneumothorax Challenge!\n\nLearn More: https://siim.org/page/2019cmimi\n\n## Resources\n\nTutorials:\n- Retrieving the dataset from the Google Cloud Healthcare API is described in this tutorial page or in pdf format here.\n- A DICOM to JPEG convertor kernel can be found in this kernel, for easily converting the dataset to PNG, JPEG, JPEG2000, or any other more easily viewable image format. Special thanks to Shunxing Bao for his contribution.\n- A reference model framework for getting started on this dataset can be reviewed in this starter kernel. Special thanks to Shunxing Bao for his contribution.\n\nWe're distributing $300 Coupons for Google Cloud Platform (GCP) Credit to teams competing in this competition. Requests can be made by submitting this form before July 15, 2019. Only one request should be made per team.\n\n## SIIM Cloud Healthcare API Tutorial\n\n**UPDATE:** This dataset is no longer available via the Cloud Healthcare API. Others have shared the dataset on Kaggle, if you're interested in accessing it through those methods.\n\nFor your convenience, this tutorial is also provided in pdf format here.\n\n### About this Tutorial\n\nThis tutorial is necessary to retrieve the dataset for participating in the SIIM-ACR Pnuemothorax Segmentation Competition on Kaggle. The dataset is only hosted on Google Cloud Platform (GCP) through the Cloud Healthcare (CHC) API.\n\nThere are methods to retrieve these datasets at no cost to you. And it is also possible to use GCP to do your modeling, with associated pricing structures for those activities, depending on which tools are used. This competition is also issuing GCP Credits for users interested in exploring the latter. Review the competition's Resources page for more details.\n\n### Prerequisites\n\n- A Google Cloud account. If you don't have an account, register for one here. For new users, the Google Cloud Products (GCP) free tier will give you credits to use on GCP for things like storage or using GCP ML tools where there are costs incurred.\n- A Google Cloud project.\n- The google email associated with your Google Cloud account must join the SIIM-Kaggle google group to gain permissions to the necessary Healthcare dataset. Note that once you've completed the \"join\" to the group, you will self-join without any further approval. There is no further action to take. It is not a real e-mail group, so you should not see any posts nor should you be able to make any posts. It exists purely to provision access to the competition's datastore.\n- Download and install gcloud.\n- Download and install gsutil.\n\n### About the Cloud Healthcare API\n\nThe Cloud Healthcare (CHC) API is a managed service that can be used to store, retrieve, and query medical formats such as DICOM and FHIR. In the following steps, we will use this API to query and retrieve the DICOM images and the images' FHIR annotations.\n\n### Authentication\n\nThe easiest way to authenticate is to use Application Default Credentials. This will set up all Cloud SDK tooling and API client libraries to use your user credentials.\n\nIn a new shell, run the following command and then follow the instructions:\n\n```\ngcloud auth application-default login\n```\n\nNote: There are various other ways to authenticate to the Cloud Healthcare API. The method you choose depends on many factors, including whether you are running as an end-user or as a service account (useful for running in a GCP VM). The following guides discuss the authentication options:\n- Authorizing Cloud SDK tools\n- Authentication to the Cloud Healthcare API\n\n### Introduction to DICOMWeb\n\nThe DICOMWeb protocol can be used to interact with the Cloud Healthcare API's DICOMWeb endpoint. DICOM objects are structured in this form:\n\nStudy 1\n  Series 1A\n    Image/Instance 1A1\n    Image/Instance 1A2\n    ...etc\n  Series 1B\nStudy 2\n  ...etc\n\nThink of an instance as a single object, like an image or a report. The series can contain multiple instances (e.g. a CT scan series). The study can contain multiple series (e.g. a series with medical images, and a series with a report). In the dataset we are going to deal with most studies contain a single series, and most series contain a single instance.\n\nIn the \"Exploring and Downloading DICOM instances\" section below, you'll be presented with 2 methods to retrieve the instances from the competition datasets.\n\nIn the \"Downloading the FHIR annotations\" section below, you'll be shown how to retrieve the training set annotations.\n\n### Exploring and Downloading the DICOM instances\n\nThe DICOMWeb protocol can be used to interact with the Cloud Healthcare API's DICOMWeb endpoint. For information about the features supported by this endpoint, see the DICOM conformance statement.\n\nThere are two DICOM stores, one for hosting training data and one for hosting test data.\n\nTraining\n```\nPROJECT_ID=kaggle-siim-healthcare\nREGION=us-central1\nDATASET_ID=siim-pneumothorax\nDICOM_STORE_ID=dicom-images-train\n```\n\nTest\n```\nPROJECT_ID=kaggle-siim-healthcare\nREGION=us-central1\nDATASET_ID=siim-pneumothorax\nDICOM_STORE_ID=dicom-images-test\n```\n\nThere will be 2 methods presented in this tutorial for retrieving the DICOM images:\n\n1. Download DICOM Studies, Series, Instances or Frames locally. This can be achieved using the script provided. Please give it a some time as it is downloading around ~10K images.\n2. Export instances to your GCS bucket. This is recommended if you want to work with the dataset with some other GCP tooling (i.e. GCE/GCK, Cloud ML Engine, Colaboratory). From the bucket, you can also download from the bucket locally. Note: You can expect to incur charges for the storage of the dataset to your GCS bucket (and possibly other costs associated with using Cloud tools).\n\n#### Method 1: Downloading DICOM Studies, Series, Instances or Frames Locally\n\nThis method uses a script to download the datastore (hosted on the kaggle-siim-healthcare project) to your local directory.\n\n1. Authentication\nAs previously mentioned in the Authentication section please run the following to authenticate your user credentials.\n```\ngcloud auth application-default login\n```\n\n2. Install the necessary Python libraries.\nFor the script below you will need the retrying library and the google auth library. And, if running on Python 2.7 you will need to install the futures library.\n```\npip install retrying\npip install google-auth\n# Only if running on Python 2.7\npip install futures\n```\n\n3. Run the script below.\nWe've provided a download_images.py script on the Data Page that downloads all images in both DICOM stores and saves them in the directory where the script is run. The train set is about 1.5 GB total. The test set is about 180 MB total. If, instead, you're interested in retrieving specific studies, series, instances, you should review the information under the \"Other Cloud Healthcare API Actions\" section. You'll have to then revise the script to retrieve the specific data you are looking for.\n\nNote: Due to the large number of images, this may take 10-15 minutes.\n\n#### Method 2: Export DICOM instances to your GCS bucket\n\nThis method will post the datastore's DICOM instances (hosted on the kaggle-siim-healthcare project) to a GCS bucket that you name in your GCP project.\n\n1. Creating a GCS bucket\nGoogle Cloud Storage (GCS) is a storage system used to store and access objects on GCP. The Cloud Healthcare API can export all DICOM instances to a GCS bucket. To do this, we first create a GCS bucket and assign it to a BUCKET variable. For more detailed guidance, see Creating storage buckets.\nMY_PROJECT represents the project name for your GCP project where you'll be transferring the dataset.\n```\nMY_PROJECT=\"MY_PROJECT\"\nBUCKET=\"gs://${USER}-siim-bucket-dicom/\"\ngsutil mb -p ${MY_PROJECT} -c regional -l us-central1 -b on ${BUCKET}\n```\n\n2. Granting permissions to export to GCS\nTo grant permissions for the Cloud Healthcare API to write to the new bucket, follow the instructions for Exporting data to Cloud Storage.\n\nIMPORTANT: The roles required should be set for this member: service-38125765871@gcp-sa-healthcare.iam.gserviceaccount.com -- Note that without allowing the Storage Object Admin role for this specific member, you will not be able to download the dataset to a GCS bucket.\n\n3. Run export operation to copy DICOM store into GCS bucket\nTo run the export operation and inspect its status, follow the instructions in Exporting DICOM instances. You can find a code sample to export the training dataset below. You can use the DIRECTORY variable to control which sub-directory to output the DICOM instances to in the bucket.\n\nNote: Depending on the notation required by the method you are using to execute the below code, your uriPrefix may need to be adjusted. The goal is to name it to match the format: 'gs://YOUR_BUCKET/train' where YOUR_BUCKET is the bucket you created in the prior section.\n```\nPROJECT_ID=\"kaggle-siim-healthcare\"\nREGION=\"us-central1\"\nDATASET_ID=\"siim-pneumothorax\"\nDICOM_STORE_ID=\"dicom-images-train\"\nDIRECTORY=\"train-3\"\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n--data \"{\n  'gcsDestination': {\n    'uriPrefix': '\"${BUCKET}${DIRECTORY}\"'\n  }\n}\" \"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/dicomStores/${DICOM_STORE_ID}:export\"\n```\n\nSimilarly, run the following command to retrieve the DICOM test set.\n```\nDIRECTORY_TEST=\"test\"\nDICOM_STORE_ID_TEST=\"dicom-images-test\"\ncurl -X POST \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n--data \"{\n  'gcsDestination': {\n    'uriPrefix': '\"${BUCKET}${DIRECTORY_TEST}\"'\n  }\n}\" \"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/dicomStores/${DICOM_STORE_ID_TEST}:export\"\n```\n\n4. Check operation status\nCheck status of the long-running operation by replacing OPERATION_ID with the operation ID that is returned in the JSON response to the prior request.\n```\ncurl -X GET \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/operations/OPERATION_ID\"\n```\n\nTo list the contents of the output bucket, run the following command.\n```\ngsutil ls ${BUCKET}${DIRECTORY}/**\n```\n\n5. Copy GCS bucket to local file system\nTo copy the GCS bucket to your local file system, run the following command:\n```\nmkdir /tmp/siim-dicom\ngsutil -m cp -R ${BUCKET}${DIRECTORY} /tmp/siim-dicom/\n```\n\n### Downloading the FHIR annotations\n\nThe annotations for this dataset are stored as FHIR resources (specifically as a DocumentReference resource). The Cloud Healthcare API can be used to query and download these FHIR resources. The features supported by this endpoint can be found in the FHIR conformance statement.\n\nThere is one FHIR store containing the training annotations:\n```\nPROJECT_ID=kaggle-siim-healthcare\nREGION=us-central1\nDATASET_ID=siim-pneumothorax\nFHIR_STORE_ID=fhir-masks-train\nDOCUMENT_REFERENCE_ID=d70d8f3e-990a-4bc0-b11f-c87349f5d4eb\n```\n\nThe DocumentReference resources contain a URL to the CSV file hosted on siim.org that contains the annotated masks.\n\nThe following code sample will access the FHIR datastore for the training dataset. See Getting a FHIR resource for samples in other languages.\n\nNote that the response from the below code will point to a URL for the .CSV that contains the annotated masks for each ImageId.\n```\nPROJECT_ID=\"kaggle-siim-healthcare\"\nREGION=\"us-central1\"\nDATASET_ID=\"siim-pneumothorax\"\nFHIR_STORE_ID=\"fhir-masks-train\"\nDOCUMENT_REFERENCE_ID=\"d70d8f3e-990a-4bc0-b11f-c87349f5d4eb\"\ncurl -X GET \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/fhirStores/${FHIR_STORE_ID}/fhir/DocumentReference/${DOCUMENT_REFERENCE_ID}\"\n```\n\n### Other Cloud Healthcare API Actions\n\n#### Searching for DICOM Instances\n\nThe SearchTransaction protocol can be used to search for studies, series, or instances in a dicomStore. The following code sample searches for all instances below on the training dataset.\n\nSee Searching for studies, series, instances, and frames for additional examples. See the supported search modes and the supported DICOM tags.\n```\ncurl -X GET \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/dicomStores/${DICOM_STORE_ID}/dicomWeb/instances\"\n```\n\n#### Fetching a specific study, series, instance or frame\n\nIf you'd like to fetch specific study, series, instance or frame to your local machine, you can use the RetrieveTransaction protocol. You can find sample call below to retrieve an individual instance. For this example, a known Study UID from the training dataset has been declared. See Retrieving a study, series, instance, or frame for samples in other languages.\n```\nSTUDY_UID=\"1.2.276.0.7230010.3.1.2.8323329.12562.1517875239.738011\"\ncurl -X GET \\\n-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n\"https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID}/locations/${REGION}/datasets/${DATASET_ID}/dicomStores/${DICOM_STORE_ID}/dicomWeb/studies/${STUDY_UID}\"\n```\n\nThe response of this call is MIME Multipart. We can decode the MIME Multipart message and retrieve the individual instances using common libraries, such as https://github.com/requests/toolbelt (you will need to install this).\n\n#### Exporting DICOM metadata to BigQuery\n\nYou can export the metadata of all DICOMs in a DICOM store to BigQuery. This might be useful if you'd like to do some quick analytics on the metadata. For more information, please see https://cloud.google.com/healthcare/docs/how-tos/dicom-export-bigquery.\n\n## Citation\n\nAnna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, Paras Lakhani, Phil Culliton, and Shunxing Bao. SIIM-ACR Pneumothorax Segmentation. https://kaggle.com/competitions/siim-acr-pneumothorax-segmentation, 2019. Kaggle.\n\nData description:\nSIIM-ACR Pneumothorax Segmentation\nIdentify Pneumothorax disease in chest x-rays\n\nUPDATE: This dataset is no longer available via the Cloud Healthcare API. Others have shared the dataset on Kaggle, if you're interested in accessing it through those methods.\nStage 2 Note: the stage 1 files (if needed) should be downloaded using the special downloading instructions. The stage 2 files must be downloaded directly from Kaggle. See below.\nNote: this is a two-stage competition with special downloading instructions. Please read carefully and ask any questions you might have!\n\nWhat files do I need?\nThe stage 2 training set consists of the stage 1 training set and the stage 1 test set combined. If you need to download their images, please follow these instructions for bothtrainANDtest, usingthis tutorial pageor in.pdf format here.\nThe annotations for the stage 2 training set should be downloaded below - the filename isstage_2_train.csv. The stage 2 test images can be downloaded directly from this page - contained instage_2_images.zip.\nYou may also need the sample submission (which contains all of the test IDs for stage 2) to aid you in making predictions.\n\nWhat should I expect the data format to be?\nThe data is comprised of images in DICOM format and annotations in the form of image IDs and run-length-encoded (RLE) masks. Some of the images contain instances of pneumothorax (collapsed lung), which are indicated by encoded binary masks in the annotations. Some training images have multiple annotations.\n1.2.276.0.7230010.3.1.4.8323329.14508.1517875252.443873,387620 23996339864397751968589626595670952749497694679\nImages without pneumothorax have a mask value of -1.\n1.2.276.0.7230010.3.1.4.8323329.1034.1517875166.8504,-1\n\nWhat am I predicting?\nWe are attempting to a) predict the existence of pneumothorax in our test images and b) indicate the location and extent of the condition using masks. Your model should create binary masks and encode them using RLE. Note that we are using arelativeform of RLE (meaning that pixel locations are measured from the end of the previous run) as indicated below:\n1.2.276.0.7230010.3.1.4.8323329.14508.1517875252.443873,387620 23996339864397751968589626595670952749497694679\nSample code is available for download that may help with encoding and decoding this form of RLE.\nEach test image may only have one mask submitted for it. It should combine all predicted masks for that image.",
      "docker_challenge_path": "/data/siim-acr-pneumothorax-segmentation",
      "competition_description": "## Description\n\nImagine suddenly gasping for air, helplessly breathless for no apparent reason. Could it be a collapsed lung? In the future, your entry in this competition could predict the answer.\n\nPneumothorax can be caused by a blunt chest injury, damage from underlying lung disease, or most horrifying\u2014it may occur for no obvious reason at all. On some occasions, a collapsed lung can be a life-threatening event.\n\nPneumothorax is usually diagnosed by a radiologist on a chest x-ray, and can sometimes be very difficult to confirm. An accurate AI algorithm to detect pneumothorax would be useful in a lot of clinical scenarios. AI could be used to triage chest radiographs for priority interpretation, or to provide a more more confident diagnosis for non-radiologists.\n\nThe Society for Imaging Informatics in Medicine (SIIM) is the leading healthcare organization for those interested in the current and future use of informatics in medical imaging. Their mission is to advance medical imaging informatics across the enterprise through education, research, and innovation in a multi-disciplinary community. Today, they need your help.\n\nIn this competition, you'll develop a model to classify (and if present, segment) pneumothorax from a set of chest radiographic images. If successful, you could aid in the early recognition of pneumothoraces and save lives.\n\nIf you're up for the challenge, take a deep breath, and get started now.\n\n**Note:** As specified on the Data Page, the dataset must be retrieved from Cloud Healthcare. Review this tutorial (or in pdf format) for instructions on how to do so.",
      "evaluation_metric": "## Evaluation\n\nThis competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$ \\frac{2 * |X \\\\cap Y|}{|X| + |Y|},$$\n\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each image in the test set.",
      "dataset_description": "Data description:\nSIIM-ACR Pneumothorax Segmentation\nIdentify Pneumothorax disease in chest x-rays\n\nUPDATE: This dataset is no longer available via the Cloud Healthcare API. Others have shared the dataset on Kaggle, if you're interested in accessing it through those methods.\nStage 2 Note: the stage 1 files (if needed) should be downloaded using the special downloading instructions. The stage 2 files must be downloaded directly from Kaggle. See below.\nNote: this is a two-stage competition with special downloading instructions. Please read carefully and ask any questions you might have!\n\nWhat files do I need?\nThe stage 2 training set consists of the stage 1 training set and the stage 1 test set combined. If you need to download their images, please follow these instructions for bothtrainANDtest, usingthis tutorial pageor in.pdf format here.\nThe annotations for the stage 2 training set should be downloaded below - the filename isstage_2_train.csv. The stage 2 test images can be downloaded directly from this page - contained instage_2_images.zip.\nYou may also need the sample submission (which contains all of the test IDs for stage 2) to aid you in making predictions.\n\nWhat should I expect the data format to be?\nThe data is comprised of images in DICOM format and annotations in the form of image IDs and run-length-encoded (RLE) masks. Some of the images contain instances of pneumothorax (collapsed lung), which are indicated by encoded binary masks in the annotations. Some training images have multiple annotations.\n1.2.276.0.7230010.3.1.4.8323329.14508.1517875252.443873,387620 23996339864397751968589626595670952749497694679\nImages without pneumothorax have a mask value of -1.\n1.2.276.0.7230010.3.1.4.8323329.1034.1517875166.8504,-1\n\nWhat am I predicting?\nWe are attempting to a) predict the existence of pneumothorax in our test images and b) indicate the location and extent of the condition using masks. Your model should create binary masks and encode them using RLE. Note that we are using arelativeform of RLE (meaning that pixel locations are measured from the end of the previous run) as indicated below:\n1.2.276.0.7230010.3.1.4.8323329.14508.1517875252.443873,387620 23996339864397751968589626595670952749497694679\nSample code is available for download that may help with encoding and decoding this form of RLE.\nEach test image may only have one mask submitted for it. It should combine all predicted masks for that image.",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "segmentation",
          "images",
          "unet/cnn",
          "medical_imaging",
          "dice"
        ]
      }
    },
    {
      "challenge_name": "sp-society-camera-model-identification",
      "description": "Challenge description:\nCompetition Host: IEEE Signal Processing Society\nPrizes & Awards: $25,000\n\nFinding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains\u2013is the footage real?\nToday, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself.\nThis problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved.\nFor this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.\n\nThis competition is evaluated on the weighted categorization accuracy of your predictions (the percentage of camera models correctly predicted).\n$$\\text{weighted_accuracy}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{w_i  (y_i = \\hat{y}_i)}{\\sum{w_i}} $$\nwhere $n$ is the number of samples in the test set, $y$ is the true camera label, $\\hat{y}$ is the predicted camera label, and $w_i$ is 0.7 for unaltered images, and 0.3 for altered images.\n\nSubmission File:\nFor each image $fname$ in the test set, you must predict the correct camera model. The submission file should contain a header and have the following format:\nfname,camera\nimg_0002a04_manip.tif,iPhone-6\nimg_001e31c_unalt.tif,iPhone-6\nimg_00275cf_manip.tif,iPhone-6\nimg_0034113_unalt.tif,iPhone-6\n\nFebruary 1, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nFebruary 1, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nFebruary 8, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\nApril 22-27, 2018 - ICASSP 2018\nThe top three eligible finalists of this competition will be invited to the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing ('SP Cup') in Alberta, Canada. See 'Prizes' Tab for more information on eligibility for this prize.\nAny and all teams are welcome to register for and attend ICASSP!\n\nFirst Prize: $12,000\nSecond Prize: $8,000\nThird Prize: $5,000\n\nSecond Stage Eligibility (Optional):\nThe top three eligible teams with the highest performance in the Kaggle competition will be selected as finalists and will be invited to participate in the final \"SP Cup,\" live at ICASSP 2018 (not held on Kaggle), which will be held April 22-27, 2018.\nTo be eligible, the team must be composed of:\n- one faculty member (as the team supervisor),\n- at most one graduate student (as a tutor),\n- at least three but no more than ten undergraduate students,\n- at least three of the undergraduate team members must hold either regular or student memberships of the IEEE Signal Processing Society.\nThe top three eligible teams can receive up to $1,200 for continental travel or $1,700 for intercontinental travel per member, (at most three people from each team) to participate in the final SP Cup challenge at the IEEE International Conference on Speech Acoustics and Signal Processing (ICASSP) in Calgary, Alberta, Canada.\nClick here for more information on eligibility requirements for the SP Cup!\n\nAddison Howard, inversion, Mark McDonald, Matthew Stamm, and Paolo Bestagini. IEEE's Signal Processing Society - Camera Model Identification. https://kaggle.com/competitions/sp-society-camera-model-identification, 2017. Kaggle.\n\nData description:\nIEEE Signal Processing Society \u00b7 Featured Prediction Competition \u00b7 8 years ago  \nLate Submission  \nIEEE's Signal Processing Society - Camera Model Identification  \nIdentify from which camera an image was taken  \n\nDataset Description  \nHow the data was collected  \nImages in the training set were captured with 10 different camera models, a single device per model, with 275 full images from each device.  \nThe list of camera models is as follows:  \nSony NEX-7  \nMotorola Moto X  \nMotorola Nexus 6  \nMotorola DROID MAXX  \nLG Nexus 5x  \nApple iPhone 6  \nApple iPhone 4s  \nHTC One M7  \nSamsung Galaxy S4  \nSamsung Galaxy Note 3  \nImages in the test set were captured with the same 10 camera models, but using a second device. For example, if the images in the train data for the iPhone 6 were taken with Ben Hamner's device (Camera 1), the images in the test data were taken with Ben Hamner'sseconddevice (Camera 2), since he lost the first device in the Bay while kite-surfing.  \nNone of the images in the test data were taken with the samedeviceas in the train data.  \nWhile the train data includes full images, the test data contains only single 512 x 512 pixel blocks cropped from the center of a single image taken with the device. No two image blocks come from the same original image.  \nHalf of the images in the test set have been altered. The image names indicate whether or not they were manipulated (_manip) from the original or unaltered (_unalt). While you are not explicitly told how each individual image was altered, the set of possible processing operations that were performed are as follows:  \nJPEG compression with quality factor = 70  \nJPEG compression with quality factor = 90  \nresizing (via bicubic interpolation) by a factor of 0.5  \nresizing (via bicubic interpolation) by a factor of 0.8  \nresizing (via bicubic interpolation) by a factor of 1.5  \nresizing (via bicubic interpolation) by a factor of 2.0  \ngamma correction using gamma = 0.8  \ngamma correction using gamma = 1.2  \n\nFile descriptions  \ntrain.zip - the training set, organized in folders that correspond the the appropriatecameramodel label  \ntest.zip - the test set; please see above description of how these files were created  \nsample_submission.csv - a sample submission file in the correct format  \nFiles5391 filesSize11.45 GBTypejpg, tif, csvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/sp-society-camera-model-identification",
      "competition_description": "Challenge description:\nCompetition Host: IEEE Signal Processing Society\nPrizes & Awards: $25,000\n\nFinding footage of a crime caught on tape is an investigator's dream. But even with crystal clear, damning evidence, one critical question always remains\u2013is the footage real?\nToday, one way to help authenticate footage is to identify the camera that the image was taken with. Forgeries often require splicing together content from two different cameras. But, unfortunately, the most common way to do this now is using image metadata, which can be easily falsified itself.\nThis problem is actively studied by several researchers around the world. Many machine learning solutions have been proposed in the past: least-squares estimates of a camera's color demosaicing filters as classification features, co-occurrences of pixel value prediction errors as features that are passed to sophisticated ensemble classifiers, and using CNNs to learn camera model identification features. However, this is a problem yet to be sufficiently solved.\nFor this competition, the IEEE Signal Processing Society is challenging you to build an algorithm that identifies which camera model captured an image by using traces intrinsically left in the image. Helping to solve this problem would have a big impact on the verification of evidence used in criminal and civil trials and even news reporting.",
      "evaluation_metric": "This competition is evaluated on the weighted categorization accuracy of your predictions (the percentage of camera models correctly predicted).\n$$\\text{weighted_accuracy}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{w_i  (y_i = \\hat{y}_i)}{\\sum{w_i}} $$\nwhere $n$ is the number of samples in the test set, $y$ is the true camera label, $\\hat{y}$ is the predicted camera label, and $w_i$ is 0.7 for unaltered images, and 0.3 for altered images.",
      "dataset_description": "Data description:\nIEEE Signal Processing Society \u00b7 Featured Prediction Competition \u00b7 8 years ago  \nLate Submission  \nIEEE's Signal Processing Society - Camera Model Identification  \nIdentify from which camera an image was taken  \n\nDataset Description  \nHow the data was collected  \nImages in the training set were captured with 10 different camera models, a single device per model, with 275 full images from each device.  \nThe list of camera models is as follows:  \nSony NEX-7  \nMotorola Moto X  \nMotorola Nexus 6  \nMotorola DROID MAXX  \nLG Nexus 5x  \nApple iPhone 6  \nApple iPhone 4s  \nHTC One M7  \nSamsung Galaxy S4  \nSamsung Galaxy Note 3  \nImages in the test set were captured with the same 10 camera models, but using a second device. For example, if the images in the train data for the iPhone 6 were taken with Ben Hamner's device (Camera 1), the images in the test data were taken with Ben Hamner'sseconddevice (Camera 2), since he lost the first device in the Bay while kite-surfing.  \nNone of the images in the test data were taken with the samedeviceas in the train data.  \nWhile the train data includes full images, the test data contains only single 512 x 512 pixel blocks cropped from the center of a single image taken with the device. No two image blocks come from the same original image.  \nHalf of the images in the test set have been altered. The image names indicate whether or not they were manipulated (_manip) from the original or unaltered (_unalt). While you are not explicitly told how each individual image was altered, the set of possible processing operations that were performed are as follows:  \nJPEG compression with quality factor = 70  \nJPEG compression with quality factor = 90  \nresizing (via bicubic interpolation) by a factor of 0.5  \nresizing (via bicubic interpolation) by a factor of 0.8  \nresizing (via bicubic interpolation) by a factor of 1.5  \nresizing (via bicubic interpolation) by a factor of 2.0  \ngamma correction using gamma = 0.8  \ngamma correction using gamma = 1.2  \n\nFile descriptions  \ntrain.zip - the training set, organized in folders that correspond the the appropriatecameramodel label  \ntest.zip - the test set; please see above description of how these files were created  \nsample_submission.csv - a sample submission file in the correct format  \nFiles5391 filesSize11.45 GBTypejpg, tif, csvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "classification",
          "images",
          "cnn",
          "image-forensics",
          "weighted-accuracy"
        ]
      }
    },
    {
      "challenge_name": "spaceship-titanic",
      "description": "Challenge description:\nWelcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination\u2014the torrid 55 Cancri E\u2014the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship's damaged computer system.\n\nHelp save them and change history!\n\nWe highly recommend Titanic - Machine Learning from Disaster to get familiar with the basics of machine learning and Kaggle competitions.\n\nTo get started quickly, feel free to take advantage of this starter notebook.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nPhotos by Joel Filipe, Richard Gatley and Action Vance on Unsplash.\n\nSubmissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.\n\nThe submission format for the competition is a csv file with the following format:\nPassengerId,Transported\n0013_01,False\n0018_01,False\n0019_01,False\n0021_01,False\netc.\n\nWhat is a Getting Started competition?\nGetting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.\nGetting Started competitions are a non-competitive way to get familiar with Kaggle's platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.\n\nHow do I create and manage a team?\nWhen you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the Team page.\nWe've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.\n\nWhat are Notebooks?\nKaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Notebooks support scripts in Python and R, Jupyter Notebooks, and RMarkdown reports. You can visit the Notebooks tab to view all of the publicly shared code for the Spaceship Titanic competition. For more on how to use Notebooks to learn data science, check out our Courses!\n\nWhy did my team disappear from the leaderboard?\nTo keep with the spirit of getting-started competitions, we have implemented a two month rolling window on submissions. Once a submission is more than two months old, it will be invalidated and no longer count towards the leaderboard.\nIf your team has no submissions in the previous two months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.\n\"I worked so hard to get that score! Give it back!\" Read more about our decision to implement a rolling leaderboard here.\n\nHow do I contact Support?\nKaggle does not have a dedicated support team so you'll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you'll want to use the Spaceship Titanic discussion forum).\nSupport is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can't find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\nIf your problem persists or it seems to be effective all participants then please contact us.\n\nAddison Howard, Ashley Chow, and Ryan Holbrook. Spaceship Titanic. https://kaggle.com/competitions/spaceship-titanic, 2022. Kaggle.\n\nCompetition Host: Kaggle\nPrizes & Awards: Does not award Points or Medals\nThis competition runs indefinitely with a rolling leaderboard.\n\nData description:\nSpaceship Titanic: Predict which passengers are transported to an alternate dimension\n\nIn this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n\nFile and Data Field Descriptions\n\ntrain.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\nPassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\nHomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\nCryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\nCabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\nDestination - The planet the passenger will be debarking to.\nAge - The age of the passenger.\nVIP - Whether the passenger has paid for special VIP service during the voyage.\nRoomService,FoodCourt,ShoppingMall,Spa,VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\nName - The first and last names of the passenger.\nTransported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\ntest.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n\nsample_submission.csv - A submission file in the correct format.\nPassengerId - Id for each passenger in the test set.\nTransported - The target. For each passenger, predict either True or False.\n\nLicense Attribution 4.0 International (CC BY 4.0)",
      "docker_challenge_path": "/data/spaceship-titanic",
      "competition_description": "Challenge description:\nWelcome to the year 2912, where your data science skills are needed to solve a cosmic mystery. We've received a transmission from four lightyears away and things aren't looking good.\n\nThe Spaceship Titanic was an interstellar passenger liner launched a month ago. With almost 13,000 passengers on board, the vessel set out on its maiden voyage transporting emigrants from our solar system to three newly habitable exoplanets orbiting nearby stars.\n\nWhile rounding Alpha Centauri en route to its first destination\u2014the torrid 55 Cancri E\u2014the unwary Spaceship Titanic collided with a spacetime anomaly hidden within a dust cloud. Sadly, it met a similar fate as its namesake from 1000 years before. Though the ship stayed intact, almost half of the passengers were transported to an alternate dimension!\n\nTo help rescue crews and retrieve the lost passengers, you are challenged to predict which passengers were transported by the anomaly using records recovered from the spaceship's damaged computer system.\n\nHelp save them and change history!\n\nWe highly recommend Titanic - Machine Learning from Disaster to get familiar with the basics of machine learning and Kaggle competitions.\n\nTo get started quickly, feel free to take advantage of this starter notebook.\n\nIf you want to talk with other users about this competition, come join our Discord! We've got channels for competitions, job postings and career discussions, resources, and socializing with your fellow data scientists. Follow the link here: https://discord.gg/kaggle\n\nPhotos by Joel Filipe, Richard Gatley and Action Vance on Unsplash.",
      "evaluation_metric": "Submissions are evaluated based on their classification accuracy, the percentage of predicted labels that are correct.",
      "dataset_description": "Data description:\nSpaceship Titanic: Predict which passengers are transported to an alternate dimension\n\nIn this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n\nFile and Data Field Descriptions\n\ntrain.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\nPassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\nHomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\nCryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\nCabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\nDestination - The planet the passenger will be debarking to.\nAge - The age of the passenger.\nVIP - Whether the passenger has paid for special VIP service during the voyage.\nRoomService,FoodCourt,ShoppingMall,Spa,VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\nName - The first and last names of the passenger.\nTransported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n\ntest.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n\nsample_submission.csv - A submission file in the correct format.\nPassengerId - Id for each passenger in the test set.\nTransported - The target. For each passenger, predict either True or False.\n\nLicense Attribution 4.0 International (CC BY 4.0)",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "binary classification",
          "tabular",
          "feature engineering",
          "passenger demographics",
          "accuracy"
        ]
      }
    },
    {
      "challenge_name": "stanford-covid-vaccine",
      "description": "Challenge description:\n# OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\n\n## Description\n\nWinning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.\n\nmRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.\n\nResearchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.\n\nThe Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford's School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world's most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.\n\nIn this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!\n\nImproving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.\n\n## Evaluation\n\nSubmissions are scored using MCRMSE, mean columnwise root mean squared error:\n\n$$\\textrm{MCRMSE} = \\frac{1}{N_{t}}\\sum_{j=1}^{N_{t}}\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{ij} - \\hat{y}_{ij})^2}$$\n\nwhere $N_t$ is the number of scored ground truth target columns, and $y$ and $\\hat{y}$ are the actual and predicted values, respectively.\n\nFrom the Data page: There are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.\n\n### Submission File\n\nFor each sample_id in the test set, you must predict targets for each sequence position (seqpos), one per row. If the length of the sequence of an id is, e.g., 107, then you should make 107 predictions. Positions greater than the seq_scored value of a sample are not scored, but still need a value in the solution file.\n\n```\nid_seqpos,reactivity,deg_Mg_pH10,deg_pH10,deg_Mg_50C,deg_50C\nid_00073f8be_0,0.1,0.3,0.2,0.5,0.4\nid_00073f8be_1,0.3,0.2,0.5,0.4,0.2\nid_00073f8be_2,0.5,0.4,0.2,0.1,0.2\netc.\n```\n\n## Timeline\n\nOctober 2, 2020 - Entry deadline. You must accept the competition rules before this date in order to compete.\n\nOctober 2, 2020 - Team Merger deadline. This is the last day participants may join or merge teams.\n\nOctober 6, 2020 - Final submission deadline. Date extended by one day for new data refresh.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n1st Place - $12,000\n\n2nd Place - $8,000\n\n3rd Place - $5,000\n\n## Additional Resources\n\nHow RNA vaccines work, and their issues, featuring Dr. Rhiju Das\nhttps://www.pbs.org/wgbh/nova/video/rna-coronavirus-vaccine/\n\nLaunch of the OpenVaccine challenge\nhttps://scopeblog.stanford.edu/2020/05/20/stanford-biochemist-works-with-gamers-to-develop-covid-19-vaccine/\n\nThe impossibility of mass immunization with current RNA vaccines\nhttps://www.wsj.com/articles/from-freezer-farms-to-jets-logistics-operators-prepare-for-a-covid-19-vaccine-11598639012\n\nCDC prepares for frozen mRNA vaccines\nhttps://www.cdc.gov/vaccines/acip/meetings/downloads/slides-2020-08/COVID-08-Dooling.pdf\n\nEterna, the crowdsourcing platform for RNA design where OpenVaccine's RNA sequences are coming from\nhttps://eternagame.org\n\nHow to build a better vaccine from the comfort of your own web browser\nhttps://medium.com/eternaproject/how-to-build-a-better-vaccine-from-the-comfort-of-your-own-web-browser-233343e0210d\n\n## Citation\n\nRhiju Das, H Wayment-Steele, Do Soon Kim, Christian Choe, Bojan Tunguz, Walter Reade, and Maggie Demkin. OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction. https://kaggle.com/competitions/stanford-covid-vaccine, 2020. Kaggle.\n\n## Competition Host\n\nStanford University\n\nData description:\nOpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\nUrgent need to bring the COVID-19 vaccine to mass production\n\nIn this competition, you will be predicting the degradation rates at various locations along RNA sequence.\nThere are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.\n\nFiles\ntrain.json- the training data\ntest.json- the test set, without any columns associated with the ground truth.\nsample_submission.csv- a sample submission file in the correct format\n\nColumns\nid- An arbitrary identifier for each sample.\nseq_scored- (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of reactivity, deg_* and *_error_* columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\nseq_length- (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of sequence. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\nsequence- (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of A, G, U, and C for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in seq_scored (note: indexed starting at 0).\nstructure- (1x107 string in Train and Public Test, 130 in Private Test) An array of (, ), and . characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.\nreactivity- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likely secondary structure of the RNA sample.\ndeg_pH10- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).\ndeg_Mg_pH10- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\ndeg_50C- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\ndeg_Mg_50C- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n*_error_*- An array of floating point numbers, should have the same length as the corresponding reactivity or deg_* columns, calculated errors in experimental values obtained in reactivity and deg_* columns.\npredicted_loop_type- (1x107 string) Describes the structural context (also referred to as 'loop type') of each character in sequence. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: \n    S: paired \"Stem\"\n    M: Multiloop\n    I: Internal loop\n    B: Bulge\n    H: Hairpin loop\n    E: dangling End\n    X: eXternal loop\nS/N filter- Indicates if the sample passed filters described below in Additional Notes.\n\nAdditional Notes\nAt the beginning of the competition, Stanford scientists have data on 3029 RNA sequences of length 107. For technical reasons, measurements cannot be carried out on the final bases of these RNA sequences, so we have experimental data (ground truth) in 5 conditions for the first 68 bases.\nWe have split out 629 of these 3029 sequences for a public test set to allow for continuous evaluation through the competition, on the public leaderboard. These sequences, in test.json, have been additionally filtered based on three criteria detailed below to ensure that this subset is not dominated by any large cluster of RNA molecules with poor data, which might bias the public leaderboard. The remaining 2400 sequences for which we have data are in train.json.\nFor our final and most important scoring (the Private Leaderboard), Stanford scientists are carrying out measurements on 3005 new RNAs, which have somewhat longer lengths of 130 bases. For these data, we expect to have measurements for the first 91 bases, again missing the ends of the RNA. These sequences constitute another 3005 of the 3634 sequences in test.json.\nFor those interested in how the 629 107-base sequences in test.json were filtered, here were the steps to ensure a diverse and high quality test set for public leaderboard scoring:\n1. Minimum value across all 5 conditions must be greater than -0.5.\n2. Mean signal/noise across all 5 conditions must be greater than 1.0. [Signal/noise is defined as mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]\n3. To help ensure sequence diversity, the resulting sequences were clustered into clusters with less than 50% sequence similarity, and the 629 test set sequences were chosen from clusters with 3 or fewer members. That is, any sequence in the test set should be sequence similar to at most 2 other sequences.\nNote that these filters have not been applied to the 2400 RNAs in the public training data train.json\u2014 some of those measurements have negative values or poor signal-to-noise, or some RNA sequences have near-identical sequences in that set. But we are providing all those data in case competitors can squeeze out more signal.\nThe three filters noted above will also not be applied to Private Test on 3005 sequences.\n[Update as of 18 Sep 2020] After discussion, the three filters noted above will be applied to Private Test on 3005 sequences, and predictions on sequences that do not pass the filters will not be included in scoring.",
      "docker_challenge_path": "/data/stanford-covid-vaccine",
      "competition_description": "## Description\n\nWinning the fight against the COVID-19 pandemic will require an effective vaccine that can be equitably and widely distributed. Building upon decades of research has allowed scientists to accelerate the search for a vaccine against COVID-19, but every day that goes by without a vaccine has enormous costs for the world nonetheless. We need new, fresh ideas from all corners of the world. Could online gaming and crowdsourcing help solve a worldwide pandemic? Pairing scientific and crowdsourced intelligence could help computational biochemists make measurable progress.\n\nmRNA vaccines have taken the lead as the fastest vaccine candidates for COVID-19, but currently, they face key potential limitations. One of the biggest challenges right now is how to design super stable messenger RNA molecules (mRNA). Conventional vaccines (like your seasonal flu shots) are packaged in disposable syringes and shipped under refrigeration around the world, but that is not currently possible for mRNA vaccines.\n\nResearchers have observed that RNA molecules have the tendency to spontaneously degrade. This is a serious limitation--a single cut can render the mRNA vaccine useless. Currently, little is known on the details of where in the backbone of a given RNA is most prone to being affected. Without this knowledge, current mRNA vaccines against COVID-19 must be prepared and shipped under intense refrigeration, and are unlikely to reach more than a tiny fraction of human beings on the planet unless they can be stabilized.\n\nThe Eterna community, led by Professor Rhiju Das, a computational biochemist at Stanford's School of Medicine, brings together scientists and gamers to solve puzzles and invent medicine. Eterna is an online video game platform that challenges players to solve scientific problems such as mRNA design through puzzles. The solutions are synthesized and experimentally tested at Stanford by researchers to gain new insights about RNA molecules. The Eterna community has previously unlocked new scientific principles, made new diagnostics against deadly diseases, and engaged the world's most potent intellectual resources for the betterment of the public. The Eterna community has advanced biotechnology through its contribution in over 20 publications, including advances in RNA biotechnology.\n\nIn this competition, we are looking to leverage the data science expertise of the Kaggle community to develop models and design rules for RNA degradation. Your model will predict likely degradation rates at each base of an RNA molecule, trained on a subset of an Eterna dataset comprising over 3000 RNA molecules (which span a panoply of sequences and structures) and their degradation rates at each position. We will then score your models on a second generation of RNA sequences that have just been devised by Eterna players for COVID-19 mRNA vaccines. These final test sequences are currently being synthesized and experimentally characterized at Stanford University in parallel to your modeling efforts -- Nature will score your models!\n\nImproving the stability of mRNA vaccines was a problem that was being explored before the pandemic but was expected to take many years to solve. Now, we must solve this deep scientific challenge in months, if not weeks, to accelerate mRNA vaccine research and deliver a refrigerator-stable vaccine against SARS-CoV-2, the virus behind COVID-19. The problem we are trying to solve has eluded academic labs, industry R&D groups, and supercomputers, and so we are turning to you. To help, you can join the team of video game players, scientists, and developers at Eterna to unlock the key in our fight against this devastating pandemic.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are scored using MCRMSE, mean columnwise root mean squared error:\n\n$$\\textrm{MCRMSE} = \\frac{1}{N_{t}}\\sum_{j=1}^{N_{t}}\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_{ij} - \\hat{y}_{ij})^2}$$\n\nwhere $N_t$ is the number of scored ground truth target columns, and $y$ and $\\hat{y}$ are the actual and predicted values, respectively.",
      "dataset_description": "Data description:\nOpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction\nUrgent need to bring the COVID-19 vaccine to mass production\n\nIn this competition, you will be predicting the degradation rates at various locations along RNA sequence.\nThere are multiple ground truth values provided in the training data. While the submission format requires all 5 to be predicted, only the following are scored: reactivity, deg_Mg_pH10, and deg_Mg_50C.\n\nFiles\ntrain.json- the training data\ntest.json- the test set, without any columns associated with the ground truth.\nsample_submission.csv- a sample submission file in the correct format\n\nColumns\nid- An arbitrary identifier for each sample.\nseq_scored- (68 in Train and Public Test, 91 in Private Test) Integer value denoting the number of positions used in scoring with predicted values. This should match the length of reactivity, deg_* and *_error_* columns. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\nseq_length- (107 in Train and Public Test, 130 in Private Test) Integer values, denotes the length of sequence. Note that molecules used for the Private Test will be longer than those in the Train and Public Test data, so the size of this vector will be different.\nsequence- (1x107 string in Train and Public Test, 130 in Private Test) Describes the RNA sequence, a combination of A, G, U, and C for each sample. Should be 107 characters long, and the first 68 bases should correspond to the 68 positions specified in seq_scored (note: indexed starting at 0).\nstructure- (1x107 string in Train and Public Test, 130 in Private Test) An array of (, ), and . characters that describe whether a base is estimated to be paired or unpaired. Paired bases are denoted by opening and closing parentheses e.g. (....) means that base 0 is paired to base 5, and bases 1-4 are unpaired.\nreactivity- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likely secondary structure of the RNA sample.\ndeg_pH10- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high pH (pH 10).\ndeg_Mg_pH10- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium in high pH (pH 10).\ndeg_50C- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating without magnesium at high temperature (50 degrees Celsius).\ndeg_Mg_50C- (1x68 vector in Train and Public Test, 1x91 in Private Test) An array of floating point numbers, should have the same length as seq_scored. These numbers are reactivity values for the first 68 bases as denoted in sequence, and used to determine the likelihood of degradation at the base/linkage after incubating with magnesium at high temperature (50 degrees Celsius).\n*_error_*- An array of floating point numbers, should have the same length as the corresponding reactivity or deg_* columns, calculated errors in experimental values obtained in reactivity and deg_* columns.\npredicted_loop_type- (1x107 string) Describes the structural context (also referred to as 'loop type') of each character in sequence. Loop types assigned by bpRNA from Vienna RNAfold 2 structure. From the bpRNA_documentation: \n    S: paired \"Stem\"\n    M: Multiloop\n    I: Internal loop\n    B: Bulge\n    H: Hairpin loop\n    E: dangling End\n    X: eXternal loop\nS/N filter- Indicates if the sample passed filters described below in Additional Notes.\n\nAdditional Notes\nAt the beginning of the competition, Stanford scientists have data on 3029 RNA sequences of length 107. For technical reasons, measurements cannot be carried out on the final bases of these RNA sequences, so we have experimental data (ground truth) in 5 conditions for the first 68 bases.\nWe have split out 629 of these 3029 sequences for a public test set to allow for continuous evaluation through the competition, on the public leaderboard. These sequences, in test.json, have been additionally filtered based on three criteria detailed below to ensure that this subset is not dominated by any large cluster of RNA molecules with poor data, which might bias the public leaderboard. The remaining 2400 sequences for which we have data are in train.json.\nFor our final and most important scoring (the Private Leaderboard), Stanford scientists are carrying out measurements on 3005 new RNAs, which have somewhat longer lengths of 130 bases. For these data, we expect to have measurements for the first 91 bases, again missing the ends of the RNA. These sequences constitute another 3005 of the 3634 sequences in test.json.\nFor those interested in how the 629 107-base sequences in test.json were filtered, here were the steps to ensure a diverse and high quality test set for public leaderboard scoring:\n1. Minimum value across all 5 conditions must be greater than -0.5.\n2. Mean signal/noise across all 5 conditions must be greater than 1.0. [Signal/noise is defined as mean( measurement value over 68 nts )/mean( statistical error in measurement value over 68 nts)]\n3. To help ensure sequence diversity, the resulting sequences were clustered into clusters with less than 50% sequence similarity, and the 629 test set sequences were chosen from clusters with 3 or fewer members. That is, any sequence in the test set should be sequence similar to at most 2 other sequences.\nNote that these filters have not been applied to the 2400 RNAs in the public training data train.json\u2014 some of those measurements have negative values or poor signal-to-noise, or some RNA sequences have near-identical sequences in that set. But we are providing all those data in case competitors can squeeze out more signal.\nThe three filters noted above will also not be applied to Private Test on 3005 sequences.\n[Update as of 18 Sep 2020] After discussion, the three filters noted above will be applied to Private Test on 3005 sequences, and predictions on sequences that do not pass the filters will not be included in scoring.",
      "metadata": {
        "domain": "bioinformatics",
        "keywords": [
          "regression",
          "chemistry",
          "sequence and structure embedding",
          "biology",
          "rmse"
        ]
      }
    },
    {
      "challenge_name": "statoil-iceberg-classifier-challenge",
      "description": "Challenge description:\nStatoil/C-CORE Iceberg Classifier Challenge\nShip or iceberg, can you decide from space?\n\nDrifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.\nCurrently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs. However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite.\nStatoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible.\nIn this competition, you're challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.\n\nSubmissions are evaluated on the log loss between the predicted values and the ground truth.\nFor each id in the test set, you must predict the probability that the image contains an iceberg (a number between 0 and 1). The file should contain a header and have the following format:\nid,is_iceberg\n809385f7,0.57\n535f0cd,0.43\naa99a38,0.9\netc.\n\nSee the Rules Section labeled \"Prizes\" for the terms on receiving prize money.\n1st place - $25,000\n2nd place - $15,000\n3rd place - $10,000\n\nJanuary 16, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nJanuary 16, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nJanuary 23, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nThe remote sensing systems used to detect icebergs are housed on satellites over 600 kilometers above the Earth. The Sentinel-1 satellite constellation is used to monitor Land and Ocean. Orbiting 14 times a day, the satellite captures images of the Earth's surface at a given location, at a given instant in time. The C-Band radar operates at a frequency that \"sees\" through darkness, rain, cloud and even fog. Since it emits its own energy source it can capture images day or night.\nSatellite radar works in much the same way as blips on a ship or aircraft radar. It bounces a signal off an object and records the echo, then that data is translated into an image. An object will appear as a bright spot because it reflects more radar energy than its surroundings, but strong echoes can come from anything solid - land, islands, sea ice, as well as icebergs and ships. The energy reflected back to the radar is referred to as backscatter.\nWhen the radar detects an object, it can't tell an iceberg from a ship or any other solid object. The object needs to be analyzed for certain characteristics - shape, size and brightness - to find that out. The area surrounding the object, in this case ocean, can also be analyzed or modeled. Many things affect the backscatter of the ocean or background area. High winds will generate a brighter background. Conversely, low winds will generate a darker background. The Sentinel-1 satellite is a side looking radar, which means it sees the image area at an angle (incidence angle). Generally, the ocean background will be darker at a higher incidence angle. You also need to consider the radar polarization, which is how the radar transmits and receives the energy. More advanced radars like Sentinel-1, can transmit and receive in the horizontal and vertical plane. Using this, you can get what is called a dual-polarization image.\nFor this contest you will see data with two channels: HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). This can play an important role in the object characteristics, since objects tend to reflect energy differently.\n\nAddison Howard, Brad Elliott, Carl Howell, chardy709@gmail.com, David Wade, Francesco S, Hallgeir, Harald W, Jan Richard Sagli, Kelley Dodge, Knut Sebastian, Mark McDonald, richh55, and Wendy Kan. Statoil/C-CORE Iceberg Classifier Challenge. https://kaggle.com/competitions/statoil-iceberg-classifier-challenge, 2017. Kaggle.\n\nData description:\nStatoil/C-CORE Iceberg Classifier Challenge\nShip or iceberg, can you decide from space?\n\nDataset Description\nIn this competition, you will predict whether an image contains a ship or an iceberg. The labels are provided by human experts and geographic knowledge on the target. All the images are 75x75 images with two bands.\n\nData fields\ntrain.json, test.json\nThe data (train.json, test.json) is presented in json format. The files consist of a list of images, and for each image, you can find the following fields:\nid - the id of the image\nband_1, band_2 - the flattened image data. Each band has 75x75 pixel values in the list, so the list has 5625 elements. Note that these values are not the normal non-negative integers in image files since they have physical meanings - these are float numbers with unit being dB. Band 1 and Band 2 are signals characterized by radar backscatter produced from different polarizations at a particular incidence angle. The polarizations correspond to HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). More background on the satellite imagery can be found here.\ninc_angle - the incidence angle of which the image was taken. Note that this field has missing data marked as \"na\", and those images with \"na\" incidence angles are all in the training data to prevent leakage.\nis_iceberg - the target variable, set to 1 if it is an iceberg, and 0 if it is a ship. This field only exists in train.json.\n\nPlease note that we have included machine-generated images in the test set to prevent hand labeling. They are excluded in scoring.\n\nsample_submission.csv\nThe submission file in the correct format:\nid - the id of the image\nis_iceberg - your predicted probability that this image is iceberg.\n\nFiles\n3 files\nSize 302.1 MB\nType 7z\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/statoil-iceberg-classifier-challenge",
      "competition_description": "Challenge description:\nStatoil/C-CORE Iceberg Classifier Challenge\nShip or iceberg, can you decide from space?\n\nDrifting icebergs present threats to navigation and activities in areas such as offshore of the East Coast of Canada.\nCurrently, many institutions and companies use aerial reconnaissance and shore-based support to monitor environmental conditions and assess risks from icebergs. However, in remote areas with particularly harsh weather, these methods are not feasible, and the only viable monitoring option is via satellite.\nStatoil, an international energy company operating worldwide, has worked closely with companies like C-CORE. C-CORE have been using satellite data for over 30 years and have built a computer vision based surveillance system. To keep operations safe and efficient, Statoil is interested in getting a fresh new perspective on how to use machine learning to more accurately detect and discriminate against threatening icebergs as early as possible.\nIn this competition, you're challenged to build an algorithm that automatically identifies if a remotely sensed target is a ship or iceberg. Improvements made will help drive the costs down for maintaining safe working conditions.",
      "evaluation_metric": "Submissions are evaluated on the log loss between the predicted values and the ground truth.",
      "dataset_description": "Data description:\nStatoil/C-CORE Iceberg Classifier Challenge\nShip or iceberg, can you decide from space?\n\nDataset Description\nIn this competition, you will predict whether an image contains a ship or an iceberg. The labels are provided by human experts and geographic knowledge on the target. All the images are 75x75 images with two bands.\n\nData fields\ntrain.json, test.json\nThe data (train.json, test.json) is presented in json format. The files consist of a list of images, and for each image, you can find the following fields:\nid - the id of the image\nband_1, band_2 - the flattened image data. Each band has 75x75 pixel values in the list, so the list has 5625 elements. Note that these values are not the normal non-negative integers in image files since they have physical meanings - these are float numbers with unit being dB. Band 1 and Band 2 are signals characterized by radar backscatter produced from different polarizations at a particular incidence angle. The polarizations correspond to HH (transmit/receive horizontally) and HV (transmit horizontally and receive vertically). More background on the satellite imagery can be found here.\ninc_angle - the incidence angle of which the image was taken. Note that this field has missing data marked as \"na\", and those images with \"na\" incidence angles are all in the training data to prevent leakage.\nis_iceberg - the target variable, set to 1 if it is an iceberg, and 0 if it is a ship. This field only exists in train.json.\n\nPlease note that we have included machine-generated images in the test set to prevent hand labeling. They are excluded in scoring.\n\nsample_submission.csv\nThe submission file in the correct format:\nid - the id of the image\nis_iceberg - your predicted probability that this image is iceberg.\n\nFiles\n3 files\nSize 302.1 MB\nType 7z\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "binary_classification",
          "images",
          "cnn",
          "remote_sensing",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "store-sales-time-series-forecasting",
      "description": "Challenge description:\n# Store Sales - Time Series Forecasting\n\n## Goal of the Competition\nIn this \"getting started\" competition, you'll use time-series forecasting to forecast store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer.\n\nSpecifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n\nWe highly recommend the Time Series course, which walks you through how to make your first submission. The lessons in this course are inspired by winning solutions from past Kaggle time series forecasting competitions.\n\n## Context\nForecasts aren't just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand\u2014a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.\n\nCurrent subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing.\n\n## Potential Impact\nIf successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.\n\n## Evaluation\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as:\n$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n\nwhere:\n- $n$ is the total number of instances,\n- $\\hat{y}_i$ is the predicted value of the target for instance (i),\n- $y_i$ is the actual value of the target for instance (i), and,\n- $\\log$ is the natural logarithm.\n\n## Submission File\nFor each id in the test set, you must predict a value for the sales variable. The file should contain a header and have the following format:\n```\nid,sales\n3000888,0.0\n3000889,0.0\n3000890,0.0\n3000891,0.0\n3000892,0.0\netc.\n```\n\n## Frequently Asked Questions\n\n### What is a Getting Started competition?\nGetting Started competitions were created by Kaggle data scientists for people who have little to no machine learning background. They are a great place to begin if you are new to data science or just finished a MOOC and want to get involved in Kaggle.\n\nGetting Started competitions are a non-competitive way to get familiar with Kaggle's platform, learn basic machine learning concepts, and start meeting people in the community. They have no cash prize and are on a rolling timeline.\n\n### What's the difference between a private and public leaderboard?\nIn this competition, because it is a Getting Started competition, there is no difference. We're scoring the entire test set on the Public Leaderboard. And we will refresh the competition every three months, so the Private Leaderboard is irrelevant.\n\nFor non-Getting Started Kaggle competitions, there is the concept of a public and private leaderboard to prevent participants from \"overfitting\" to the leaderboard. If your model is \"overfit\" to a dataset then it is not generalizable outside of the dataset you trained it on. This means that your model would have low accuracy on another sample of data taken from a similar dataset.\n\n### How do I create and manage a team?\nWhen you accept the competition rules, a team will be created for you. You can invite others to your team, accept a merger with another team, and update basic information like team name by going to the Team page.\n\nWe've heard from many Kagglers that teaming up is the best way to learn new skills AND have fun. If you don't have a teammate already, consider asking if anyone wants to team up in the discussion forum.\n\n### What are Notebooks?\nKaggle Notebooks is a cloud computational environment that enables reproducible and collaborative analysis. Kernels supports scripts in R and Python, Jupyter Notebooks, and RMarkdown reports. Go to the Notebooks tab to view all of the publicly shared code on this competition. For more on how to use Notebooks to learn data science, visit Kaggle's Learn Courses.\n\n### How do I make a submission?\nIn this code competition, your submission.csv file must be generated as an output from a Kaggle notebook. To submit from a notebook, you should:\n1. \"Commit\" / \"Save & Run\" a notebook that generates a submission.csv containing your predictions in the correct format (requirements described above).\n2. Go to the \"Output\" section of the notebook in viewer mode.\n3. The submission.csv can then be submitted to the competition via the \"Submit to Competition\" button.\n\nThis is not a code competition with a hidden test set. And there is no private leaderboard. Therefore, your code will not be re-run on a private test set, and the test set provided on the Data page is the full set of observations for which your submission.csv must make predictions.\n\n### Why did my team disappear from the leaderboard?\nTo keep with the spirit of getting-started competitions, we have implemented a three month rolling window on submissions. Once a submission is more than three months old, it will be invalidated and no longer count towards the leaderboard.\n\nIf your team has no submissions in the previous three months, the team will also drop from the leaderboard. This will keep the leaderboard at a manageable size, freshen it up, and prevent newcomers from getting lost in a sea of abandoned scores.\n\n\"I worked so hard to get that score! Give it back!\" Read more about our decision to implement rolling leaderboards, covered in the Titanic Getting Started Competition forum here.\n\n### How do I contact Support?\nKaggle does not have a dedicated support team so you'll typically find that you receive a response more quickly by asking your question in the appropriate forum. (For this competition, you'll want to use this competition's discussion forum.\n\nSupport is only able to help with issues that are being experienced by all participants. Before contacting support, please check the discussion forum for information on your problem. If you can't find it, you can post your problem in the forum so a fellow participant or a Kaggle team member can provide help. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!\n\nIf your problem persists or it seems to be effective all participants then please contact us.\n\n## Citation\nAlexis Cook, DanB, inversion, and Ryan Holbrook. Store Sales - Time Series Forecasting. https://kaggle.com/competitions/store-sales-time-series-forecasting, 2021. Kaggle.\n\n## Competition Host\nKaggle\n\n## Prizes & Awards\nDoes not award Points or Medals\n\n## Participation\n20,498 Entrants\n591 Participants\n571 Teams\n2,282 Submissions\n\n## Tags\nTabular, Time Series Analysis, Beginner, Root Mean Squared Logarithmic Error\n\nData description:\nStore Sales - Time Series Forecasting  \nUse machine learning to predict grocery sales  \n\nIn this competition, you will predict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models.  \nSubject to Competition Rules  \n\nFile Descriptions and Data Field Information  \ntrain.csv  \nThe training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.  \nstore_nbr identifies the store at which the products are sold.  \nfamily identifies the type of product sold.  \nsales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).  \nonpromotion gives the total number of items in a product family that were being promoted at a store at a given date.  \n\ntest.csv  \nThe test data, having the same features as the training data. You will predict the target sales for the dates in this file.  \nThe dates in the test data are for the 15 days after the last date in the training data.  \n\nsample_submission.csv  \nA sample submission file in the correct format.  \n\nstores.csv  \nStore metadata, including city, state, type, and cluster.  \ncluster is a grouping of similar stores.  \n\noil.csv  \nDaily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)  \n\nholidays_events.csv  \nHolidays and Events, with metadata  \nNOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.  \nAdditional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).  \n\nAdditional Notes  \nWages in the public sector are paid every two weeks on the 15th and on the last day of the month. Supermarket sales could be affected by this.  \nA magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.",
      "docker_challenge_path": "/data/store-sales-time-series-forecasting",
      "competition_description": "# Store Sales - Time Series Forecasting\n\n## Goal of the Competition\nIn this \"getting started\" competition, you'll use time-series forecasting to forecast store sales on data from Corporaci\u00f3n Favorita, a large Ecuadorian-based grocery retailer.\n\nSpecifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores. You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n\nWe highly recommend the Time Series course, which walks you through how to make your first submission. The lessons in this course are inspired by winning solutions from past Kaggle time series forecasting competitions.\n\n## Context\nForecasts aren't just for meteorologists. Governments forecast economic growth. Scientists attempt to predict the future population. And businesses forecast product demand\u2014a common task of professional data scientists. Forecasts are especially relevant to brick-and-mortar grocery stores, which must dance delicately with how much inventory to buy. Predict a little over, and grocers are stuck with overstocked, perishable goods. Guess a little under, and popular items quickly sell out, leading to lost revenue and upset customers. More accurate forecasting, thanks to machine learning, could help ensure retailers please customers by having just enough of the right products at the right time.\n\nCurrent subjective forecasting methods for retail have little data to back them up and are unlikely to be automated. The problem becomes even more complex as retailers add new locations with unique needs, new products, ever-transitioning seasonal tastes, and unpredictable product marketing.\n\n## Potential Impact\nIf successful, you'll have flexed some new skills in a real world example. For grocery stores, more accurate forecasting can decrease food waste related to overstocking and improve customer satisfaction. The results of this ongoing competition, over time, might even ensure your local store has exactly what you need the next time you shop.",
      "evaluation_metric": "## Evaluation\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as:\n$$\\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2}$$\n\nwhere:\n- $n$ is the total number of instances,\n- $\\hat{y}_i$ is the predicted value of the target for instance (i),\n- $y_i$ is the actual value of the target for instance (i), and,\n- $\\log$ is the natural logarithm.",
      "dataset_description": "Store Sales - Time Series Forecasting  \nUse machine learning to predict grocery sales  \n\nIn this competition, you will predict sales for the thousands of product families sold at Favorita stores located in Ecuador. The training data includes dates, store and product information, whether that item was being promoted, as well as the sales numbers. Additional files include supplementary information that may be useful in building your models.  \nSubject to Competition Rules  \n\nFile Descriptions and Data Field Information  \ntrain.csv  \nThe training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.  \nstore_nbr identifies the store at which the products are sold.  \nfamily identifies the type of product sold.  \nsales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).  \nonpromotion gives the total number of items in a product family that were being promoted at a store at a given date.  \n\ntest.csv  \nThe test data, having the same features as the training data. You will predict the target sales for the dates in this file.  \nThe dates in the test data are for the 15 days after the last date in the training data.  \n\nsample_submission.csv  \nA sample submission file in the correct format.  \n\nstores.csv  \nStore metadata, including city, state, type, and cluster.  \ncluster is a grouping of similar stores.  \n\noil.csv  \nDaily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)  \n\nholidays_events.csv  \nHolidays and Events, with metadata  \nNOTE: Pay special attention to the transferred column. A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government. A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is Transfer. For example, the holiday Independencia de Guayaquil was transferred from 2012-10-09 to 2012-10-12, which means it was celebrated on 2012-10-12. Days that are type Bridge are extra days that are added to a holiday (e.g., to extend the break across a long weekend). These are frequently made up by the type Work Day which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the Bridge.  \nAdditional holidays are days added a regular calendar holiday, for example, as typically happens around Christmas (making Christmas Eve a holiday).  \n\nAdditional Notes  \nWages in the public sector are paid every two weeks on the 15th and on the last day of the month. Supermarket sales could be affected by this.  \nA magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake.",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "forecasting",
          "time_series",
          "feature engineering",
          "retail",
          "rmse"
        ]
      }
    },
    {
      "challenge_name": "talkingdata-adtracking-fraud-detection",
      "description": "Challenge description:\nTalkingData AdTracking Fraud Detection Challenge\nCan you detect fraudulent click traffic for mobile app ads?\n\nFraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fraudulent traffic.\nTalkingData, China\u2019s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user\u2019s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.\nWhile successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you\u2019re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\nFor each click_id in the test set, you must predict a probability for the target is_attributed variable. The file should contain a header and have the following format:\nclick_id,is_attributed\n1,0.003\n2,0.001\n3,0.000\netc.\n\n1st Place - $12,500\n2nd Place - $7,500\n3rd Place - $5,000\n\nApril 30, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\nApril 30, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\nMay 7, 2018 - Final submission deadline.\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nAaron Yin, Jack Kleinman, Tony Yana, Walter Reade, and Julia Elliott. TalkingData AdTracking Fraud Detection Challenge. https://kaggle.com/competitions/talkingdata-adtracking-fraud-detection, 2018. Kaggle.\n\nData description:\nTalkingData AdTracking Fraud Detection Challenge\n\nCan you detect fraudulent click traffic for mobile app ads?\n\nFor this competition, your objective is to predict whether a user will download an app after clicking a mobile app advertisement.\n\nDataset Description\n\nFile descriptions\ntrain.csv - the training set\ntrain_sample.csv - 100,000 randomly-selected rows of training data, to inspect data before downloading full set\ntest.csv - the test set\nsampleSubmission.csv - a sample submission file in the correct format\nUPDATE: test_supplement.csv - This is a larger test set that was unintentionally released at the start of the competition. It is not necessary to use this data, but it is permitted to do so. The official test data is a subset of this data.\n\nData fields\nEach row of the training data contains a click record, with the following features.\nip: ip address of click.\napp: app id for marketing.\ndevice: devicetypeid of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)\nos: os version id of user mobile phone\nchannel: channel id of mobile ad publisher\nclick_time: timestamp of click (UTC)\nattributed_time: if user download the app for after clicking an ad, this is the time of the app download\nis_attributed: the target that is to be predicted, indicating the app was downloaded\n\nNote that ip, app, device, os, and channel are encoded.\n\nThe test data is similar, with the following differences:\nclick_id: reference for making predictions\nis_attributed: not included\n\nFiles: 5 files, Size: 11.27 GB, Type: csv, License: Subject to Competition Rules",
      "docker_challenge_path": "/data/talkingdata-adtracking-fraud-detection",
      "competition_description": "Challenge description:\nTalkingData AdTracking Fraud Detection Challenge\nCan you detect fraudulent click traffic for mobile app ads?\n\nFraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fraudulent traffic.\nTalkingData, China\u2019s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user\u2019s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.\nWhile successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you\u2019re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!",
      "evaluation_metric": "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.",
      "dataset_description": "Data description:\nTalkingData AdTracking Fraud Detection Challenge\n\nCan you detect fraudulent click traffic for mobile app ads?\n\nFor this competition, your objective is to predict whether a user will download an app after clicking a mobile app advertisement.\n\nDataset Description\n\nFile descriptions\ntrain.csv - the training set\ntrain_sample.csv - 100,000 randomly-selected rows of training data, to inspect data before downloading full set\ntest.csv - the test set\nsampleSubmission.csv - a sample submission file in the correct format\nUPDATE: test_supplement.csv - This is a larger test set that was unintentionally released at the start of the competition. It is not necessary to use this data, but it is permitted to do so. The official test data is a subset of this data.\n\nData fields\nEach row of the training data contains a click record, with the following features.\nip: ip address of click.\napp: app id for marketing.\ndevice: devicetypeid of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)\nos: os version id of user mobile phone\nchannel: channel id of mobile ad publisher\nclick_time: timestamp of click (UTC)\nattributed_time: if user download the app for after clicking an ad, this is the time of the app download\nis_attributed: the target that is to be predicted, indicating the app was downloaded\n\nNote that ip, app, device, os, and channel are encoded.\n\nThe test data is similar, with the following differences:\nclick_id: reference for making predictions\nis_attributed: not included\n\nFiles: 5 files, Size: 11.27 GB, Type: csv, License: Subject to Competition Rules",
      "metadata": {
        "domain": "machine_learning",
        "keywords": [
          "classification",
          "tabular",
          "feature engineering",
          "ad fraud",
          "auc"
        ]
      }
    },
    {
      "challenge_name": "tensorflow-speech-recognition-challenge",
      "description": "Challenge description:\n# TensorFlow Speech Recognition Challenge\n\nCan you build an algorithm that understands simple speech commands?\n\nWe might be on the verge of too many screens. It seems like everyday, new versions of common objects are \"re-invented\" with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.\n\nBut, for independent makers and entrepreneurs, it's hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people.\n\nIn this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility.\n\n## Evaluation\n\nSubmissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.\n\nNote: There are only 12 possible labels for the Test set: yes, no, up, down, left, right, on, off, stop, go, silence, unknown.\n\nThe unknown label should be used for a command that is not one of the first 10 labels or that is not silence.\n\nFor audio clip in the test set, you must predict the correct label. The submission file should contain a header and have the following format:\n```\nfname,label\nclip_000044442.wav,silence\nclip_0000adecb.wav,left\nclip_0000d4322.wav,unknown\n```\n\n## Prizes\n\nLeaderboard Prizes:\n- 1st place - $8,000\n- 2nd place - $6,000\n- 3rd place - $3,000\n- Special TensorFlow Prize: $8,000\n\nUPDATE 01/17/2018: Information about being considered for this prize can be found on this forum post.\n\nThe goal of the special prize is to encourage contestants to create a model that can be useful in practice to recognize commands on a Raspberry Pi 3. In order to do this, the models must:\n- Be runnable as frozen TensorFlow GraphDef files with no additional dependencies beyond TensorFlow 1.4.\n- Be small in size (below 5,000,000 bytes).\n- Have a standard set of inputs and outputs:\n  - decoded_sample_data:0, taking a [16000, 1] float tensor as input, representing the audio PCM-encoded data.\n  - decoded_sample_data:1, taking a scalar [] int32 tensor as input, representing the sample rate, which must be the value 16000.\n  - labels_softmax:0, a [12] float tensor representing the probabilities of each class label as an output, from zero to one.\n- Run in less than 200ms on a stock Raspberry Pi 3 running Raspbian GNU/Linux 8 (Jessie), with no overclocking.\n- Must come with code to train the model, which must be license-compatible with TensorFlow (Apache), and be submittable through Google's CLA to the TensorFlow project.\n\nEligibility for the Special Prize will be qualified by the host by re-running your solution within his environment. The model evaluated must match the one generating the predictions scored on the leaderboard. At the close of the competition, you will have the ability to self-select for consideration as a special prize winner. Due to possible inherent discrepancies in environments, it's recommended that you target a latency closer to 175ms or faster. For calibration purposes, the host has provided:\n\nTo evaluate the latency of each model, we'll be running the following command:\n```\n./benchmark_model --graph=<Your Graph> --input_layer=\"decoded_sample_data:0,decoded_sample_data:1\" --input_layer_shape=\"16000,1:\" --input_layer_type=\"float,int32\" --input_layer_values=\":16000\" --output_layer=\"labels_softmax:0\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true\n```\n\nThe result will be the 'avg=' value shown in the line with the prefix 'Timings (microseconds)' near the end of the logging. This represents the mean time in microseconds of repeated runs by the command. To be eligible for the prize, the result must be below 200,000 microseconds (200 milliseconds). Because of variability in timings, I recommend that you aim for 175ms to have a margin of safety.\n\nTo reproduce the process on your own Raspberry Pi 3, here's how to download the benchmark binary and run it on a pretrained graph produced by the training tutorial:\n```\ncurl -O https://storage.googleapis.com/download.tensorflow.org/models/speech_commands_v0.01.zip\nunzip speech_commands_v0.01.zip\n\ncurl -O https://storage.googleapis.com/download.tensorflow.org/deps/pi/2017_10_07/benchmark_model\nchmod +x benchmark_model\n./benchmark_model --graph=conv_actions_frozen.pb --input_layer=\"decoded_sample_data:0,decoded_sample_data:1\" --input_layer_shape=\"16000,1:\" --input_layer_type=\"float,int32\" --input_layer_values=\":16000\" --output_layer=\"labels_softmax:0\" --show_run_order=false --show_time=false --show_memory=false --show_summary=true --show_flops=true\n```\n\n## Timeline\n\n- January 9, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\n- January 9, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\n- January 16, 2018 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Tutorials & More Info\n\nThe host has provided the following resources:\n- Google Research Blog Post announcing the Speech Commands Dataset. Note that much of what is provided as part of the training set is already public. However, the test set is not.\n- TensorFlow Audio Recognition Tutorial\n- Link to purchase Raspberry Pi 3 on Amazon. This will be at your own expense.\n- Also review the Prizes tab for details and tools for how the special prize will be evaluated.\n\n## Citation\n\ninversion, Julia Elliott, Mark McDonald, Pete Warden, and Raziel. TensorFlow Speech Recognition Challenge. https://kaggle.com/competitions/tensorflow-speech-recognition-challenge, 2017. Kaggle.\n\n## Competition Host\n\nGoogle Brain\n\nData description:\n# TensorFlow Speech Recognition Challenge\n\nCan you build an algorithm that understands simple speech commands?\n\n## Dataset Description\n\n### File descriptions\n- **train.7z**: Contains a few informational files and a folder of audio files. The audio folder contains subfolders with 1 second clips of voice commands, with the folder name being the label of the audio clip. There are more labels that should be predicted. The labels you will need to predict in Test are yes, no, up, down, left, right, on, off, stop, go. Everything else should be considered either unknown or silence. The folder_background_noise_ contains longer clips of \"silence\" that you can break up and use as training input.\n\nThe files contained in the training audio are not uniquely named across labels, but they are unique if you include the label folder. For example, 00f0204f_nohash_0.wav is found in 14 folders, but that file is a different speech command in each folder.\n\nThe files are named so the first element is the subject id of the person who gave the voice command, and the last element indicated repeated commands. Repeated commands are when the subject repeats the same word multiple times. Subject id is not provided for the test data, and you can assume that the majority of commands in the test data were from subjects not seen in train.\n\nYou can expect some inconsistencies in the properties of the training data (e.g., length of the audio).\n\n- **test.7z**: Contains an audio folder with 150,000+ files in the format clip_000044442.wav. The task is to predict the correct label. Not all of the files are evaluated for the leaderboard score.\n\n- **sample_submission.csv**: A sample submission file in the correct format.\n\n- **link_to_gcp_credits_form.txt**: Provides the URL to request $500 in GCP credits, provided to the first 500 requestors. Please see this clarification on credit qualification.\n\n### Speech Commands Data Set v0.01\nThis is a set of one-second .wav audio files, each containing a single spoken English word. These words are from a small set of commands, and are spoken by a variety of different speakers. The audio files are organized into folders based on the word they contain, and this data set is designed to help train simple machine learning models.\n\nIt's licensed under the Creative Commons BY 4.0 license. See the LICENSE file in this folder for full details. Its original location was at http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz.\n\n### History\nThis is version 0.01 of the data set containing 64,727 audio files, released on August 3rd 2017.\n\n### Collection\nThe audio files were collected using crowdsourcing, see aiyprojects.withgoogle.com/open_speech_recording for some of the open source audio collection code we used (and please consider contributing to enlarge this data set). The goal was to gather examples of people speaking single-word commands, rather than conversational sentences, so they were prompted for individual words over the course of a five minute session.\n\nTwenty core command words were recorded, with most speakers saying each of them five times. The core words are \"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\".\n\nTo help distinguish unrecognized words, there are also ten auxiliary words, which most speakers only said once. These include \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", and \"Wow\".\n\n### Organization\nThe files are organized into folders, with each directory name labelling the word that is spoken in all the contained audio files. No details were kept of any of the participants age, gender, or location, and random ids were assigned to each individual. These ids are stable though, and encoded in each file name as the first part before the underscore.\n\nIf a participant contributed multiple utterances of the same word, these are distinguished by the number at the end of the file name. For example, the file path happy/3cfc6b3a_nohash_2.wav indicates that the word spoken was \"happy\", the speaker's id was \"3cfc6b3a\", and this is the third utterance of that word by this speaker in the data set.\n\nThe 'nohash' section is to ensure that all the utterances by a single speaker are sorted into the same training partition, to keep very similar repetitions from giving unrealistically optimistic evaluation scores.\n\n### Partitioning\nThe audio clips haven't been separated into training, test, and validation sets explicitly, but by convention a hashing function is used to stably assign each file to a set. Here's some Python code demonstrating how a complete file path and the desired validation and test set sizes (usually both 10%) are used to assign a set:\n\n```\nMAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\ndef which_set(filename, validation_percentage, testing_percentage):\n    \"\"\"Determines which data partition the file should belong to.\n\n    We want to keep files in the same training, validation, or testing sets even\n    if new ones are added over time. This makes it less likely that testing\n    samples will accidentally be reused in training when long runs are restarted\n    for example. To keep this stability, a hash of the filename is taken and used\n    to determine which set it should belong to. This determination only depends on\n    the name and the set proportions, so it won't change as other files are added.\n\n    It's also useful to associate particular files as related (for example words\n    spoken by the same person), so anything after '_nohash_' in a filename is\n    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n    'bobby_nohash_1.wav' are always in the same set, for example.\n\n    Args:\n        filename: File path of the data sample.\n        validation_percentage: How much of the data set to use for validation.\n        testing_percentage: How much of the data set to use for testing.\n\n    Returns:\n        String, one of 'training', 'validation', or 'testing'.\n    \"\"\"\n    base_name = os.path.basename(filename)\n    # We want to ignore anything after '_nohash_' in the file name when\n    # deciding which set to put a wav in, so the data set creator has a way of\n    # grouping wavs that are close variations of each other.\n    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n    # This looks a bit magical, but we need to decide whether this file should\n    # go into the training, testing, or validation sets, and we want to keep\n    # existing files in the same set even if more files are subsequently\n    # added.\n    # To do that, we need a stable way of deciding based on just the file name\n    # itself, so we do a hash of that and then use that to generate a\n    # probability value that we use to assign it.\n    hash_name_hashed = hashlib.sha1(hash_name.encode()).hexdigest()\n    percentage_hash = ((int(hash_name_hashed, 16) %\n                        (MAX_NUM_WAVS_PER_CLASS + 1)) *\n                       (100.0 / MAX_NUM_WAVS_PER_CLASS))\n    if percentage_hash < validation_percentage:\n        result = 'validation'\n    elif percentage_hash < (testing_percentage + validation_percentage):\n        result = 'testing'\n    else:\n        result = 'training'\n    return result\n```\n\nThe results of running this over the current set are included in this archive as validation_list.txt and testing_list.txt. These text files contain the paths to all the files in each set, with each path on a new line. Any files that aren't in either of these lists can be considered to be part of the training set.\n\n### Processing\nThe original audio files were collected in uncontrolled locations by people around the world. We requested that they do the recording in a closed room for privacy reasons, but didn't stipulate any quality requirements. This was by design, since we wanted examples of the sort of speech data that we're likely to encounter in consumer and robotics applications, where we don't have much control over the recording equipment or environment.\n\nThe data was captured in a variety of formats, for example Ogg Vorbis encoding for the web app, and then converted to a 16-bit little-endian PCM-encoded WAVE file at a 16000 sample rate. The audio was then trimmed to a one second length to align most utterances, using the extract_loudest_section tool. The audio files were then screened for silence or incorrect words, and arranged into folders by label.\n\n### Background Noise\nTo help train networks to cope with noisy environments, it can be helpful to mix in realistic background audio. The _background_noise_ folder contains a set of longer audio clips that are either recordings or mathematical simulations of noise. For more details, see the _background_noise_/README.md.\n\n### Citations\nIf you use the Speech Commands dataset in your work, please cite it as:\n\nAPA-style citation: \"Warden P. Speech Commands: A public dataset for single-word speech recognition, 2017. Available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\".\n\nBibTeX:\n```\n@article{speechcommands, \n    title={Speech Commands: A public dataset for single-word speech recognition.}, \n    author={Warden, Pete}, \n    journal={Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}, \n    year={2017}\n}\n```\n\n### Credits\nMassive thanks are due to everyone who donated recordings to this data set, I'm very grateful. I also couldn't have put this together without the help and support of Billy Rutledge, Rajat Monga, Raziel Alvarez, Brad Krueger, Barbara Petit, Gursheesh Kour, and all the AIY and TensorFlow teams.\n\nPete Warden, petewarden@google.com\n\nFiles: 4 files\nSize: 3.76 GB\nType: 7z, txt\nLicense: Subject to Competition Rules",
      "docker_challenge_path": "/data/tensorflow-speech-recognition-challenge",
      "competition_description": "Challenge description:\n# TensorFlow Speech Recognition Challenge\n\nCan you build an algorithm that understands simple speech commands?\n\nWe might be on the verge of too many screens. It seems like everyday, new versions of common objects are \"re-invented\" with built-in wifi and bright touchscreens. A promising antidote to our screen addiction are voice interfaces.\n\nBut, for independent makers and entrepreneurs, it's hard to build a simple speech detector using free, open data and code. Many voice recognition datasets require preprocessing before a neural network model can be built on them. To help with this, TensorFlow recently released the Speech Commands Datasets. It includes 65,000 one-second long utterances of 30 short words, by thousands of different people.\n\nIn this competition, you're challenged to use the Speech Commands Dataset to build an algorithm that understands simple spoken commands. By improving the recognition accuracy of open-sourced voice interface tools, we can improve product effectiveness and their accessibility.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on Multiclass Accuracy, which is simply the average number of observations with the correct label.\n\nNote: There are only 12 possible labels for the Test set: yes, no, up, down, left, right, on, off, stop, go, silence, unknown.\n\nThe unknown label should be used for a command that is not one of the first 10 labels or that is not silence.",
      "dataset_description": "## Dataset Description\n\n### File descriptions\n- **train.7z**: Contains a few informational files and a folder of audio files. The audio folder contains subfolders with 1 second clips of voice commands, with the folder name being the label of the audio clip. There are more labels that should be predicted. The labels you will need to predict in Test are yes, no, up, down, left, right, on, off, stop, go. Everything else should be considered either unknown or silence. The folder_background_noise_ contains longer clips of \"silence\" that you can break up and use as training input.\n\nThe files contained in the training audio are not uniquely named across labels, but they are unique if you include the label folder. For example, 00f0204f_nohash_0.wav is found in 14 folders, but that file is a different speech command in each folder.\n\nThe files are named so the first element is the subject id of the person who gave the voice command, and the last element indicated repeated commands. Repeated commands are when the subject repeats the same word multiple times. Subject id is not provided for the test data, and you can assume that the majority of commands in the test data were from subjects not seen in train.\n\nYou can expect some inconsistencies in the properties of the training data (e.g., length of the audio).\n\n- **test.7z**: Contains an audio folder with 150,000+ files in the format clip_000044442.wav. The task is to predict the correct label. Not all of the files are evaluated for the leaderboard score.\n\n- **sample_submission.csv**: A sample submission file in the correct format.\n\n- **link_to_gcp_credits_form.txt**: Provides the URL to request $500 in GCP credits, provided to the first 500 requestors. Please see this clarification on credit qualification.\n\n### Speech Commands Data Set v0.01\nThis is a set of one-second .wav audio files, each containing a single spoken English word. These words are from a small set of commands, and are spoken by a variety of different speakers. The audio files are organized into folders based on the word they contain, and this data set is designed to help train simple machine learning models.\n\nIt's licensed under the Creative Commons BY 4.0 license. See the LICENSE file in this folder for full details. Its original location was at http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz.\n\n### History\nThis is version 0.01 of the data set containing 64,727 audio files, released on August 3rd 2017.\n\n### Collection\nThe audio files were collected using crowdsourcing, see aiyprojects.withgoogle.com/open_speech_recording for some of the open source audio collection code we used (and please consider contributing to enlarge this data set). The goal was to gather examples of people speaking single-word commands, rather than conversational sentences, so they were prompted for individual words over the course of a five minute session.\n\nTwenty core command words were recorded, with most speakers saying each of them five times. The core words are \"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Off\", \"Stop\", \"Go\", \"Zero\", \"One\", \"Two\", \"Three\", \"Four\", \"Five\", \"Six\", \"Seven\", \"Eight\", and \"Nine\".\n\nTo help distinguish unrecognized words, there are also ten auxiliary words, which most speakers only said once. These include \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", and \"Wow\".\n\n### Organization\nThe files are organized into folders, with each directory name labelling the word that is spoken in all the contained audio files. No details were kept of any of the participants age, gender, or location, and random ids were assigned to each individual. These ids are stable though, and encoded in each file name as the first part before the underscore.\n\nIf a participant contributed multiple utterances of the same word, these are distinguished by the number at the end of the file name. For example, the file path happy/3cfc6b3a_nohash_2.wav indicates that the word spoken was \"happy\", the speaker's id was \"3cfc6b3a\", and this is the third utterance of that word by this speaker in the data set.\n\nThe 'nohash' section is to ensure that all the utterances by a single speaker are sorted into the same training partition, to keep very similar repetitions from giving unrealistically optimistic evaluation scores.\n\n### Partitioning\nThe audio clips haven't been separated into training, test, and validation sets explicitly, but by convention a hashing function is used to stably assign each file to a set. Here's some Python code demonstrating how a complete file path and the desired validation and test set sizes (usually both 10%) are used to assign a set:\n\n```\nMAX_NUM_WAVS_PER_CLASS = 2**27 - 1  # ~134M\ndef which_set(filename, validation_percentage, testing_percentage):\n    \"\"\"Determines which data partition the file should belong to.\n\n    We want to keep files in the same training, validation, or testing sets even\n    if new ones are added over time. This makes it less likely that testing\n    samples will accidentally be reused in training when long runs are restarted\n    for example. To keep this stability, a hash of the filename is taken and used\n    to determine which set it should belong to. This determination only depends on\n    the name and the set proportions, so it won't change as other files are added.\n\n    It's also useful to associate particular files as related (for example words\n    spoken by the same person), so anything after '_nohash_' in a filename is\n    ignored for set determination. This ensures that 'bobby_nohash_0.wav' and\n    'bobby_nohash_1.wav' are always in the same set, for example.\n\n    Args:\n        filename: File path of the data sample.\n        validation_percentage: How much of the data set to use for validation.\n        testing_percentage: How much of the data set to use for testing.\n\n    Returns:\n        String, one of 'training', 'validation', or 'testing'.\n    \"\"\"\n    base_name = os.path.basename(filename)\n    # We want to ignore anything after '_nohash_' in the file name when\n    # deciding which set to put a wav in, so the data set creator has a way of\n    # grouping wavs that are close variations of each other.\n    hash_name = re.sub(r'_nohash_.*$', '', base_name)\n    # This looks a bit magical, but we need to decide whether this file should\n    # go into the training, testing, or validation sets, and we want to keep\n    # existing files in the same set even if more files are subsequently\n    # added.\n    # To do that, we need a stable way of deciding based on just the file name\n    # itself, so we do a hash of that and then use that to generate a\n    # probability value that we use to assign it.\n    hash_name_hashed = hashlib.sha1(hash_name.encode()).hexdigest()\n    percentage_hash = ((int(hash_name_hashed, 16) %\n                        (MAX_NUM_WAVS_PER_CLASS + 1)) *\n                       (100.0 / MAX_NUM_WAVS_PER_CLASS))\n    if percentage_hash < validation_percentage:\n        result = 'validation'\n    elif percentage_hash < (testing_percentage + validation_percentage):\n        result = 'testing'\n    else:\n        result = 'training'\n    return result\n```\n\nThe results of running this over the current set are included in this archive as validation_list.txt and testing_list.txt. These text files contain the paths to all the files in each set, with each path on a new line. Any files that aren't in either of these lists can be considered to be part of the training set.\n\n### Processing\nThe original audio files were collected in uncontrolled locations by people around the world. We requested that they do the recording in a closed room for privacy reasons, but didn't stipulate any quality requirements. This was by design, since we wanted examples of the sort of speech data that we're likely to encounter in consumer and robotics applications, where we don't have much control over the recording equipment or environment.\n\nThe data was captured in a variety of formats, for example Ogg Vorbis encoding for the web app, and then converted to a 16-bit little-endian PCM-encoded WAVE file at a 16000 sample rate. The audio was then trimmed to a one second length to align most utterances, using the extract_loudest_section tool. The audio files were then screened for silence or incorrect words, and arranged into folders by label.\n\n### Background Noise\nTo help train networks to cope with noisy environments, it can be helpful to mix in realistic background audio. The _background_noise_ folder contains a set of longer audio clips that are either recordings or mathematical simulations of noise. For more details, see the _background_noise_/README.md.\n\n### Citations\nIf you use the Speech Commands dataset in your work, please cite it as:\n\nAPA-style citation: \"Warden P. Speech Commands: A public dataset for single-word speech recognition, 2017. Available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz\".\n\nBibTeX:\n```\n@article{speechcommands, \n    title={Speech Commands: A public dataset for single-word speech recognition.}, \n    author={Warden, Pete}, \n    journal={Dataset available from http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz}, \n    year={2017}\n}\n```\n\n### Credits\nMassive thanks are due to everyone who donated recordings to this data set, I'm very grateful. I also couldn't have put this together without the help and support of Billy Rutledge, Rajat Monga, Raziel Alvarez, Brad Krueger, Barbara Petit, Gursheesh Kour, and all the AIY and TensorFlow teams.\n\nPete Warden, petewarden@google.com\n\nFiles: 4 files\nSize: 3.76 GB\nType: 7z, txt\nLicense: Subject to Competition Rules",
      "metadata": {
        "domain": "audio_speech",
        "keywords": [
          "multiclass classification",
          "sensor_signal",
          "spectrogram-mfcc",
          "speech",
          "accuracy"
        ]
      }
    },
    {
      "challenge_name": "tgs-salt-identification-challenge",
      "description": "Challenge description:\n# TGS Salt Identification Challenge\n\n## Description\n\nSeveral areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.\n\nTo create the most accurate seismic images and 3D renderings, TGS (the world's leading geoscience data company) is hoping Kaggle's machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.\n\n## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$\n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.\n\n### Submission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nThe metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.\n\nThe file should contain a header and have the following format. Each row in your submission represents a single predicted salt segmentation for the given image.\n\n```\nid,rle_mask\n3e06571ef3,1\n1a51b08d882,1\n1c32590b06f,1\netc.\n```\n\n## Timeline\n\n- October 12, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\n- October 12, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\n- October 19, 2018 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\n- 1st Place - $50,000\n- 2nd Place - $25,000\n- 3rd Place - $15,000\n- 4th Place - $10,000\n\n## Citation\n\nAddison Howard, Arvind Sharma, Ashleigh Lenamond, cenyen, Compu Ter, John Adamck, Mark McDonald, Sathiya, Sri Kainkaryam, and Will Cukierski. TGS Salt Identification Challenge. https://kaggle.com/competitions/tgs-salt-identification-challenge, 2018. Kaggle.\n\n## Competition Details\n\n- **Competition Host**: TGS\n- **Total Prize Pool**: $100,000\n- **Participants**: 3,726\n- **Teams**: 3,219\n- **Submissions**: 76,185\n- **Competition Type**: Image Segmentation with Custom Metric\n- **Domain**: Geology\n\nData description:\nTGS Salt Identification Challenge\nSegment salt deposits beneath the Earth's surface\n\nDataset Description\n\nBackground\nSeismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth\u2019s interior. Reflection seismology is similar to X-ray, sonar and echolocation.\nA seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.\nThere are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.\n\nData\nThe data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt.\n\nFiles 7 files\nSize 483.07 MB\nType zip, csv\nLicense Subject to Competition Rules\n\ncompetition_data.zip (227.13 MB)\ndepths.csv\nflamingo.zip\nsample_submission.csv\ntest.zip\ntrain.csv\ntrain.zip",
      "docker_challenge_path": "/data/tgs-salt-identification-challenge",
      "competition_description": "Challenge description:\n# TGS Salt Identification Challenge\n\n## Description\n\nSeveral areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface. But unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.\n\nTo create the most accurate seismic images and 3D renderings, TGS (the world's leading geoscience data company) is hoping Kaggle's machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.",
      "evaluation_metric": "## Evaluation\n\nThis competition is evaluated on the mean average precision at different intersection over union (IoU) thresholds. The IoU of a proposed set of object pixels and a set of true object pixels is calculated as:\n\n$$IoU(A,B) = \\frac{A \\cap B}{A \\cup B}.$$\n\nThe metric sweeps over a range of IoU thresholds, at each point calculating an average precision value. The threshold values range from 0.5 to 0.95 with a step size of 0.05: (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). In other words, at a threshold of 0.5, a predicted object is considered a \"hit\" if its intersection over union with a ground truth object is greater than 0.5.\n\nAt each threshold value \\(t\\), a precision value is calculated based on the number of true positives (TP), false negatives (FN), and false positives (FP) resulting from comparing the predicted object to all ground truth objects:\n\n$$\\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nA true positive is counted when a single predicted object matches a ground truth object with an IoU above the threshold. A false positive indicates a predicted object had no associated ground truth object. A false negative indicates a ground truth object had no associated predicted object. The average precision of a single image is then calculated as the mean of the above precision values at each IoU threshold:\n\n$$\\frac{1}{|thresholds|} \\sum_t \\frac{TP(t)}{TP(t) + FP(t) + FN(t)}.$$\n\nLastly, the score returned by the competition metric is the mean taken over the individual average precisions of each image in the test dataset.",
      "dataset_description": "Data description:\nTGS Salt Identification Challenge\nSegment salt deposits beneath the Earth's surface\n\nDataset Description\n\nBackground\nSeismic data is collected using reflection seismology, or seismic reflection. The method requires a controlled seismic source of energy, such as compressed air or a seismic vibrator, and sensors record the reflection from rock interfaces within the subsurface. The recorded data is then processed to create a 3D view of earth\u2019s interior. Reflection seismology is similar to X-ray, sonar and echolocation.\nA seismic image is produced from imaging the reflection coming from rock boundaries. The seismic image shows the boundaries between different rock types. In theory, the strength of reflection is directly proportional to the difference in the physical properties on either sides of the interface. While seismic images show rock boundaries, they don't say much about the rock themselves; some rocks are easy to identify while some are difficult.\nThere are several areas of the world where there are vast quantities of salt in the subsurface. One of the challenges of seismic imaging is to identify the part of subsurface which is salt. Salt has characteristics that makes it both simple and hard to identify. Salt density is usually 2.14 g/cc which is lower than most surrounding rocks. The seismic velocity of salt is 4.5 km/sec, which is usually faster than its surrounding rocks. This difference creates a sharp reflection at the salt-sediment interface. Usually salt is an amorphous rock without much internal structure. This means that there is typically not much reflectivity inside the salt, unless there are sediments trapped inside it. The unusually high seismic velocity of salt can create problems with seismic imaging.\n\nData\nThe data is a set of images chosen at various locations chosen at random in the subsurface. The images are 101 x 101 pixels and each pixel is classified as either salt or sediment. In addition to the seismic images, the depth of the imaged location is provided for each image. The goal of the competition is to segment regions that contain salt.\n\nFiles 7 files\nSize 483.07 MB\nType zip, csv\nLicense Subject to Competition Rules\n\ncompetition_data.zip (227.13 MB)\ndepths.csv\nflamingo.zip\nsample_submission.csv\ntest.zip\ntrain.csv\ntrain.zip",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "segmentation",
          "images",
          "cnn",
          "augmentation",
          "geoscience",
          "map"
        ]
      }
    },
    {
      "challenge_name": "trec-covid-information-retrieval",
      "description": "Challenge description:\n# TREC-COVID Information Retrieval\n\n## Competition Description\n\nThis competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.\n\nResearchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.\n\nKaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.\n\nThis is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (\"runs\") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document). The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.\n\nFollowing the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment.\n\nWith your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments.\n\n## Acknowledgments\n\nThe Text REtrieval Conference (TREC) was founded in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.\n\nThe TREC-COVID Challenge is being organized by the Allen Institute for Artificial Intelligence (AI2), the National Institute of Standards and Technology (NIST), the National Library of Medicine (NLM), Oregon Health and Science University (OHSU), and the University of Texas Health Science Center at Houston (UTHealth).\n\nSee the NIST press release for more information.\n\n## Evaluation\n\n### Metric\nSubmissions will be evaluated using NDCG (Normalized Discounted Cumulative Gain).\n\n### Relevance Judgements\nThe relevance judgments will be made by human annotators that have biomedical expertise. Annotators will use a three-way scale:\n- Relevant: the article is fully responsive to the information need as expressed by the topic, i.e. answers the Question in the topic. The article need not contain all information on the topic, but must, on its own, provide an answer to the question.\n- Partially Relevant: the article answers part of the question but would need to be combined with other information to get a complete answer.\n- Not Relevant: everything else.\n\n### Submission File\nThe submission should be a list of topicid-docid ranked from top to bottom according to their relevance (i.e., the top docid is the most relevant document for a the specified topic). Each line should be a single topicid-docid.\n\nThe columns are as follows:\n- topicid - is the topic number (1..40)\n- docid - the cord_uid of the document retrieved in this position. It must be a valid cord_id in the May 19 release of CORD-19. If it has already been judged for this topic, it will be removed.\n\n### Example\n```\ntopicid,docid\n1,000ajevz\n1,000q5l5n\n...\n1,000tfenb\n```\n\n## Timeline\n- June 3, 2020 at 11:00 AM UTC - Entry and Final submission deadline.\n- June 23, 2020 - Estimated private leaderboard reveal date. Because this competition's relevance rankings are being evaluated post-submission, we will rescore the final private leaderboard in approximately 2 weeks following the submission deadline.\n\n## Citation\nEllen Voorhees, Ian Soboroff, Walter Reade, and Julia Elliott. TREC-COVID Information Retrieval. https://kaggle.com/competitions/trec-covid-information-retrieval, 2020. Kaggle.\n\n## Competition Host\nTREC-COVID Organizers\n\n## Prizes & Awards\nDoes not award Points or Medals\n\nData description:\nTREC-COVID Information Retrieval\nBuild a pandemic document retrieval system\nDataset Description\nThe task of this competition is to identify relevant documents from the CORD-19 dataset that match specific queries. This data page is an abbreviated version of theTREC-COVID Round 3page. Please reference it for a more in-depth description of this challenge.\nNote: The Public leaderboard is scored from data that has been previously released. We discourage submitting perfect submissions, but it's not unlikely that you will see scores of1.0on the leaderboard. This is effectively meaningless, since the final Private evaluation will be based on labels obtained after the competition close.\nFiles\nCORD-19- folder containing the May 19, 2020 version of CORD-19 documents and metadata\ntopics-rnd3.csv- file containing thetopic-id,query,question, andnarrativefor each topic.\ndocids-rnd3.txt- the list of documents that are can be predicted as relevant; these exclude documents that have been scored in previous rounds for a topic\nqrels.csv- contains the relevance judgements for documents that have been evaluated in previous rounds\nsample_submission.csv- a sample submission file in the correct format (with random document entries)\nFiles103321 filesSize12.73 GBTypejson, csv, txt + 1 otherLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/trec-covid-information-retrieval",
      "competition_description": "## Competition Description\n\nThis competition was launched and opened for submissions on May 27th 2020. Submissions will close in 1 week at 11:00 AM UTC on June 3rd 2020. The public leaderboard is based on the TREC-COVID Round 2 dataset. The private leaderboard will be based on the Round 3 dataset, which will be evaluated after the competition closes. Review the Data page for more details.\n\nResearchers, clinicians, and policy makers involved with the response to COVID-19 are constantly searching for reliable information on the virus and its impact. This presents a unique opportunity for the information retrieval (IR) and text processing communities to contribute to the response to this pandemic, as well as to study methods for quickly standing up information systems for similar future events. The results of the TREC-COVID Challenge will identify answers for some of today's questions while building infrastructure to improve tomorrow's search systems.\n\nKaggle first teamed up with the Allen Institute for AI in the launch of the COVID-19 Open Research Dataset (CORD-19). TREC-COVID builds on the CORD-19 Challenge by using the same document set, a collection of biomedical literature articles that has been updated on a weekly rolling basis.\n\nThis is the 3rd Round of the TREC-COVID Challenge. Prior runs were hosted directly on the TREC-COVID Site. For this round, you have the option to submit on Kaggle or directly to the TREC-COVID platform. The organizers have added 5 additional COVID-related topics to the 35 topics from the first two rounds, for a total of 40 topics. You will create a retrieval system that returns ranked lists of documents from CORD-19 for (a) each of these additional Round 3 topics (\"runs\") and as well as (b) residual rankings on the completed Round 1 & 2 topics, i.e., for any documents not judged in the CORD-19 dataset (not previously included as a ranked document). The eligible population of documents for Round 3 is anything included in the CORD-19 release up to Round 3's launch date, last updated on May 19th 2020.\n\nFollowing the close of Round 3, NIST will gather the collective set of participants' runs, to include those participants submitting directly through TREC-COVID. The organizers will then assess some reasonable subset of these submissions for relevance by human annotators with biomedical expertise. The results of the human annotation, known as relevance judgments, will then be used to score the submitted runs. It is important to understand that not all documents will be assessed, and thus the private leaderboard score will be based on partial document assessment.\n\nWith your help, the final document and topic sets together with the cumulative relevance judgments will comprise a COVID test collection. The incremental nature of the collection will support research on search systems for dynamic environments.",
      "evaluation_metric": "### Metric\nSubmissions will be evaluated using NDCG (Normalized Discounted Cumulative Gain).",
      "dataset_description": "Data description:\nTREC-COVID Information Retrieval\nBuild a pandemic document retrieval system\nDataset Description\nThe task of this competition is to identify relevant documents from the CORD-19 dataset that match specific queries. This data page is an abbreviated version of theTREC-COVID Round 3page. Please reference it for a more in-depth description of this challenge.\nNote: The Public leaderboard is scored from data that has been previously released. We discourage submitting perfect submissions, but it's not unlikely that you will see scores of1.0on the leaderboard. This is effectively meaningless, since the final Private evaluation will be based on labels obtained after the competition close.\nFiles\nCORD-19- folder containing the May 19, 2020 version of CORD-19 documents and metadata\ntopics-rnd3.csv- file containing thetopic-id,query,question, andnarrativefor each topic.\ndocids-rnd3.txt- the list of documents that are can be predicted as relevant; these exclude documents that have been scored in previous rounds for a topic\nqrels.csv- contains the relevance judgements for documents that have been evaluated in previous rounds\nsample_submission.csv- a sample submission file in the correct format (with random document entries)\nFiles103321 filesSize12.73 GBTypejson, csv, txt + 1 otherLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "information_retrieval",
        "keywords": [
          "ranking",
          "text",
          "semantic-retrieval",
          "biomedical",
          "ndcg"
        ]
      }
    },
    {
      "challenge_name": "understanding_cloud_organization",
      "description": "Challenge description:\nUnderstanding Clouds from Satellite Images\nCan you classify cloud structures from satellites?\n\nDescription\nClimate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition's dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world's ever-changing atmosphere and they need your help to better understand the clouds.\n\nShallow clouds play a huge role in determining the Earth's climate. They're also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.\n\nThere are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features\u2014such as clouds that resemble flowers.\n\nIn this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you'll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections.\n\nHelp us remove the haze from climate models and bring clarity to cloud identification.\n\nFor more information on the scientific background and how the labels were created see the following paper.\n\nEvaluation\nThis competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n$$\\frac{2 * |X \\cap Y|}{|X| + |Y|}$$\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each <Image, Label> pair in the test set.\n\nEncodedPixels\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values.\nInstead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nIMPORTANT\nThe predicted encodings should be against images that are scaled by 0.25 per side. In other words, while the images in Train and Test are 1400 x 2100 pixels, the predictions should be scaled down to a 350 x 525 pixel image. The reduction is required to achieve reasonable submission evaluation times.\n\nSubmission File Format\nYour submission file should be in csv format, with a header and columns names: Image_Label, EncodedPixels. Each row in your submission represents a single predicted cloud type segmentation for the given Image, and predicted Label, and you should have the same number of rows as num_images * num_labels. The segment for cloud type in an image will be encoded into a single row, even if there are several non-contiguous cloud type locations in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank.\n\nImage_Label,EncodedPixels\n002f507.jpg_Fish,11\n002f507.jpg_Flower,11\n002f507.jpg_Gravel,2183749\netc...\n\nTimeline\nNovember 11, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nNovember 11, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nNovember 18, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\nPrizes\n1st Place - $5,000\n2nd Place - $3,000\n3rd Place - $2,000\n\nCitation\nStephan Rasp, Hauke Schulz, Walter Reade, and Maggie Demkin. Understanding Clouds from Satellite Images. https://kaggle.com/competitions/understanding_cloud_organization, 2019. Kaggle.\n\nCompetition Host\nMax Planck Institute for Meteorology\n\nCompetition Details\nStart: Aug 16, 2019\nClose: Nov 18, 2019\nTotal Prize Money: $10,000\nTags: Image, Atmospheric Science, Dice\n\nData description:\nUnderstanding Clouds from Satellite Images\nCan you classify cloud structures from satellites?\nIn this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names:Fish,Flower,Gravel,Sugar.  For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four.\nThe images were downloaded from NASA Worldview.  Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.\nThe labels were created in a crowd-sourcing activity at the Max-Planck-Institite for Meteorology in Hamburg, Germany, and the Laboratoire de m\u00e9t\u00e9orologie dynamique in Paris, France. A team of 68 scientists identified areas of cloud patterns in each image, and each images was labeled by approximately 3 different scientists. Ground truth was determined by the union of the areas marked by all labelers for that image, after removing any black band area from the areas.\nThe segment for each cloud formation label for an image is encoded into a single row, even if there are several non-contiguous areas of the same formation in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank. You can read more about the encoding standard on the Evaluation page.\nFiles\ntrain.csv- the run length encoded segmentations for each image-label pair in the train_images\ntrain_images.zip- folder of training images\ntest_images.zip- folder of test images; your task is to predict the segmentations masks of each of the 4 cloud types (labels) for each image.\nIMPORTANT: Your prediction masks should be scaled down to 350 x 525 px.\nsample_submission.csv- a sample submission file in the correct format\nFiles 9246 files\nSize 6.41 GB\nType jpg, csv\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/understanding_cloud_organization",
      "competition_description": "Description\nClimate change has been at the top of our minds and on the forefront of important political decision-making for many years. We hope you can use this competition's dataset to help demystify an important climatic variable. Scientists, like those at Max Planck Institute for Meteorology, are leading the charge with new research on the world's ever-changing atmosphere and they need your help to better understand the clouds.\n\nShallow clouds play a huge role in determining the Earth's climate. They're also difficult to understand and to represent in climate models. By classifying different types of cloud organization, researchers at Max Planck hope to improve our physical understanding of these clouds, which in turn will help us build better climate models.\n\nThere are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features\u2014such as clouds that resemble flowers.\n\nIn this challenge, you will build a model to classify cloud organization patterns from satellite images. If successful, you'll help scientists to better understand how clouds will shape our future climate. This research will guide the development of next-generation models which could reduce uncertainties in climate projections.\n\nHelp us remove the haze from climate models and bring clarity to cloud identification.\n\nFor more information on the scientific background and how the labels were created see the following paper.",
      "evaluation_metric": "Evaluation\nThis competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n$$\\frac{2 * |X \\cap Y|}{|X| + |Y|}$$\nwhere X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each <Image, Label> pair in the test set.",
      "dataset_description": "Data description:\nUnderstanding Clouds from Satellite Images\nCan you classify cloud structures from satellites?\nIn this competition you will be identifying regions in satellite images that contain certain cloud formations, with label names:Fish,Flower,Gravel,Sugar.  For each image in the test set, you must segment the regions of each cloud formation label. Each image has at least one cloud formation, and can possibly contain up to all all four.\nThe images were downloaded from NASA Worldview.  Three regions, spanning 21 degrees longitude and 14 degrees latitude, were chosen. The true-color images were taken from two polar-orbiting satellites, TERRA and AQUA, each of which pass a specific region once a day. Due to the small footprint of the imager (MODIS) on board these satellites, an image might be stitched together from two orbits. The remaining area, which has not been covered by two succeeding orbits, is marked black.\nThe labels were created in a crowd-sourcing activity at the Max-Planck-Institite for Meteorology in Hamburg, Germany, and the Laboratoire de m\u00e9t\u00e9orologie dynamique in Paris, France. A team of 68 scientists identified areas of cloud patterns in each image, and each images was labeled by approximately 3 different scientists. Ground truth was determined by the union of the areas marked by all labelers for that image, after removing any black band area from the areas.\nThe segment for each cloud formation label for an image is encoded into a single row, even if there are several non-contiguous areas of the same formation in an image. If there is no area of a certain cloud type for an image, the corresponding EncodedPixels prediction should be left blank. You can read more about the encoding standard on the Evaluation page.\nFiles\ntrain.csv- the run length encoded segmentations for each image-label pair in the train_images\ntrain_images.zip- folder of training images\ntest_images.zip- folder of test images; your task is to predict the segmentations masks of each of the 4 cloud types (labels) for each image.\nIMPORTANT: Your prediction masks should be scaled down to 350 x 525 px.\nsample_submission.csv- a sample submission file in the correct format\nFiles 9246 files\nSize 6.41 GB\nType jpg, csv\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "segmentation",
          "images",
          "cnn_unet",
          "geospatial",
          "dice"
        ]
      }
    },
    {
      "challenge_name": "ventilator-pressure-prediction",
      "description": "Challenge description:\nWhat do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.  \nCurrent simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.  \nPartnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.  \nIn this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.  \nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.  \nPhoto by Nino Liverani on Unsplash  \n\nThe competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:  \n$$|X-Y|$$  \nwhere \\(X\\) is the vector of predicted pressure and \\(Y\\) is the vector of actual pressures across all breaths in the test set.  \nSubmission File  \nFor each id in the test set, you must predict a value for the pressure variable. The file should contain a header and have the following format:  \nid,pressure  \n1,20  \n2,23  \n3,24  \netc.  \n\nSeptember 22, 2021- Start Date.  \nOctober 27, 2021- Entry Deadline. You must accept the competition rules before this date in order to compete.  \nOctober 27, 2021- Team Merger Deadline. This is the last day participants may join or merge teams.  \nNovember 3, 2021- Final Submission Deadline.  \nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.  \n\n1st Place - $2,500  \n2nd Place - $2,500  \n3rd Place - $2,500  \n\nAddison Howard, alexjyu, Daniel Suo, and Will Cukierski. Google Brain - Ventilator Pressure Prediction. https://kaggle.com/competitions/ventilator-pressure-prediction, 2021. Kaggle.  \n\nCompetition Host  \nGoogle Brain  \nPrizes & Awards  \n$7,500\n\nData description:\nGoogle Brain\u00b7 Research Prediction Competition \u00b74 years agoGoogle Brain - Ventilator Pressure PredictionSimulate a ventilator connected to a sedated patient's lungDataset DescriptionThe ventilator data used in this competition was produced using a modifiedopen-source ventilatorconnected to anartificial bellows test lungvia a respiratory circuit. The diagram below illustrates the setup, with the two control inputs highlighted in green and the state variable (airway pressure) to predict in blue. The first control input is a continuous variable from 0 to 100 representing the percentage the inspiratory solenoid valve is open to let air into the lung (i.e., 0 is completely closed and no air is let in and 100 is completely open). The second control input is a binary variable representing whether the exploratory valve is open (1) or closed (0) to let air out.In this competition, participants are given numerous time series of breaths and will learn to predict the airway pressure in the respiratory circuit during the breath, given the time series of control inputs.Each time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below.Filestrain.csv- the training settest.csv- the test setsample_submission.csv- a sample submission file in the correct formatColumnsid- globally-unique time step identifier across an entire filebreath_id- globally-unique time step for breathsR- lung attribute indicating how restricted the airway is (in cmH2O/L/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can changeRby changing the diameter of the straw, with higherRbeing harder to blow.C- lung attribute indicating how compliant the lung is (in mL/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can changeCby changing the thickness of the balloon\u2019s latex, with higherChaving thinner latex and easier to blow.time_step- the actual time stamp.u_in- the control input for the inspiratory solenoid valve. Ranges from 0 to 100.u_out- the control input for the exploratory solenoid valve. Either 0 or 1.pressure- the airway pressure measured in the respiratory circuit, measured in cmH2O.Files3 filesSize698.79 MBTypecsvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/ventilator-pressure-prediction",
      "competition_description": "Challenge description:\nWhat do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.  \nCurrent simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.  \nPartnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.  \nIn this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.  \nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.  \nPhoto by Nino Liverani on Unsplash",
      "evaluation_metric": "The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:  \n$$|X-Y|$$  \nwhere \\(X\\) is the vector of predicted pressure and \\(Y\\) is the vector of actual pressures across all breaths in the test set.",
      "dataset_description": "Data description:\nGoogle Brain\u00b7 Research Prediction Competition \u00b74 years agoGoogle Brain - Ventilator Pressure PredictionSimulate a ventilator connected to a sedated patient's lungDataset DescriptionThe ventilator data used in this competition was produced using a modifiedopen-source ventilatorconnected to anartificial bellows test lungvia a respiratory circuit. The diagram below illustrates the setup, with the two control inputs highlighted in green and the state variable (airway pressure) to predict in blue. The first control input is a continuous variable from 0 to 100 representing the percentage the inspiratory solenoid valve is open to let air into the lung (i.e., 0 is completely closed and no air is let in and 100 is completely open). The second control input is a binary variable representing whether the exploratory valve is open (1) or closed (0) to let air out.In this competition, participants are given numerous time series of breaths and will learn to predict the airway pressure in the respiratory circuit during the breath, given the time series of control inputs.Each time series represents an approximately 3-second breath. The files are organized such that each row is a time step in a breath and gives the two control signals, the resulting airway pressure, and relevant attributes of the lung, described below.Filestrain.csv- the training settest.csv- the test setsample_submission.csv- a sample submission file in the correct formatColumnsid- globally-unique time step identifier across an entire filebreath_id- globally-unique time step for breathsR- lung attribute indicating how restricted the airway is (in cmH2O/L/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can changeRby changing the diameter of the straw, with higherRbeing harder to blow.C- lung attribute indicating how compliant the lung is (in mL/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can changeCby changing the thickness of the balloon\u2019s latex, with higherChaving thinner latex and easier to blow.time_step- the actual time stamp.u_in- the control input for the inspiratory solenoid valve. Ranges from 0 to 100.u_out- the control input for the exploratory solenoid valve. Either 0 or 1.pressure- the airway pressure measured in the respiratory circuit, measured in cmH2O.Files3 filesSize698.79 MBTypecsvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "regression",
          "time_series",
          "sequence modeling",
          "healthcare",
          "mae"
        ]
      }
    },
    {
      "challenge_name": "vsb-power-line-fault-detection",
      "description": "Challenge description:\n# VSB Power Line Fault Detection\n\nCan you detect faults in above-ground electrical lines?\n\nMedium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge \u2014 an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.\n\nYour challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at V\u0160B. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.\n\nENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials.\n\nBy developing a solution to detect partial discharge you'll help reduce maintenance costs, and prevent power outages.\n\n## Evaluation\n\nSubmissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:\n\n$$ MCC = \\frac{(TP*TN) - (FP * FN)}{\\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}}, $$\n\nwhere TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.\n\n### Submission File\n\nFor each signal in the test set, you must predict a binary prediction for the target variable. The file should contain a header and have the following format:\n\nsignal_id,target\n0,0\n1,1\n2,0\netc.\n\n## Prizes\n\nCash Prizes\nParticipants with the best score on the private leaderboard are eligible to receive:\n1st Place - $ 12,000\n2nd Place - $ 8,000\n3rd Place - $ 5,000\n\nThe winning participants will have the option to contribute a description of their solution to a review paper the hosts will write to celebrate the progress in this field and also this competition. The host may also invite other contributions at their discretion, so we encourage you to post a description of the novel components of your solution once the competition is over.\n\n## Timeline\n\nMarch 14, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\nMarch 14, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\nMarch 21, 2019 - Final submission deadline.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Citation\n\nAddison Howard, Sohier Dane, and Tomas Vantuch. VSB Power Line Fault Detection. https://kaggle.com/competitions/vsb-power-line-fault-detection, 2018. Kaggle.\n\n## Competition Host\n\nEnet Centre, VSB - T.U. of Ostrava\n\n## Participation Statistics\n\n7,695 Entrants\n1,589 Participants\n1,445 Teams\n19,481 Submissions\n\n## Tags\n\nTabular, Binary Classification, Signal Processing, Matthews correlation coefficient\n\nData description:\nVSB Power Line Fault Detection\nCan you detect faults in above-ground electrical lines?\n\nEnet Centre, VSB - T.U. of Ostrava\u00b7 Featured Prediction Competition \u00b77 years ago\n\nFaults in electric transmission lines can lead to a destructive phenomenon calledpartial discharge. If left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs.\n\nEach signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a3-phase power scheme, and all three phases are measured simultaneously.\n\nFile Descriptions\nmetadata_[train/test].csv\nid_measurement: the ID code for a trio of signals recorded at the same time.\nsignal_id: the foreign key for the signal data. Each signal ID is unique across both train and test, so the first ID in train is '0' but the first ID in test is '8712'.\nphase: the phase ID code within the signal trio. The phases may or may not all be impacted by a fault on the line.\ntarget: 0 if the power line is undamaged, 1 if there is a fault.\n\n[train/test].parquet- The signal data. Each column contains one signal; 800,000 int8 measurements as exported with pyarrow.parquet version 0.11. Please note that this is different than our usual data orientation of one row per observation; the switch makes it possible loading a subset of the signals efficiently. If you haven't worked withApache Parquetbefore, please refer to eitherthe Python data loading starter kernel.\n\nsample_submission.csv: a valid sample submission.\n\nFiles5 filesSize12.62 GBTypecsv, parquetLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/vsb-power-line-fault-detection",
      "competition_description": "Challenge description:\n# VSB Power Line Fault Detection\n\nCan you detect faults in above-ground electrical lines?\n\nMedium voltage overhead power lines run for hundreds of miles to supply power to cities. These great distances make it expensive to manually inspect the lines for damage that doesn't immediately lead to a power outage, such as a tree branch hitting the line or a flaw in the insulator. These modes of damage lead to a phenomenon known as partial discharge \u2014 an electrical discharge which does not bridge the electrodes between an insulation system completely. Partial discharges slowly damage the power line, so left unrepaired they will eventually lead to a power outage or start a fire.\n\nYour challenge is to detect partial discharge patterns in signals acquired from these power lines with a new meter designed at the ENET Centre at V\u0160B. Effective classifiers using this data will make it possible to continuously monitor power lines for faults.\n\nENET Centre researches and develops renewable energy resources with the goal of reducing or eliminating harmful environmental impacts. Their efforts focus on developing technology solutions around transportation and processing of energy raw materials.\n\nBy developing a solution to detect partial discharge you'll help reduce maintenance costs, and prevent power outages.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated on the Matthews correlation coefficient (MCC) between the predicted and the observed response. The MCC is given by:\n\n$$ MCC = \\frac{(TP*TN) - (FP * FN)}{\\sqrt{(TP+FP)(TP+FN)(TN + FP)(TN+FN)}}, $$\n\nwhere TP is the number of true positives, TN the number of true negatives, FP the number of false positives, and FN the number of false negatives.",
      "dataset_description": "Data description:\nVSB Power Line Fault Detection\nCan you detect faults in above-ground electrical lines?\n\nEnet Centre, VSB - T.U. of Ostrava\u00b7 Featured Prediction Competition \u00b77 years ago\n\nFaults in electric transmission lines can lead to a destructive phenomenon calledpartial discharge. If left alone, partial discharges can damage equipment to the point that it stops functioning entirely. Your challenge is to detect partial discharges so that repairs can be made before any lasting harm occurs.\n\nEach signal contains 800,000 measurements of a power line's voltage, taken over 20 milliseconds. As the underlying electric grid operates at 50 Hz, this means each signal covers a single complete grid cycle. The grid itself operates on a3-phase power scheme, and all three phases are measured simultaneously.\n\nFile Descriptions\nmetadata_[train/test].csv\nid_measurement: the ID code for a trio of signals recorded at the same time.\nsignal_id: the foreign key for the signal data. Each signal ID is unique across both train and test, so the first ID in train is '0' but the first ID in test is '8712'.\nphase: the phase ID code within the signal trio. The phases may or may not all be impacted by a fault on the line.\ntarget: 0 if the power line is undamaged, 1 if there is a fault.\n\n[train/test].parquet- The signal data. Each column contains one signal; 800,000 int8 measurements as exported with pyarrow.parquet version 0.11. Please note that this is different than our usual data orientation of one row per observation; the switch makes it possible loading a subset of the signals efficiently. If you haven't worked withApache Parquetbefore, please refer to eitherthe Python data loading starter kernel.\n\nsample_submission.csv: a valid sample submission.",
      "metadata": {
        "domain": "sensor_signal",
        "keywords": [
          "binary classification",
          "time_series",
          "signal_processing",
          "feature_engineering",
          "mcc"
        ]
      }
    },
    {
      "challenge_name": "web-traffic-time-series-forecasting",
      "description": "Challenge description:\n# Web Traffic Time Series Forecasting\n\n## Competition Objective\nForecast future traffic to Wikipedia pages. This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.\n\nSequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast.\n\nThis competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.\n\nYou have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.\n\nWe thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it.\n\nKaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.\n\n## Evaluation Criteria\nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.\n\n## Submission Requirements\nFor each article and day combination (see key.csv), you must predict the web traffic. The file should contain a header and have the following format:\n\nId,Visits\nbf4edcf969af,0\n929ed2bf52b9,0\nff29d0f51d5c0,etc.\n\nDue to the large file size and number of rows, submissions may take a few minutes to score. Thank you for your patience.\n\n## Prizes\n$12,000\n$8,000\n$5,000\n\nTop submissions will also have the opportunity to present their work at the NIPS Time Series Workshop in Long Beach, California, co-located with the top machine learning conference NIPS 2017. Attending the workshop is not required to participate in the competition, however only teams that are attending the workshop will be considered to present their work.\n\nAttendees presenting in person are responsible for all costs associated with travel, expenses, and fees to attend NIPS 2017.\n\n## Timeline\nThis competition has a training phase and a future forecasting phase. During the training phase, participants build models and predict on historical values. During the future phase, participants will forecast future traffic values.\n\nSeptember 1st, 2017 - Deadline to accept competition rules.\nSeptember 1st, 2017 - Team Merger deadline. This is the last day participants may join or merge teams.\nSeptember 1st, 2017 - Final dataset is released.\nSeptember 12th 7:59 PM UTC - Final submission deadline.\n\nCompetition winners will be revealed after November 13, 2017.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Competition Details\n- **Competition Host**: Google\n- **Competition Type**: Research Prediction Competition\n- **Start Date**: July 13, 2017\n- **Close Date**: November 15, 2017\n- **Total Prize Pool**: $25,000\n\n## Citation\nMaggie, Oren Anava, Vitaly Kuznetsov, and Will Cukierski. Web Traffic Time Series Forecasting. https://kaggle.com/competitions/web-traffic-time-series-forecasting, 2017. Kaggle.\n\nData description:\nWeb Traffic Time Series Forecasting Forecast future traffic to Wikipedia pages Dataset Description The training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until March 1st, 2017. The second stage will use training data up until September 1st, 2017. The final ranking of the competition will be based on predictions of daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset. You will submit your forecasts for these dates by September 12th. For each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day. To reduce the submission file size, each page and date combination has been given a shorter Id. The mapping between page names and the submission Id is given in the key files. File descriptions Files used for the first stage will end in '_1'. Files used for the second stage will end in '_2'. Both will have identical formats. The complete training data for the second stage will be made available prior to the second stage. train_*.csv- contains traffic data. This a csv file where each row corresponds to a particular article and each column correspond to a particular date. Some entries are missing data. The page names contain the Wikipedia project (e.g. en.wikipedia.org), type of access (e.g. desktop) and type of agent (e.g. spider). In other words, each article name has the following format: 'name_project_access_agent' (e.g. 'AKB48_zh.wikipedia.org_all-access_spider'). key_*.csv- gives the mapping between the page names and the shortened Id column used for prediction sample_submission_*.csv- a submission file showing the correct format License Subject to Competition Rules",
      "docker_challenge_path": "/data/web-traffic-time-series-forecasting",
      "competition_description": "Challenge description:\n# Web Traffic Time Series Forecasting\n\n## Competition Objective\nForecast future traffic to Wikipedia pages. This competition focuses on the problem of forecasting the future values of multiple time series, as it has always been one of the most challenging problems in the field. More specifically, we aim the competition at testing state-of-the-art methods designed by the participants, on the problem of forecasting future web traffic for approximately 145,000 Wikipedia articles.\n\nSequential or temporal observations emerge in many key real-world problems, ranging from biological data, financial markets, weather forecasting, to audio and video processing. The field of time series encapsulates many different problems, ranging from analysis and inference to classification and forecast.\n\nThis competition will run as two stages and involves prediction of actual future events. There will be a training stage during which the leaderboard is based on historical data, followed by a stage where participants are scored on real future events.\n\nYou have complete freedom in how to produce your forecasts: e.g. use of univariate vs multi-variate models, use of metadata (article identifier), hierarchical time series modeling (for different types of traffic), data augmentation (e.g. using Google Trends data to extend the dataset), anomaly and outlier detection and cleaning, different strategies for missing value imputation, and many more types of approaches.\n\nWe thank Google Inc. and Voleon for sponsorship of this competition, and Oren Anava and Vitaly Kuznetsov for organizing it.\n\nKaggle is excited to partner with research groups to push forward the frontier of machine learning. Research competitions make use of Kaggle's platform and experience, but are largely organized by the research group's data science team. Any questions or concerns regarding the competition data, quality, or topic will be addressed by them.",
      "evaluation_metric": "## Evaluation Criteria\nSubmissions are evaluated on SMAPE between forecasts and actual values. We define SMAPE = 0 when the actual and predicted values are both 0.",
      "dataset_description": "Data description:\nWeb Traffic Time Series Forecasting Forecast future traffic to Wikipedia pages Dataset Description The training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until March 1st, 2017. The second stage will use training data up until September 1st, 2017. The final ranking of the competition will be based on predictions of daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset. You will submit your forecasts for these dates by September 12th. For each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day. To reduce the submission file size, each page and date combination has been given a shorter Id. The mapping between page names and the submission Id is given in the key files. File descriptions Files used for the first stage will end in '_1'. Files used for the second stage will end in '_2'. Both will have identical formats. The complete training data for the second stage will be made available prior to the second stage. train_*.csv- contains traffic data. This a csv file where each row corresponds to a particular article and each column correspond to a particular date. Some entries are missing data. The page names contain the Wikipedia project (e.g. en.wikipedia.org), type of access (e.g. desktop) and type of agent (e.g. spider). In other words, each article name has the following format: 'name_project_access_agent' (e.g. 'AKB48_zh.wikipedia.org_all-access_spider'). key_*.csv- gives the mapping between the page names and the shortened Id column used for prediction sample_submission_*.csv- a submission file showing the correct format License Subject to Competition Rules",
      "metadata": {
        "domain": "time_series",
        "keywords": [
          "forecasting",
          "time_series",
          "feature_engineering",
          "web_traffic",
          "smape"
        ]
      }
    },
    {
      "challenge_name": "womens-machine-learning-competition-2019",
      "description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Women's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nAs a result of the continued collaboration between Google Cloud and the NCAA\u00ae, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.\n\nAs the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!\n\nThis page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.\n\n## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\n\nwhere:\n- n is the number of games played\n- $ \\hat{y}_i $ is the predicted probability of team 1 beating team 2\n- $ y_i $ is 1 if team 1 wins, 0 if team 2 wins\n- $ log() $ is the natural (base e) logarithm\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.\n\n### Submission File\n\nThe file you submit will depend on whether the competition is in stage 1 (historical model building) or stage 2 (the 2019 tournament). Sample submission files will be provided for both stages. The format is a list of every possible matchup between the tournament teams. Since team1 vs. team2 is the same as team2 vs. team1, we only include the game pairs where team1 has the lower team id. For example, in a tournament of 64 teams, you will predict (64*63)/2 = 2,016 matchups.\n\nEach game has a unique id created by concatenating the season in which the game was played, the team1 id, and the team2 id. For example, \"2013_3104_3129\" indicates team 3104 played team 3129 in the year 2013. You must predict the probability that the team with the lower id beats the team with the higher id.\n\nThe resulting submission format looks like the following, where \"pred\" represents the predicted probability that the first team will win:\n```\nid,pred\n2013_3103_3107,0.5\n2013_3103_3112,0.5\n2013_3103_3125,0.5\n...\n```\n\n## FAQs\n\n**Is one tournament enough to decide the best basketball algorithm?**\nIt's better to be lucky than good! Besides, when was the last time your office held a cross validation pool?\n\n**Can I update my predictions after the tournament starts?**\nNo changes are permitted once the tournament begins.\n\n**How is the leaderboard going to work when the event being scored hasn't yet happened?**\nYou won't submit after the tournament starts. We'll update the solution file as the games occur, which will cause the ranks on the leaderboard to change.\n\n**Why don't predictions in later rounds count for more?**\nWhile it is possible to weight the later games, we chose to keep scoring simple and count all games equally. Any weights we pick would be mostly arbitrary (how many first-round games is a championship game worth?). Also, weighting any game increases the role that luck plays in determining a winner. We've also structured the competition so that people can still be in the running even if there are early-round upsets.\n\n## Prizes\n\n1st Place - $10,000  \n2nd Place - $7,000  \n3rd Place - $5,000  \n4th Place - $2,000  \n5th Place - $1,000  \n\nKaggle Swag (e.g. t-shirt, coffee mug, stickers), will be awarded to the top three most upvoted Kernels, as of the close of the end of the competition. Only Kernels created/forked after the start of the competition will be eligible for a prize.\n\nNCAA/Google Cloud Swag will be given for the best Competitiveness Score submissions. See https://www.kaggle.com/c/womens-machine-learning-competition-2019/overview/competitiveness-score for more information\n\nStage 1 will not count towards Kaggle rankings/points.  \nStage 2 will count toward Kaggle rankings/points.\n\n## Timeline\n\n**Stage 1 - Model Building**  \nSunday, March 17 - Prior to this deadline, competitors build and test models on historical data. The leaderboard shows the model performance on historical tournament outcomes.\n\n**Stage 2 - Championship**  \nMonday, March 18 - Selection Monday\u00ae (64 teams announced)  \nTuesday, March 19 - Kaggle begins to accept 2019 predictions. Release of up-to-date 2018-2019 season data.  \nFriday, March 22 3PM UTC - Final deadline to submit 2019 predictions.  \nMarch 22 onward - Watch your tournament results play out! The Kaggle Team will refresh the leaderboard throughout the competition as games are finalized.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Tutorials & Getting Started\n\nCheck out our starter tutorial notebook in Kaggle Kernels.\n\nGoogle Cloud Platform: Work with this competition's data on Google Cloud Platform (GCP). New users can receive $300 in free trial credits.\n\nInterested in learning more about Cloud Machine Learning Engine and accessing the data in a Cloud Bucket? Check out our tutorial here.\n\n## Competitiveness Score\n\nThere's a reason why it's called MarchMadness\u00ae. Upsets happen, underdogs become \"cinderellas\" and games that analysts expected to be blowouts become nail-biters through the final seconds. A team's competitiveness is what keeps games exciting and the tournament truly \"mad.\"\n\nIn addition to the predictive modeling competition, we are hosting a separate competition for kernels that present an exploratory analysis of a competitiveness score. To enter this complementary challenge, all you have to do is publish a public kernel before the final day of the competition, and add \"Competitiveness\" in the title. Entries will be judged by the Google Cloud team in the weeks following the competition and the top three winners will receive a piece of Google Cloud/NCAA\u00ae swag! You may submit as many entries as you wish!\n\nWhat is competitiveness? The ability of team to \"stay in the game\" and possibly have a chance to win late in the contest\u2026\n\nThis is more than likely not a scalar metric, but possibly a clustering of types of competitiveness and then a rating within each. Does this metric have predictive power? The interpretation is up to you.\n\nA few examples:\n- Expected performance relative to match up. Typically this is communicated as point spread. Team A is a 13 point favorite over Team B (the betting line from oddsmakers built to entice 50 percent of wagering on each side -- Team A minus 13 points or Team B plus 13 points)\n- Performance in road and neutral floor games relative to opponent/travel-- i.e. how does neutral floor NCAA Tournament setting affect competitiveness?\n- \"Effort\" category performance-- rebounding, blocks, steals, etc -- relative to seasonal averages weighted against caliber of opponent and opponent's \"effort\" category performance expectation going into the game\n- Leading scorers and/or most significant players performance relative to expectation and/or season averages. Did the best players fail to match, match, or exceed expectations? Or - did a team effectively neutralize the opponent's leading scorers and/or most significant players?\n\nHere's some examples to what factors that would be used in making a better submission. Please make all Kernels public and add \"[Competitiveness]\" to the front of the Kernel's name.\n\nKernels must be made by the end of the tournament, and judging will begin shortly thereafter.\n\nWinners will receive NCAA/Google Cloud swag.\n\nHappy Modeling!\n\n## Citation\n\nAddison Howard, Danielle Notaro, Eric Schmidt, Jeff Sonas, Jen Raulli, Rachel Ahn, Tiffany Martin, and Will Cukierski. Google Cloud & NCAA\u00ae ML Competition 2019-Women's. https://kaggle.com/competitions/womens-machine-learning-competition-2019, 2019. Kaggle.\n\nData description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Women's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\n## Note for Stage 2:\nAll of the data files have been updated through the end of the current regular season. You can find everything you need in these two files:\n1) Stage2WDataFiles.zip (this contains the same type of information as the WDataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n2) WSampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)\n\nEach season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the tournament, we encourage reading the Wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, and we have assigned a \"W\" prefix to all files pertinent to women's college basketball, so if you are re-using code from the men's contest, you may need to adjust for this.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018).\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins.\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1997-98 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1997-98 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:**\nThe college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women's Division I games were played on November 8th, 2018 and the women's national championship game will be played on April 7th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: WTeams.csv**\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=3465) and North Alabama (TeamID=3466), and so you will not see any historical data for these teams prior to the current season.\n\n- TeamID- a 4 digit id number, from 3000-3999, uniquely identifying each NCAA\u00ae women's team. A school's TeamID does not change from one year to the next, so for instance the Duke women's TeamID is 3181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName- a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n\n**Data Section 1 file: WSeasons.csv**\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season- indicates the year in which the tournament was played. Remember that the current season counts as 2019.\n- DayZero- tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that (each year) Selection Monday is on day 133. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z- by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were DesMoines, Fresno, Kingston, and Raleigh. Being the first alphabetically, DesMoines becomes W. Since the Fresno regional champion (Stanford) played against the DesMoines regional champion (Baylor) in the national semifinals, that makes Fresno be region X. For the other two (Kingston and Raleigh), since Kingston comes first alphabetically, that makes Kingston Y and therefore Raleigh is Z. So for that season, the W/X/Y/Z are DesMoines,Fresno,Kingston,Raleigh. And so for instance, Baylor, the #1 seed in the DesMoines region, is listed in the WNCAATourneySeeds file with a seed of W01, meaning they were the #1 seed in the W region (the DesMoines region). We will not know the final W/X/Y/Z designations until Selection Monday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\nThe game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament).\n\nThis doesn't necessarily mean that the regular season will always start exactly on Day #0; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety.\n\n**Data Section 1 file: WNCAATourneySeeds.csv**\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 18, 2019.\n\n- Season- the year that the tournament was played in\n- Seed- this is a 3-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For example, the first record in the file is seed W01, which means we are looking at the #1 seed in the W region (which we can see from the \"WSeasons.csv\" file was the East region).\n- TeamID- this identifies the id number of the team, as specified in the WTeams.csv file\n\n**Data Section 1 file: WRegularSeasonCompactResults.csv**\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season- this is the year of the associated entry in WSeasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum- this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"WSeasons.csv\" file. For example, the first game in the file was DayNum=18. Combined with the fact from the \"WSeasons.csv\" file that day zero was 10/27/1997 that year, this means the first game was played 18 days later, or 11/14/1997. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID).\n- WTeamID- this identifies the id number of the team that won the game, as listed in the \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore- this identifies the number of points scored by the winning team.\n- LTeamID- this identifies the id number of the team that lost the game.\n- LScore- this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- NumOT- this indicates the number of overtime periods in the game, an integer 0 or higher.\n- WLoc- this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\n**Data Section 1 file: WNCAATourneyCompactResults.csv**\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games.\n\nAlthough the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20 years for the women's tournament, as follows:\n\n2017 season through 2019 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)\n- Round 4 = days 146/147 (Elite Eight, Sun/Mon)\n- National Seminfinal = day 151 (Fri)\n- National Final = day 153 (Sun)\n\n2015 season and 2016 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)\n- Round 4 = days 146/147 (Elite Eight, Sun/Mon)\n- National Seminfinal = day 153 (Sun)\n- National Final = day 155 (Tue)\n\n2003 season through 2014 season:\n- Round 1 = days 138/139 (Sat/Sun)\n- Round 2 = days 140/141 (Mon/Tue)\n- Round 3 = days 145/146 (Sweet Sixteen, Sat/Sun)\n- Round 4 = days 147/148 (Elite Eight, Mon/Tue)\n- National Seminfinal = day 153 (Sun)\n- National Final = day 155 (Tue)\n\n1998 season through 2002 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = day 145 only (Sweet Sixteen, Sat)\n- Round 4 = day 147 only (Elite Eight, Mon)\n- National Seminfinal = day 151 (Fri)\n- National Final = day 153 (Sun)\n\n**Data Section 1 file: WSampleSubmissionStage1.csv**\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, and 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019).\n\nSince there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*5=10,080 data rows.\n\n- ID- this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred- this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2005_3112_3181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2005_3181_3314,0.516\n\n### Data Section 2 - Team Box Scores\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n\n- WFGM- field goals made (by the winning team)\n- WFGA- field goals attempted (by the winning team)\n- WFGM3- three pointers made (by the winning team)\n- WFGA3- three pointers attempted (by the winning team)\n- WFTM- free throws made (by the winning team)\n- WFTA- free throws attempted (by the winning team)\n- WOR- offensive rebounds (pulled by the winning team)\n- WDR- defensive rebounds (pulled by the winning team)\n- WAst- assists (by the winning team)\n- WTO- turnovers committed (by the winning team)\n- WStl- steals (accomplished by the winning team)\n- WBlk- blocks (accomplished by the winning team)\n- WPF- personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: WRegularSeasonDetailedResults.csv**\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file.\n\n**Data Section 2 file: WNCAATourneyDetailedResults.csv**\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: WCities.csv**\nThis file provides a master list of cities that have been locations for games played.\n\n- CityID- a four-digit ID number uniquely identifying a city.\n- City- the text name of the city.\n- State- the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: WGameCities.csv**\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together.\n\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results file.\n- CRType- this can be either Regular or NCAA. If it is Regular, you can find more about the game in the WRegularSeasonCompactResults.csv file. If it is NCAA, you can find more about the game in the WNCAATourneyCompactResults.csv file.\n- CityID- the ID of the city where the game was played, as specified by the CityID column in the WCities.csv file.\n\n### Data Section 4 - Supplements\nThis section contains additional supporting information, including alternative team name spellings and representations of bracket structure\n\n**Data Section 4 file: WTeamSpellings.csv**\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling- this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID- this identifies the TeamID for the team that has the alternative spelling (as described in WTeams.csv).\n\n**Data Section 4 file: WNCAATourneySlots**\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season.\n\n- Slot- this uniquely identifies one of the tournament games. It is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed- this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\". Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed- this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column.",
      "docker_challenge_path": "/data/womens-machine-learning-competition-2019",
      "competition_description": "Challenge description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Women's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\nAs a result of the continued collaboration between Google Cloud and the NCAA\u00ae, the sixth annual Kaggle-backed March Madness competition is underway! Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard test. Kagglers will join the millions of fans who attempt to forecast the outcomes of March Madness during this year's NCAA Division I Men's and Women's Basketball Championships. But unlike most fans, you will pick your bracket using a combination of NCAA's historical data and your computing power, while the ground truth unfolds on national television.\n\nIn the first stage of the competition, Kagglers will rely on results of past tournaments to build and test models. We encourage you to post any useful external data as a dataset. In the second stage, competitors will forecast outcomes of all possible matchups in the 2019 NCAA Division I Men's and Women's Basketball Championships. You don't need to participate in the first stage to enter the second. The first stage exists to incentivize model building and provide a means to score predictions. The real competition is forecasting the 2019 results.\n\nAs the official public cloud provider of the NCAA, Google Cloud is proud to provide a competition to help participants strengthen their knowledge of basketball, statistics, data modeling, and cloud technology. As part of its journey to the cloud, the NCAA has migrated 80+ years of historical and play-by-play data, from 90 championships and 24 sports, to Google Cloud Platform (GCP). The NCAA has tapped into decades of historical basketball data using BigQuery, Cloud Spanner, Datalab, Cloud Machine Learning and Cloud Dataflow, to power the analysis of team and player performance. The mission of the NCAA has long been about serving the needs of schools, their teams and students. Google Cloud is proud to support that mission by helping the NCAA use data and machine learning to better engage with its millions of fans, 500,000 student-athletes and more than 19,000 teams. Game on!\n\nThis page is for the NCAA Division I Women's tournament. Check out the NCAA Division I Men's tournament here.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are scored on the log loss:\n$$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right],$$\n\nwhere:\n- n is the number of games played\n- $ \\hat{y}_i $ is the predicted probability of team 1 beating team 2\n- $ y_i $ is 1 if team 1 wins, 0 if team 2 wins\n- $ log() $ is the natural (base e) logarithm\n\nThe use of the logarithm provides extreme punishments for being both confident and wrong. In the worst possible case, a prediction that something is true when it is actually false will add an infinite amount to your error score. In order to prevent this, predictions are bounded away from the extremes by a small value.",
      "dataset_description": "Data description:\n# Google Cloud & NCAA\u00ae ML Competition 2019-Women's: Apply Machine Learning to NCAA\u00ae March Madness\u00ae\n\n## Note for Stage 2:\nAll of the data files have been updated through the end of the current regular season. You can find everything you need in these two files:\n1) Stage2WDataFiles.zip (this contains the same type of information as the WDataFiles.zip did for Stage 1, but it now includes 2019 data as well)\n2) WSampleSubmissionStage2.csv (this has the proper number of rows, and the proper teams for the 2019 tourney only, which is all you predict in Stage 2)\n\nEach season there are thousands of NCAA\u00ae basketball games played between Division I women's teams, culminating in March Madness, the 64-team national championship that starts in the middle of March. We have provided a large amount of historical data about college basketball games and teams, going back many years. Armed with this historical data, you can explore it and develop your own distinctive ways of predicting March Madness\u00ae game outcomes. You can even evaluate and compare different approaches by seeing which of them would have done best at predicting tournament games from the past.\n\nIf you are unfamiliar with the format and intricacies of the tournament, we encourage reading the Wikipedia page before diving into the data. The data description and schema may seem daunting at first, but is not as complicated as it appears.\n\nAs a reminder, you are encouraged to incorporate your own sources of data. We have provided extensive historical data to jump-start the modeling process, and this data is self-consistent (for instance, dates and team ID's are always treated the same way). Nevertheless, you may also be able to make productive use of external data. If you head down this path, please be forewarned that many sources have their own distinctive way of identifying the names of teams, and this can make it challenging to link up with our data. The WTeamSpellings file, which is listed in the bottom section below, may help you map external team references into our own Team ID structure, and you may also need to understand exactly how dates work in our data.\n\nPlease also note that we have standardized the spelling of column names and some filenames, and we have assigned a \"W\" prefix to all files pertinent to women's college basketball, so if you are re-using code from the men's contest, you may need to adjust for this.\n\nWe extend our gratitude to Kenneth Massey for providing much of the historical data.\n\nSpecial Acknowledgment to Jeff Sonas of Sonas Consulting for his support in assembling the dataset for this competition.\n\n## What to predict\nStage 1 - You should submit predicted probabilities for every possible matchup in the past 5 NCAA\u00ae tournaments (2014-2018).\nStage 2 - You should submit predicted probabilities for every possible matchup before the 2019 tournament begins.\nRefer to the Timeline page for specific dates. In both stages, the sample submission will tell you which games to predict.\n\n## File descriptions\nBelow we describe the format and fields of the contest data files. The data will likely be refreshed once in late February while Stage 1 of the competition is running. Many of the files are only complete through the end of last season. At the start of Stage 2, we will provide updates to these files to incorporate data from the current season.\n\n### Data Section 1 - The Basics\nThis section provides everything you need to build a simple prediction model and submit predictions.\n- Team ID's and Team Names\n- Tournament seeds since 1997-98 season\n- Final scores of all regular season, conference tournament, and NCAA\u00ae tournament games since 1997-98 season\n- Season-level details including dates and region names\n- Example submission file for stage 1\n\n**Special note about \"Season\" numbers:**\nThe college basketball season lasts from early November until the national championship tournament that starts in the middle of March. For instance, this year the first women's Division I games were played on November 8th, 2018 and the women's national championship game will be played on April 7th, 2019. Because a basketball season spans two calendar years like this, it can be confusing to refer to the year of the season. By convention, when we identify a particular season, we will reference the year that the season ends in, not the year that it starts in. So for instance, the current season will be identified in our data as the 2019 season, not the 2018 season or the 2018-19 season or the 2018-2019 season, though you may see any of these in everyday use outside of our data.\n\n**Data Section 1 file: WTeams.csv**\nThis file identifies the different college teams present in the dataset. Each school is uniquely identified by a 4 digit id number. You will not see games present for all teams in all seasons, because the games listing is only for matchups where both teams are Division-I teams. There are 353 teams currently in Division-I, and an overall total of 366 teams in our team listing (each year, some teams might start being Division-I programs, and others might stop being Division-I programs). This year there are two teams that are new to Division I: Cal Baptist (TeamID=3465) and North Alabama (TeamID=3466), and so you will not see any historical data for these teams prior to the current season.\n\n- TeamID- a 4 digit id number, from 3000-3999, uniquely identifying each NCAA\u00ae women's team. A school's TeamID does not change from one year to the next, so for instance the Duke women's TeamID is 3181 for all seasons. To avoid possible confusion between the men's data and the women's data, all of the men's team ID's range from 1000-1999, whereas all of the women's team ID's range from 3000-3999.\n- TeamName- a compact spelling of the team's college name, 16 characters or fewer. There are no commas or double-quotes in the team names, but you will see some characters that are not letters or spaces, e.g., Texas A&M, St Mary's CA, TAM C. Christi, and Bethune-Cookman.\n\n**Data Section 1 file: WSeasons.csv**\nThis file identifies the different seasons included in the historical data, along with certain season-level properties.\n\n- Season- indicates the year in which the tournament was played. Remember that the current season counts as 2019.\n- DayZero- tells you the date corresponding to daynum=0 during that season. All game dates have been aligned upon a common scale so that (each year) Selection Monday is on day 133. All game data includes the day number in order to make it easier to perform date calculations. If you need to know the exact date a game was played on, you can combine the game's \"daynum\" with the season's \"dayzero\". For instance, since day zero during the 2011-2012 season was 10/31/2011, if we know that the earliest regular season games that year were played on daynum=7, they were therefore played on 11/07/2011.\n- RegionW, RegionX, Region Y, Region Z- by convention, the four regions in the final tournament are always named W, X, Y, and Z. Whichever region's name comes first alphabetically, that region will be Region W. And whichever Region plays against Region W in the national semifinals, that will be Region X. For the other two regions, whichever region's name comes first alphabetically, that region will be Region Y, and the other will be Region Z. This allows us to identify the regions and brackets in a standardized way in other files. For instance, during the 2012 tournament, the four regions were DesMoines, Fresno, Kingston, and Raleigh. Being the first alphabetically, DesMoines becomes W. Since the Fresno regional champion (Stanford) played against the DesMoines regional champion (Baylor) in the national semifinals, that makes Fresno be region X. For the other two (Kingston and Raleigh), since Kingston comes first alphabetically, that makes Kingston Y and therefore Raleigh is Z. So for that season, the W/X/Y/Z are DesMoines,Fresno,Kingston,Raleigh. And so for instance, Baylor, the #1 seed in the DesMoines region, is listed in the WNCAATourneySeeds file with a seed of W01, meaning they were the #1 seed in the W region (the DesMoines region). We will not know the final W/X/Y/Z designations until Selection Monday, because the national semifinal pairings in the Final Four will depend upon the overall ranks of the four #1 seeds.\n\nThe game dates in this dataset are expressed in relative terms, as the number of days since the start of the regular season, and aligned for each season so that day number #133 is the Monday right before the tournament, when team selections are made. During any given season, day number zero is defined to be exactly 19 weeks earlier than Selection Monday, so Day #0 is a Monday in late October or early November such that Day #132 is Selection Sunday (for the men's tournament) and Day #133 is Selection Monday (for the women's tournament).\n\nThis doesn't necessarily mean that the regular season will always start exactly on Day #0; in fact, during the past decade, regular season games typically start being played on a Friday that is either Day #4 or Day #11, but further back there was more variety.\n\n**Data Section 1 file: WNCAATourneySeeds.csv**\nThis file identifies the seeds for all teams in each NCAA\u00ae tournament, for all seasons of historical data. Thus, there are exactly 64 rows for each year, since there are no play-in teams in the women's tournament. We will not know the seeds of the respective tournament teams, or even exactly which 64 teams it will be, until Selection Monday on March 18, 2019.\n\n- Season- the year that the tournament was played in\n- Seed- this is a 3-character identifier of the seed, where the first character is either W, X, Y, or Z (identifying the region the team was in) and the next two digits (either 01, 02, ..., 15, or 16) tell you the seed within the region. For example, the first record in the file is seed W01, which means we are looking at the #1 seed in the W region (which we can see from the \"WSeasons.csv\" file was the East region).\n- TeamID- this identifies the id number of the team, as specified in the WTeams.csv file\n\n**Data Section 1 file: WRegularSeasonCompactResults.csv**\nThis file identifies the game-by-game results for many seasons of historical data, starting with the 1998 season. For each season, the file includes all games played from daynum 0 through 132. It is important to realize that the \"Regular Season\" games are simply defined to be all games played on DayNum=132 or earlier (DayNum=133 is Selection Monday). Thus a game played before Selection Monday will show up here whether it was a pre-season tournament, a non-conference game, a regular conference game, a conference tournament game, or whatever.\n\n- Season- this is the year of the associated entry in WSeasons.csv (the year in which the final tournament occurs). For example, during the 2016 season, there were regular season games played between November 2015 and March 2016, and all of those games will show up with a Season of 2016.\n- DayNum- this integer always ranges from 0 to 132, and tells you what day the game was played on. It represents an offset from the \"DayZero\" date in the \"WSeasons.csv\" file. For example, the first game in the file was DayNum=18. Combined with the fact from the \"WSeasons.csv\" file that day zero was 10/27/1997 that year, this means the first game was played 18 days later, or 11/14/1997. There are no teams that ever played more than one game on a given date, so you can use this fact if you need a unique key (combining Season and DayNum and WTeamID).\n- WTeamID- this identifies the id number of the team that won the game, as listed in the \"WTeams.csv\" file. No matter whether the game was won by the home team or visiting team, or if it was a neutral-site game, the \"WTeamID\" always identifies the winning team.\n- WScore- this identifies the number of points scored by the winning team.\n- LTeamID- this identifies the id number of the team that lost the game.\n- LScore- this identifies the number of points scored by the losing team. Thus you can be confident that WScore will be greater than LScore for all games listed.\n- NumOT- this indicates the number of overtime periods in the game, an integer 0 or higher.\n- WLoc- this identifies the \"location\" of the winning team. If the winning team was the home team, this value will be \"H\". If the winning team was the visiting team, this value will be \"A\". If it was played on a neutral court, then this value will be \"N\". Sometimes it is unclear whether the site should be considered neutral, since it is near one team's home court, or even on their court during a tournament, but for this determination we have simply used the Kenneth Massey data in its current state, where the \"@\" sign is either listed with the winning team, the losing team, or neither team. If you would like to investigate this factor more closely, we invite you to explore Data Section 3, which provides the city that each game was played in, irrespective of whether it was considered to be a neutral site.\n\n**Data Section 1 file: WNCAATourneyCompactResults.csv**\nThis file identifies the game-by-game NCAA\u00ae tournament results for all seasons of historical data. The data is formatted exactly like the WRegularSeasonCompactResults data. Each season you will see 63 games listed, since there are no women's play-in games.\n\nAlthough the scheduling of the men's tournament rounds has been consistent for many years, there has been more variety in the scheduling of the women's rounds. There have been four different schedules over the course of the past 20 years for the women's tournament, as follows:\n\n2017 season through 2019 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)\n- Round 4 = days 146/147 (Elite Eight, Sun/Mon)\n- National Seminfinal = day 151 (Fri)\n- National Final = day 153 (Sun)\n\n2015 season and 2016 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = days 144/145 (Sweet Sixteen, Fri/Sat)\n- Round 4 = days 146/147 (Elite Eight, Sun/Mon)\n- National Seminfinal = day 153 (Sun)\n- National Final = day 155 (Tue)\n\n2003 season through 2014 season:\n- Round 1 = days 138/139 (Sat/Sun)\n- Round 2 = days 140/141 (Mon/Tue)\n- Round 3 = days 145/146 (Sweet Sixteen, Sat/Sun)\n- Round 4 = days 147/148 (Elite Eight, Mon/Tue)\n- National Seminfinal = day 153 (Sun)\n- National Final = day 155 (Tue)\n\n1998 season through 2002 season:\n- Round 1 = days 137/138 (Fri/Sat)\n- Round 2 = days 139/140 (Sun/Mon)\n- Round 3 = day 145 only (Sweet Sixteen, Sat)\n- Round 4 = day 147 only (Elite Eight, Mon)\n- National Seminfinal = day 151 (Fri)\n- National Final = day 153 (Sun)\n\n**Data Section 1 file: WSampleSubmissionStage1.csv**\nThis file illustrates the submission file format for Stage 1. It is the simplest possible submission: a 50% winning percentage is predicted for each possible matchup.\n\nA submission file lists every possible matchup between tournament teams for one or more years. During Stage 1, you are asked to make predictions for all possible matchups from the past five NCAA\u00ae tournaments (seasons 2014, 2015, 2016, 2017, and 2018). In Stage 2, you will be asked to make predictions for all possible matchups from the current NCAA\u00ae tournament (season 2019).\n\nSince there are 64 teams in the tournament, there are 64*63/2=2,016 predictions to make for that year, so a Stage 1 submission file will have 2,016*5=10,080 data rows.\n\n- ID- this is a 14-character string of the format SSSS_XXXX_YYYY, where SSSS is the four digit season number, XXXX is the four-digit TeamID of the lower-ID team, and YYYY is the four-digit TeamID of the higher-ID team.\n- Pred- this contains the predicted winning percentage for the first team identified in the ID field, the one represented above by XXXX.\n\nExample #1: You want to make a prediction for Duke (TeamID=3181) against Arizona (TeamID=3112) in the 2005 tournament, with Duke given a 53% chance to win and Arizona given a 47% chance to win. In this case, Arizona has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Arizona's perspective (47%):\n2005_3112_3181,0.47\n\nExample #2: You want to make a prediction for Duke (TeamID=3181) against North Carolina (TeamID=3314) in the 2005 tournament, with Duke given a 51.6% chance to win and North Carolina given a 48.4% chance to win. In this case, Duke has the lower numerical ID so they would be listed first, and the winning percentage would be expressed from Duke's perspective (51.6%):\n2005_3181_3314,0.516\n\n### Data Section 2 - Team Box Scores\nThis section provides game-by-game stats at a team level (free throws attempted, defensive rebounds, turnovers, etc.) for all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season.\n\nTeam Box Scores are provided in \"Detailed Results\" files rather than \"Compact Results\" files. However, the two files are strongly related.\n\nIn a Detailed Results file, the first eight columns (Season, DayNum, WTeamID, WScore, LTeamID, LScore, WLoc, and NumOT) are exactly the same as a Compact Results file. However, in a Detailed Results file, there are many additional columns. The column names should be self-explanatory to basketball fans (as above, \"W\" or \"L\" refers to the winning or losing team):\n\n- WFGM- field goals made (by the winning team)\n- WFGA- field goals attempted (by the winning team)\n- WFGM3- three pointers made (by the winning team)\n- WFGA3- three pointers attempted (by the winning team)\n- WFTM- free throws made (by the winning team)\n- WFTA- free throws attempted (by the winning team)\n- WOR- offensive rebounds (pulled by the winning team)\n- WDR- defensive rebounds (pulled by the winning team)\n- WAst- assists (by the winning team)\n- WTO- turnovers committed (by the winning team)\n- WStl- steals (accomplished by the winning team)\n- WBlk- blocks (accomplished by the winning team)\n- WPF- personal fouls committed (by the winning team)\n\n(and then the same set of stats from the perspective of the losing team: LFGM is the number of field goals made by the losing team, and so on up to LPF).\n\nNote: by convention, \"field goals made\" (either WFGM or LFGM) refers to the total number of fields goals made by a team, a combination of both two-point field goals and three-point field goals. And \"three point field goals made\" (either WFGM3 or LFGM3) is just the three-point fields goals made, of course. So if you want to know specifically about two-point field goals, you have to subtract one from the other (e.g., WFGM - WFGM3). And the total number of points scored is most simply expressed as 2*FGM + FGM3 + FTM.\n\n**Data Section 2 file: WRegularSeasonDetailedResults.csv**\nThis file provides team-level box scores for many regular seasons of historical data, starting with the 2010 season. All games listed in the WRegularSeasonCompactResults file since the 2010 season should exactly be present in the WRegularSeasonDetailedResults file.\n\n**Data Section 2 file: WNCAATourneyDetailedResults.csv**\nThis file provides team-level box scores for many NCAA\u00ae tournaments, starting with the 2010 season. All games listed in the WNCAATourneyCompactResults file since the 2010 season should exactly be present in the WNCAATourneyDetailedResults file.\n\n### Data Section 3 - Geography\nThis section provides city locations of all regular season, conference tournament, and NCAA\u00ae tournament games since the 2009-10 season\n\n**Data Section 3 file: WCities.csv**\nThis file provides a master list of cities that have been locations for games played.\n\n- CityID- a four-digit ID number uniquely identifying a city.\n- City- the text name of the city.\n- State- the state abbreviation of the state that the city is in. In a few rare cases, the game location is not inside one of the 50 U.S. states and so other abbreviations are used, for instance Cancun, Mexico has a state abbreviation of MX.\n\n**Data Section 3 file: WGameCities.csv**\nThis file identifies all games, starting with the 2010 season, along with the city that the game was played in. Games from the regular season and the NCAA\u00ae tourney are all listed together.\n\n- Season, DayNum, WTeamID, LTeamID- these four columns are sufficient to uniquely identify each game. Additional data, such as the score of the game and other stats, can be found in the corresponding Compact Results file.\n- CRType- this can be either Regular or NCAA. If it is Regular, you can find more about the game in the WRegularSeasonCompactResults.csv file. If it is NCAA, you can find more about the game in the WNCAATourneyCompactResults.csv file.\n- CityID- the ID of the city where the game was played, as specified by the CityID column in the WCities.csv file.\n\n### Data Section 4 - Supplements\nThis section contains additional supporting information, including alternative team name spellings and representations of bracket structure\n\n**Data Section 4 file: WTeamSpellings.csv**\nThis file indicates alternative spellings of many team names. It is intended for use in associating external spellings against our own TeamID numbers, thereby helping to relate the external data properly with our datasets. Over the years we have identified various external spellings of different team names (as an example, for Ball State we have seen \"ball st\", and \"ball st.\", and \"ball state\", and \"ball-st\", and \"ball-state\"). Other teams have had more significant changes to their names over the years; for example, \"Texas Pan-American\" and \"Texas-Rio Grande Valley\" are actually the same school. The current list is obviously not exhaustive, and we encourage participants to identify additional mappings and upload extended versions of this file to the forums.\n\n- TeamNameSpelling- this is the spelling of the team name. It is always expressed in all lowercase letters - e.g. \"ball state\" rather than \"Ball State\" - in order to emphasize that any comparisons should be case-insensitive when matching.\n- TeamID- this identifies the TeamID for the team that has the alternative spelling (as described in WTeams.csv).\n\n**Data Section 4 file: WNCAATourneySlots**\nThis file identifies the mechanism by which teams are paired against each other, depending upon their seeds, as the tournament proceeds through its rounds. It can be of use in identifying, for a given historical game, what round it occurred in, and what the seeds/slots were for the two teams (the meaning of \"slots\" is described below). You may need to know these specifics if you are trying to represent/simulate the exact workings of the tournament bracket. Unlike the analogous file on the men's side, it is not necessary to provide a Season within this file, because the women's tournament has never had play-in-games and so the 64-team women's bracket has always had the same structure each season.\n\n- Slot- this uniquely identifies one of the tournament games. It is a four-character string, where the first two characters tell you which round the game is (R1, R2, R3, R4, R5, or R6) and the second two characters tell you the expected seed of the favored team. Thus the first row is R1W1, identifying the Round 1 game played in the W bracket, where the favored team is the 1 seed. As a further example, the R2W1 slot indicates the Round 2 game that would have the 1 seed from the W bracket, assuming that all favored teams have won up to that point. The slot names are different for the final two rounds, where R5WX identifies the national semifinal game between the winners of regions W and X, and R5YZ identifies the national semifinal game between the winners of regions Y and Z, and R6CH identifies the championship game. The \"slot\" value is used in other columns in order to represent the advancement and pairings of winners of previous games.\n- StrongSeed- this indicates the expected stronger-seeded team that plays in this game. For Round 1 games, a team seed is identified in this column (as listed in the \"Seed\" column in the WNCAATourneySeeds.csv file), whereas for subsequent games, a slot is identified in this column. In the first record of this file (slot R1W1), we see that seed W01 is the \"StrongSeed\". Whereas for games from Round 2 or later, rather than a team seed, we will see a \"slot\" referenced in this column. So in the 33rd record of this file (slot R2W1), it tells us that the winners of slots R1W1 and R1W8 will face each other in Round 2. Of course, in the last few games of the tournament - the national semifinals and finals - it's not really meaningful to talk about a \"strong seed\" or \"weak seed\", since you would have #1 seeds favored to face each other, but those games are nevertheless represented in the same format for the sake of consistency.\n- WeakSeed- this indicates the expected weaker-seeded team that plays in this game, assuming all favored teams have won so far.",
      "metadata": {
        "domain": "sports",
        "keywords": [
          "probabilistic binary classification",
          "tabular",
          "feature engineering",
          "pairwise modeling",
          "logloss"
        ]
      }
    },
    {
      "challenge_name": "youtube8m-2018",
      "description": "Challenge description:\n# The 2nd YouTube-8M Video Understanding Challenge\n\n## Description\n\nThe world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day.\n\nTo spur advances in analyzing and understanding video, Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.\n\nFor a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user's personal mobile phones.\n\nIn this competition, you're challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.\n\nThis competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.\n\n## Evaluation\n\nSubmissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos.\n\nIf a submission has N predictions (label/confidence pairs) sorted by its confidence score, then the Global Average Precision is computed as:\n\n$$GAP = \\sum_{i=1}^N p(i) \\Delta r(i)$$\n\nwhere N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos), p(i) is the precision, and r(i) is the recall.\n\nA python implementation of GAP can be found at youtube-8m's github.\n\n### Submission File\n\nFor each VideoId in the test set, you must predict a list of labels and their corresponding confidence scores. The file should contain a header and have the following format:\n\nVideoId,LabelConfidencePairs\n000c,1 0.5 2 0.3 3 0.1 4 0.05 5 0.05\netc.\n\n## Prizes\n\nEach of the top 5 ranked teams (on the final private leaderboard) will receive $5,000 per team as a travel award to attend the ECCV 2018 Conference. Prize eligibility requires adherence to the Competition Rules.\n\n## Timeline\n\nJuly 30, 2018 - Entry deadline. You must accept the competition rules before this date in order to compete.\n\nJuly 30, 2018 - Team Merger deadline. This is the last day participants may join or merge teams.\n\nAugust 6, 2018 - Final submission & Model upload deadline. Ensure compliance with the Competition Rules (also reiterated under Important Special Requirements)\n\nAugust 13, 2018 - Paper submission deadline. Winners' obligations deadline.\n\nWhile awards will only be distributed as detailed in the Prizes section, any participant, regardless of ranking, is encouraged to submit a paper to the YouTube-8M 2nd Large-Scale Video Understanding Workshop, co-located at ECCV 2018.\n\nAugust 22, 2018 - Paper acceptance & Final winners confirmation.\n\nSeptember 9, 2018 - YouTube-8M Workshop at ECCV 2018.\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Getting Started\n\nStarter Code can be found within the public google/youtube8m github repo.\n\nA Starter Kernel has been created to help with exploring a sample of the dataset.\n\nInformation about the YouTube-8M 2nd Video Understanding Workshop at ECCV 2018.\n\n## Important Special Requirements\n\n### ENTRANTS' MODEL UPLOAD REQUIREMENT\n\nIn order to be eligible to be ranked on the final leaderboard for this competition, each team's final submission must include the 2 models uploaded (one per final selected submission), via Team-> Your Model, per the Competition Rules. Each model file:\n\n(A) must be a .tgz file containing the model architecture and its parameters. If the competition organizers are unable to run inference.py on the .tgz as the input to the flag --input_model_tgz then the entry will be disqualified. Note that this means that the final model must be loadable as a TensorFlow MetaGraph. The .tgz can be created using inference.py with the --output_model_tgz flag. It is possible to create your own .tgz file, without using our starter code, but you must verify its correctness by calling inference.py with flag --input_model_tgz instead of the flag --train_dir=.\n\n(B) must match that which was used to generate your selected submissions.\n\n(C) must total less than 1 GB when uncompressed.\n\n(D) must be trained only on the train and validate datasets. Test video IDs and features cannot be used during model training and are to be used only for model evaluation through the public leaderboard.\n\nThis requirement is in place for the host team to verify the performance of the uploaded models matches the selected submission files and that the models have not used test data to bypass the model size requirement.\n\nCompliance with the above will be verified by the host team. Submitters who fail to upload their models by the final submission deadline, or are found not to be in compliance, will be disqualified and removed from the final leaderboard.\n\n### WINNERS' OBLIGATIONS\n\nBecause this competition is being hosted in coordination with the ECCV 2018 conference, winners must adhere to the following special obligations, in addition to standard winners' obligations detailed in the General Competition Rules.\n\n### SPECIAL TIMELINE\n\nThe timeline for return of winners' materials is compressed. Teams that fail to upload their models by 11:59 PM UTC August 6, 2018 will be removed from the leaderboard and will not be eligible for any prizes. The competition organizers reserve the right to update this date if they deem it necessary. Review the full timeline on the Timeline section.\n\n### PAPER SUBMISSION REQUIREMENT\n\nAlso as a condition of receipt of the Prize, the Prize winner must write a paper for the YouTube-8M 2nd Large-Scale Video Understanding Workshop as part of the ECCV 2018 conference, within the required deadlines and to the requirements specified by the Competition Sponsor. Papers must be submitted by 11:59 PM UTC August 13, 2018. The competition organizers reserve the right to update the this date if they deem it necessary. Review the full timeline on the Timeline section.\n\n### WINNER LICENSE\n\nYou will adhere to the following license requirements, with respect to your Submission if you are a Competition winner:\n\nOpen Source: You will license your winning Submission and the source code used to generate the Submission under one of the following approved licenses that in no event limits commercial use of such code or model containing or depending on such code:\n- Apache 2.0\n- BSD 2-clause or 3-clause\n- LGPL\n- MIT\n\n## Citation\n\nApostol \"Paul\" Natsev, Balakrishnan V, George Toderici, Hanhan Li, Joonseok Lee, Julia Elliott, LeegleechN, Sami Abu-El-Haija, Sobi, and Wendy Kan. The 2nd YouTube-8M Video Understanding Challenge. https://kaggle.com/competitions/youtube8m-2018, 2018. Kaggle.\n\n## Competition Host\n\nGoogle Research\n\n## Participation\n\n3,346 Entrants\n380 Participants\n312 Teams\n2,554 Submissions\n\nData description:\nThe 2nd YouTube-8M Video Understanding Challenge\nCan you create a constrained-size model to predict video labels?\nGoogle Research \u00b7 Featured Prediction Competition \u00b7 7 years ago Late Submission\n\nDataset Description\nIn this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage.\nThe training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.\n\nFile descriptions\nvideo-level data\nYou may download to your local computer with instructions here\nEach video has\nid: unique id for the video, in train set it is a Youtube video id, and in test/validation they are anonymized\nlabels: list of labels of that video\nmean_rgb: float array of length 1024\nmean_audio: float array of length 128\nFiles are in TFRecords format, TensorFlow python readers are available in the github repo, and simple reader available in Kaggle Kernels\nvideo_sample.zip- a sample of video-level data including train00 and train01\n\nframe-level data\nYou may download to your local computer with instructions here\nTotal size of 1.53TB (Large file warning!)\nEach video has\nid: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.\nlabels: list of labels of that video.\nEach frame has\nrgb: float array of length 1024\nEach frame has\naudio: float array of length 128\nFiles are in TFRecords format, TensorFlow python readers are available in the github repo.\nframe_sample.zip- a sample of frame-level data including train00 and train01\n\nlabel_names_2018.csv- a mapping between label_id and label_name\nvocabulary.csv- the full data dictionary for label names and their descriptions\nsample_submission.csv- a sample submission file in the correct format\nEach video has\nVideoId- the id of the video\nLabelConfidencePairs- space delimited label/prediction pairs ordered by descending confidence.\n\nFiles 11 files\nSize 1.1 GB\nType zip, csv, rtf\nLicense Subject to Competition Rules",
      "docker_challenge_path": "/data/youtube8m-2018",
      "competition_description": "## Description\n\nThe world is generating and consuming an enormous amount of video content. Currently on YouTube, people watch over 1 billion hours of video every single day.\n\nTo spur advances in analyzing and understanding video, Google AI has publicly released a large-scale video dataset that consists of millions of YouTube video features and associated labels from a diverse vocabulary of 3,700+ visual entities called the YouTube-8M Dataset. Last year, we successfully hosted Google Cloud & YouTube-8M Video Understanding Challenge, with 742 participating teams with 946 individual competitors from 60 countries. This competition is the second Kaggle competition based on YouTube 8M dataset, and is focused on learning video representation under budget constraints.\n\nFor a lot of video tasks where there are a large number of classes, like recommending new videos or automatic video classification, compact models need to meet memory and computational requirements. This is true even if working in cloud computational environments. Also, compact models make it possible to have limited-memory or catalog indexes on devices in order to do personalized and privacy-preserving computation on user's personal mobile phones.\n\nIn this competition, you're challenged to produce a compact video classification model. Your model size must not exceed 1 GB (this is strictly enforced, through model upload). We encourage participants to train a model that most efficiently uses this budget, rather than ensembles of lots of models.\n\nThis competition is being hosted by Google AI (previously known as Google Research) as a part of the European Conference on Computer Vision (ECCV) 2018 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",
      "evaluation_metric": "## Evaluation\n\nSubmissions are evaluated the Global Average Precision (GAP) at k, where k=20. For each video, you will submit a list of predicted labels and their corresponding confidence scores. The evaluation takes the predicted labels that have the highest k confidence scores for each video, then treats each prediction and the confidence score as an individual data point in a long list of global predictions, to compute the Average Precision across all of the predictions and all the videos.\n\nIf a submission has N predictions (label/confidence pairs) sorted by its confidence score, then the Global Average Precision is computed as:\n\n$$GAP = \\sum_{i=1}^N p(i) \\Delta r(i)$$\n\nwhere N is the number of final predictions (if there are 20 predictions for each video, then N = 20 * #Videos), p(i) is the precision, and r(i) is the recall.\n\nA python implementation of GAP can be found at youtube-8m's github.",
      "dataset_description": "Dataset Description\nIn this competition, you will predict the labels of a YouTube video. We provide you extracted frame-level and video-level features. The feature data and detailed feature information can be found on the YouTube-8M dataset webpage.\nThe training dataset in this competition contain videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.\n\nFile descriptions\nvideo-level data\nYou may download to your local computer with instructions here\nEach video has\nid: unique id for the video, in train set it is a Youtube video id, and in test/validation they are anonymized\nlabels: list of labels of that video\nmean_rgb: float array of length 1024\nmean_audio: float array of length 128\nFiles are in TFRecords format, TensorFlow python readers are available in the github repo, and simple reader available in Kaggle Kernels\nvideo_sample.zip- a sample of video-level data including train00 and train01\n\nframe-level data\nYou may download to your local computer with instructions here\nTotal size of 1.53TB (Large file warning!)\nEach video has\nid: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.\nlabels: list of labels of that video.\nEach frame has\nrgb: float array of length 1024\nEach frame has\naudio: float array of length 128\nFiles are in TFRecords format, TensorFlow python readers are available in the github repo.\nframe_sample.zip- a sample of frame-level data including train00 and train01\n\nlabel_names_2018.csv- a mapping between label_id and label_name\nvocabulary.csv- the full data dictionary for label names and their descriptions\nsample_submission.csv- a sample submission file in the correct format\nEach video has\nVideoId- the id of the video\nLabelConfidencePairs- space delimited label/prediction pairs ordered by descending confidence.\n\nFiles 11 files\nSize 1.1 GB\nType zip, csv, rtf\nLicense Subject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "multi-label classification",
          "images+time_series",
          "feature aggregation",
          "model compression",
          "map"
        ]
      }
    },
    {
      "challenge_name": "youtube8m-2019",
      "description": "Challenge description:\n# The 3rd YouTube-8M Video Understanding Challenge: Temporal localization of topics within video\n\nImagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn't title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby's first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.\n\nAn example of the detected action \"blowing out candles\"\n\nIn most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren't always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others.\n\nIn previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?\n\nIf successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear.\n\nThis competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.\n\n## Evaluation\n\nSubmissions are evaluated according to the Mean Average Precision @ K (MAP@K), where \\( K = 100,000 \\).\n\n$$MAP@100,000 = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{\\sum_{k=1}^{n} P(k) \\times rel(k)}{N_{c}}$$\n\nwhere \\( C \\) is the number of Classes, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number of Segments predicted per Class, \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) Class, or zero otherwise, and \\( N_{c} \\) is the number of positively-labeled segments for the each Class.\n\n**IMPORTANT:** The evaluation for this competition is different in some important ways:\n- As noted above, for each Class you are predicting relevant Segments (and not the other way around).\n- Not all test segments have been human-rated, and only human-rated segments are used in scoring. All segments that were not explicitly rated (as either containing a Class, or not containing a Class) are removed from the prediction list before scoring.\n- The Public/Private Test split is performed at the Segment level, not the Class level. In other words, all classes (i.e., submission rows) are evaluated for the Public and Private leaderboard, but only segments for the particular split will be used in the prediction and ground truth.\n\nA python implementation of MAP@K can be found at youtube-8m's github.\n\n### Submission File\n\nFor each of the 1,000 Class values in the sample_submission.csv file, you may predict up to 100,000 Segment IDs that are predicted to contain that Class, sorted in order of confidence.\n\n**IMPORTANT:** In order to minimize submission file sizes, for segment predictions, you should only include the video id and the segment start time, but not the segment end time. (These are not needed, since all segments are 5 seconds in duration.)\n\nThe file should contain a header and have the following format:\n```\nClass,Segments\n3,002G:35002G:40002G:60...\n7,002G:35002G:40002G:60...\n```\n\n## Timeline\n\n- September 20, 2019 - 1st Round Paper submission deadline. While awards will only be distributed as detailed in the Prizes section, any participant, regardless of ranking, is encouraged to submit a paper to the YouTube-8M 3rd Large-Scale Video Understanding Workshop, co-located at ICCV 2019, especially if you want to arrange your travel in advance.\n- September 24, 2019 - 1st Round Paper acceptance notification.\n- October 4, 2019 - Entry deadline. You must accept the competition rules before this date in order to compete.\n- October 4, 2019 - Team Merger deadline. This is the last day participants may join or merge teams.\n- October 11, 2019 - Final submission deadline.\n- October 18, 2019 - Final Paper submission deadline. Winners' obligations deadline. Prize recipients are required to fulfill paper submission requirements as a condition of receiving the travel award prize. This paper obligation can be fulfilled in the 1st round (see above).\n- October 22, 2019 - Final Paper acceptance notification & Final winners announcement.\n- October 25, 2019 - Accepted papers' camera-ready deadline.\n- October 28, 2019 - 3rd YouTube-8M Video Understanding Challenge Workshop at ICCV 2019\n\nAll deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n\n## Prizes\n\nEach of the top 10 ranked teams (on the final private leaderboard) will receive $2,500 per team as a travel award to attend and present at the 3rd YouTube-8M Video Understanding Challenge Workshop at the ICCV 2019 Conference.\n\n## Getting Started\n\nWe provide Starter code that implements two simple algorithms for this year's competition task. Please follow the steps below to try out.\n\nA Starter Kernel may be useful for you to explore the dataset.\n\n### Running on a local Linux machine\n\n0. Downloading the starter code\n```\nmkdir -p ~/yt8m/code\ncd ~/yt8m/code\ngit clone https://github.com/google/youtube-8m.git\n```\n\n1. Downloading the dataset\n\n1-1. Downloading training set\n```\nmkdir -p ~/yt8m/2/frame/train\ncd ~/yt8m/2/frame/train\ncurl data.yt8m.org/download.py | partition=2/frame/train mirror=us python\n```\nThe above will take a long time and need ~2TB of disk space. To test only with 1% of the training set:\n```\ncurl data.yt8m.org/download.py | shard=1,100 partition=2/frame/train mirror=us python\n```\n(If you are running this in Europe or Asia, use 'eu' or 'asia' instead of 'us' for 'mirror' above.)\n\n1-2. Downloading validation set\n```\nmkdir -p ~/yt8m/3/frame/validate\ncd ~/yt8m/3/frame/validate\ncurl data.yt8m.org/download.py | partition=3/frame/validate mirror=us python\n```\n\n1-3. Downloading test set\n```\nmkdir -p ~/yt8m/3/frame/test\ncd ~/yt8m/3/frame/test\ncurl data.yt8m.org/download.py | partition=3/frame/test mirror=us python\n```\n\n2. Checking python and tensorflow version\n```\npython --version\npython -c 'import tensorflow as tf; print(tf.__version__)'\n```\nPython 3.6+ and Tensorflow 1.14 is required to run the starter code.\n\n3. Training\n\n3-0. Go to the directory with the sample code.\n```\ncd ${HOME}/yt8m/code/youtube-8m\n```\n\n3-1. Frame-level logistic regression model\n```\npython3 train.py --frame_features --model=FrameLevelLogisticModel --feature_names=\"rgb,audio\" --feature_sizes=\"1024,128\" --train_data_pattern=${HOME}/yt8m/2/frame/train/train*.tfrecord --train_dir=\"${HOME}/yt8m/models/frame/sample_model_logistic\" --start_new_model\n```\n\n3-2. DBoF model\n```\npython3 train.py --frame_features --model=DbofModel --feature_names=\"rgb,audio\" --feature_sizes=\"1024,128\" --train_data_pattern=${HOME}/yt8m/2/frame/train/train*.tfrecord --train_dir=\"${HOME}/yt8m/models/frame/sample_model_dbof\" --start_new_model\n```\n\n4. Evaluation\n```\npython3 eval.py --eval_data_pattern=${HOME}/yt8m/3/frame/validate/validate*.tfrecord --train_dir ${HOME}/yt8m/models/frame/sample_model_logistic --segment_labels --run_once\npython3 eval.py --eval_data_pattern=${HOME}/yt8m/3/frame/validate/validate*.tfrecord --train_dir ${HOME}/yt8m/models/frame/sample_model_dbof --segment_labels --run_once\n```\n\n5. Inference: Creating the submission file\n\nFor CPU-only inference, it may be necessary to reduce the batch_size to smaller number. In the example command below, we set 64 instead of 8192 (default).\n```\npython3 inference.py --train_dir ${HOME}/yt8m/models/frame/sample_model_logistic --output_file=${HOME}/yt8m/models/frame/sample_model_logistic/kaggle_solution.csv --input_data_pattern=${HOME}/yt8m/3/frame/test/test*.tfrecord --segment_labels --batch_size=64\npython3 inference.py --train_dir ${HOME}/yt8m/models/frame/sample_model_dbof --output_file=${HOME}/yt8m/models/frame/sample_model_dbof/kaggle_solution.csv --input_data_pattern=${HOME}/yt8m/3/frame/test/test*.tfrecord --segment_labels --batch_size=64\n```\n\n### Running on Google Cloud\n\n1. Set up your GCP project\nhttps://cloud.google.com/ml-engine/docs/tensorflow/getting-started-training-prediction\nSee the section title with \"Set up and test your Cloud environment\" for more details.\n\n2. Set up your environment\nLogin with the following command, and follow the instructions:\n```\ngcloud auth login\n```\nThen, set up the project. (It might already have been done. If you see this project id already, you may skip this step.)\n```\ngcloud config set project <project_name>\n```\n\n3. Go to the directory right above the codes\nYou need to run training and eval code right above the directory where start code files are (e.g., train.py). For instance,\n```\ncd ~/yt8m/code\n```\n\n4. Training\nFirst, set up some variables:\n```\nBUCKET_NAME=gs://${USER}_yt8m_train_bucket\ngsutil mb -l us-east1 $BUCKET_NAME\n```\n\nThis example shows running frame-level logistic model. You may run DBoF model from the starter code in a similar way, by setting model=DbofModel.\n```\ngcloud --verbosity=debug ai-platform jobs submit training $JOB_NAME --package-path=youtube-8m --module-name=youtube-8m.train --staging-bucket=$BUCKET_NAME --region=us-east1 --config=youtube-8m/cloudml-gpu.yaml -- --train_data_pattern='${HOME}/yt8m/2/frame/train/train*.tfrecord' --frame_features --model=FrameLevelLogisticModel --feature_names='rgb,audio' --feature_sizes='1024,128' --train_dir=$BUCKET_NAME/yt8m_train_frame_level_logistic_model --start_new_model\n```\n\nIf you see an error like below,\n```\nERROR: (gcloud.ai-platform.jobs.submit.training) FAILED_PRECONDITION: Field: package_uris Error: The provided GCS paths [gs://your_yt8m_train_bucket/yt8m_train_20190615_011711/d5bb0cbec14f724054d4721aecbf5d40a134b680e2699af3d8e58/youtube-8m-0.0.0.tar.gz] cannot be read by service account service-XXXXXXX@cloud-ml.google.com.iam.gserviceaccount.com.\n```\n\nrun these commands: (Edit the service account to what you see in the error.)\n```\ngsutil acl ch -u service-XXXXXXX@cloud-ml.google.com.iam.gserviceaccount.com:WRITE $BUCKET_NAME\ngsutil defacl ch -u service-XXXXXX@cloud-ml.google.com.iam.gserviceaccount.com:O $BUCKET_NAME\n```\n\nThen, retry the training command above.\n\nMonitoring command:\n```\ngcloud ai-platform jobs describe $JOB_NAME\n```\nIt will show a link to Cloud Console, and you can see logs there.\n\n5. Inference\nOnce the training is done, we are ready to do inference. (Evaluation on validation set may be done in a similar way, as instructed in the starter code.)\n```\nJOB_TO_EVAL=yt8m_train_frame_level_logistic_model\nJOB_NAME=yt8m_inference_$(date +%Y%m%d_%H%M%S)\ngcloud --verbosity=debug ai-platform jobs submit training $JOB_NAME \\\n--package-path=youtube-8m --module-name=youtube-8m.inference \\\n--staging-bucket=$BUCKET_NAME --region=us-east1 \\\n--config=youtube-8m/cloudml-gpu.yaml \\\n-- --input_data_pattern='gs://youtube8m-ml/3/frame/test/test*.tfrecord' \\\n--train_dir=$BUCKET_NAME/${JOB_TO_EVAL} --segment_labels \\\n--output_file=$BUCKET_NAME/${JOB_TO_EVAL}/predictions.csv --batch_size=512\n```\n\nHere, JOB_TO_EVAL needs to be equal to the train_dir name specified when starting training.\n\nWe use batch_size of 512 by default, but you might need to adjust it based on your model. With too large batch size, you may encounter an out-of-memory error.\n\n## ICCV'19 Workshop\n\nWe host the 3rd YouTube-8M Large-scale Video Understanding Workshop at ICCV 2019 conference on Oct 28, 2019 at Seoul, Korea.\nFor more information, please visit the workshop website.\n\n## Request GCP Credit\n\nWe're re-opening requests for $300 Coupons in Google Cloud Platform (GCP) Credit to teams competing in this competition. Requests can be made by submitting this form by September 4, 2019. Only one request should be made per team. Teams who have already received a coupon in the prior distribution will not be eligible to receive another coupon.\n\n## FAQ\n\nThis page answers to frequently asked questions in the discussion board. We organizers will keep adding questions and answers during the competition.\n\nQ. Label IDs used in segment labels are the same as last year?\nA. Yes, the label IDs are the same. The vocabulary of YouTube-8M Segment is a subset of YouTube-8M and the same label to ID mapping is used.\n\nQ. Are the segments always in same length?\nA. Yes, all segments are exactly 5 seconds long.\n\nQ. Do the start and end of the segments always happen at 5n seconds (n = 0, 1, 2, \u2026)?\nA. Yes. A segment always starts at 5n and finishes at 5(n+1).\n\nQ. It seems the very beginning and the very last few segments never appear in the validation set. Is this right?\nA. Yes, we intentionally excluded them as they may not have the main content of the video (e.g., title, ending credit).\n\nQ. Are the segment labels a subset of video level labels?\nA. Yes. We sampled segments to rate from videos where the label is positive at video-level.\n\nQ. Does each video have the same number of segments sampled, regardless of video length?\nA. Yes, we sampled 5 segments from each video-label pair. This might change later if we decide to collect more labels, but for this year's competition, this is true.\n\nQ. Can we use validation data for training?\nA. Yes. Validation data is set aside usually for model selection, but it is still okay to use some portion of it for training (with your own risk of increased chance of overfitting). Especially this year, we provide segment-level ground truth only among the validation set. You are encouraged to use these for training/fine-tuning.\n\n## Citation\n\nApostol \"Paul\" Natsev, Balakrishnan V, George Toderici, Hanhan Li, inversion, Joe Ng, Joonseok Lee, Julia Elliott, LeegleechN, and ZHENG XU. The 3rd YouTube-8M Video Understanding Challenge. https://kaggle.com/competitions/youtube8m-2019, 2019. Kaggle.\n\nData description:\nThe 3rd YouTube-8M Video Understanding ChallengeTemporal localization of topics within video\nIn this competition, you will predict theClasslabels of YouTube video segments. We provide you extracted frame-level features. The feature data and detailed feature information can be found on theYouTube-8M datasetwebpage.\nThe training dataset in this competition contains videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.\nFile descriptions\nframe-level data\nYou may download to your local computer with instructionshere\nTotal size of 1.53TB\u00a0(Large file warning!)\nEach video has\nid: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.\nlabels: list of labels of that video.\nEach frame has\nrgb: float array of length 1024,\nEach frame has\naudio: float array of length 128\nA subset of the validation set videos are provided with segment-level labels.  In addition toid,labelsand the frame level features described above, they come with\nsegment_start_times: list of segment start times.\nsegment_end_times: list of segment end times.\nsegment_labels: list of segment labels.\nsegment_scores: list of binary values indicating positive or negative corresponding to the segment labels.\nFiles are inTFRecords format, TensorFlow python readers are available in thegithub repo.\nframe-sample.zip- a sample of frame-level data includingtrain00andtrain01\nvalidate-sample.zip- a sample of validation set data includingvalidate00andvalidate01\nvocabulary.csv- the full data dictionary for label names and their descriptions\nsample_submission.csv- a sample submission file in the correct format\nFor eachClass, submit a space-delimited list of the segments your model predicts as having that class, ordered by confidence (highest first).\nIMPORTANT:In order to minimize submission file sizes, for segment predictions, you should only include the video id and the segment start time, butnotthe segment end time. (These are not needed, since all segments are 5 seconds in duration.)\nFiles4 filesSize526.32 MBTypezip, csvLicenseSubject to Competition Rules",
      "docker_challenge_path": "/data/youtube8m-2019",
      "competition_description": "Challenge description:\n# The 3rd YouTube-8M Video Understanding Challenge: Temporal localization of topics within video\n\nImagine being able to search for the moment in any video where an adorable kitten sneezes, even though the uploader didn't title or describe the video with such descriptive metadata. Now, apply that same concept to videos that cover important or special events like a baby's first steps or a game-winning goal -- and now we have the ability to quickly find and share special video moments. This technology is called temporal concept localization within video and Google Research can use your help to advance the state of the art in this area.\n\nAn example of the detected action \"blowing out candles\"\n\nIn most web searches, video retrieval and ranking is performed by matching query terms to metadata and other video-level signals. However, we know that videos can contain an array of topics that aren't always characterized by the uploader, and many of these miss localizations to brief but important moments within the video. Temporal localization can enable applications such as improved video search (including search within video), video summarization and highlight extraction, action moment detection, improved video content safety, and many others.\n\nIn previous years, participants worked on advancements in video-level annotations, building both unconstrained and constrained models. In this third challenge based on the YouTube 8M dataset, Kagglers will localize video-level labels to the precise time in the video where the label actually appears, and do this at an unprecedented scale. To put it another way: at what point in the video does the cat sneeze?\n\nIf successful, your new machine learning models will significantly improve video understanding for all, by not only identifying the topics relevant to a video, but also pinpointing where in the video they appear.\n\nThis competition is being hosted by Google Research as a part of the International Conference on Computer Vision (ICCV) 2019 selected workshop session. Please refer to the YouTube 8M Large-Scale Video Understanding Workshop Page for details about the workshop.",
      "evaluation_metric": "Submissions are evaluated according to the Mean Average Precision @ K (MAP@K), where \\( K = 100,000 \\).\n\n$$MAP@100,000 = \\frac{1}{C} \\sum_{c=1}^{C} \\frac{\\sum_{k=1}^{n} P(k) \\times rel(k)}{N_{c}}$$\n\nwhere \\( C \\) is the number of Classes, \\( P(k) \\) is the precision at cutoff \\( k \\), \\( n \\) is the number of Segments predicted per Class, \\( rel(k) \\) is an indicator function equaling 1 if the item at rank \\( k \\) is a relevant (correct) Class, or zero otherwise, and \\( N_{c} \\) is the number of positively-labeled segments for the each Class.",
      "dataset_description": "The 3rd YouTube-8M Video Understanding ChallengeTemporal localization of topics within video\nIn this competition, you will predict theClasslabels of YouTube video segments. We provide you extracted frame-level features. The feature data and detailed feature information can be found on theYouTube-8M datasetwebpage.\nThe training dataset in this competition contains videos and labels that are publicly available on YouTube, while the test data is not publicly available. The test data also has anonymized video IDs to ensure the fairness of the competition.\nFile descriptions\nframe-level data\nYou may download to your local computer with instructionshere\nTotal size of 1.53TB\u00a0(Large file warning!)\nEach video has\nid: unique id for the video, in train set it is a YouTube video id, and in test/validation they are anonymized.\nlabels: list of labels of that video.\nEach frame has\nrgb: float array of length 1024,\nEach frame has\naudio: float array of length 128\nA subset of the validation set videos are provided with segment-level labels.  In addition toid,labelsand the frame level features described above, they come with\nsegment_start_times: list of segment start times.\nsegment_end_times: list of segment end times.\nsegment_labels: list of segment labels.\nsegment_scores: list of binary values indicating positive or negative corresponding to the segment labels.\nFiles are inTFRecords format, TensorFlow python readers are available in thegithub repo.\nframe-sample.zip- a sample of frame-level data includingtrain00andtrain01\nvalidate-sample.zip- a sample of validation set data includingvalidate00andvalidate01\nvocabulary.csv- the full data dictionary for label names and their descriptions\nsample_submission.csv- a sample submission file in the correct format\nFor eachClass, submit a space-delimited list of the segments your model predicts as having that class, ordered by confidence (highest first).\nIMPORTANT:In order to minimize submission file sizes, for segment predictions, you should only include the video id and the segment start time, butnotthe segment end time. (These are not needed, since all segments are 5 seconds in duration.)\nFiles4 filesSize526.32 MBTypezip, csvLicenseSubject to Competition Rules",
      "metadata": {
        "domain": "computer_vision",
        "keywords": [
          "segmentation",
          "time_series",
          "temporal modeling",
          "multimedia",
          "map"
        ]
      }
    }
  ]